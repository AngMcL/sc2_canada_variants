---
title: "VisualizeStates"
author: "Angela"
date: "10/11/2020"
output: github_document
---

## Change log
Updates 2023-02-08
* gen2 based on this script
* fixed plotting issues with colors, axes, empty
* added averted sublineages, singletons, and cases 
* estimate of total focal var cases
* imports over time as line plots with confidence intervals, added alpha channel to non focal sources
* singletons: found tmrca of parental node, used 3/4 point b/w sample date and tmrca dt as intro date
* moved domestic transmission scripts into a different script

TO DO
* add singletons and sublins over time together
* calculate singeltons averted
* add confidence interval of sublins over time into the averted estimation

Update 2022-12-09
* make a generic version (based on the eta script) that will work on all vars
* include b/w prov transition events
* imports over time, make raw counts, consider showing singletons+sublineages
* Add text output of data queries
* add an input for whether or not there were variant specific measures, if so what date

Update 2022-11-21
update gamma script for eta
note: should really make a version of this that takes the variant name as input 
currently have to search/repl gamma with eta
check custom chunks about largest sublineages
modify key dates if needed
eta, epsilon, kappa, zeta, mu - no custom dates: remove highlight annotation and any before vs after
once complete on theses variants, write a new script to combine all the summarized results (sublins over time and descendants)

UPDATE 2022-10-24
Change file locations, start date, end date
Lineage colors should be similar to the var.WHO - specify within the script

UPDATE 2022-06-17
Many changes made for VOC-specific analysis
* voc instead of a specific subsaple strat
* need to subset out non-VOC intros / sublins
* should also subset to not allow predating voc.df$earliestsample
* 

UPDATE 2022-05-11
Incorporated changes that account for nested sublineages. If a sequence has multiple sublineage identities (due to nested sublineages), then only identify it as a unique descendant of the most recent one. Issue was that we were overinflating the size of early sublineages which were ancestral to large sections of the tree. This led to overestimate of early subs' sizes, the age since importation, etc. Multiple places where updates are necessary. Also, when only counting unique descendants, some sublineages disappeared altogether. Need to check these out - are they a result of polytomies? Were they artefacts of binarized polytomies? Once these changes made in this script, need to merge these changes into the other subsampling strategy scripts, including the comparison one.

UPDATE 2022-04-04
Decided that the 75% subsample was the best one to focus on in the figures
Restrict inputs to only have those ones
Change time range up to 2021-03-01
Change color scheme
Re-export all the figures and numbers for the results

Previous changes:
* Read in the output of the ancestral reconstruction from all boots
* Look at origins, intros, decscendants over time (using earliest sample collection date)
* Make significant code changes to read into list, extract necesssary uncertatinty related to 
  * Sublineages' #/% origin location, #/% province of introduction
  * Sublineage size distribution (what % sublins )
  * % singletons
* Signif change to merge the TMRCA_boots script INTO this one   
  
## to do
* incorporate tree bootstrap support into likelihood sensitivity analysis
* make this into a .Rscript to run via .sh script on cluster

## setup
```{r include=FALSE}
library(ape)
library(stringr)
library(dplyr)
library(tidyr)
library(plyr)
library(RColorBrewer)
library(ggplot2)
library(ggtree)
library(ggimage)
library(phytools)
library(phangorn)
library(forcats)
library(lubridate)
library(gridExtra)
library(cowplot)
library(ggstance)
library(ggalluvial)
library(ggmosaic)
library(MASS)
library(gtools)
library(treeio)
library(magick)
library(treemapify)
library(EpiEstim)
library(incidence)
library(reshape2)
library(zoo)
library(fitdistrplus)
library(ggridges)

```

## Set inputs 
```{r}
#Modifiable inputs
focal.var<-"alpha"
Focal.var<-str_to_title(focal.var)
ft_trees<-paste0("../02_trees/202208_analysis/",focal.var, "_lsd_out")
ft_in<-paste0("202208_ace_out/",focal.var,"_ace_out")
ft_trees_pre<-"../02_trees/202208_analysis/ft_alphadelta_root_res"
focal.source<-c("United Kingdom") #can be multiple
focal.source.brf<-c("UK") #for exports

#were there variant specific interventions, if so, what dates?
int.yn<-TRUE
if(int.yn==T){
  int.start<-as.Date("2020-12-20")
  int.end<-as.Date("2021-01-06")
  int.duration<-int.end-int.start
  int.descrip<-"UK flight ban"
}

#for figures - made shorter
fig.start.date<-as.Date("2020-11-01")
fig.end.date<-as.Date("2021-06-01")

#geo colors and lookup (merged to conts for small contribs)
global.colors.in<-"../01_subSample/colors_out/global.colors.tsv"
lookup.geo.in<-"../01_subSample/colors_out/lookup.geo.csv"
#variant colors
variant.colors.in<-"../01_subSample/CleanMeta_PreSubsamp/variant_colors.tsv"
#updated these
variant.df.in.glob<-paste0("../01_subSample/archive/Results_PreSubsamp_omi/variant.firstglobdate.csv")
variant.df.in<-paste0("../01_subSample/Results_PreSubsamp_omi_2023/variant.df.can.csv")
meta.glob.cases.in<-"../01_subSample/Results_PreSubsamp_omi_2023/meta.glob.var.country.daily.full3.csv"
meta.can.cases.in<-"../01_subSample/Results_PreSubsamp_omi_2023/meta.can.var.daily.full3.csv"
# study period
data.date<-"2022-03-22" 
start.date<-"2020-11-01"

# folder for exports
today<-Sys.Date()
f.out<-paste0(today,"_",focal.var,"_analysis/") 
# f.out<-"2023-07-06_alpha_analysis/"
if(!dir.exists(f.out)){dir.create(f.out)}

# EXPORT a textfile of inputs, summaries, and data queries
text.out<-paste0(f.out,today,"_DataSummary.txt")
cat(paste0("variant = ",focal.var),
    paste0("focal source geography = ",focal.source),
    paste0("lsd folder = ",ft_trees),
    paste0("var-specific intervention = ",int.yn),
    file=text.out,sep="\n",append=F)

if(int.yn==T){
  cat(paste0("var-spec intervention: ",int.descrip, ", from ",int.start," - ", int.end),
      paste0("Intervention duration=",int.end-int.start),
      file=text.out,sep="\n",append=T)
}
```

## read in color for Global and lineage groups
```{r}
#import a tsv of name and hex color for LOCATIONS
globalPalette<-read.table(global.colors.in)
  # "../../20210301_phylogeo/03_ancestral/colors_out/global.colors.tsv",sep="\t")
## make color scheme
glob.colz<-row.names(globalPalette) #NOTE THIS DIDN"T WORK
globalPalette.ch<-as.character(globalPalette$globalPalette)
names(globalPalette.ch)<-glob.colz

GlobColScale<-scale_colour_manual(name = "Location",values = globalPalette.ch,na.value="grey60")
GlobFillScale<-scale_fill_manual(name = "Location",values = globalPalette.ch,na.value="grey60")

CountryColScale<-scale_colour_manual(name = "Global region",
                                     values = globalPalette.ch[15:length( globalPalette.ch)],
                                     na.value="grey60")
CountryFillScale<-scale_fill_manual(name = "Global region",
                                    values = globalPalette.ch[15:length( globalPalette.ch)]
                                    ,na.value="grey60")

#read in lookup table to reduce cateogires to apply this color scheme (if haven't run MakeGlobalColors_boots.Rmd in this project already)
lookup.geo<-read.csv(lookup.geo.in)

## read in the variant df to use for inputs
var.df<-read.csv(variant.df.in)

#take the first canada data from this instead (complete for delta)
var.df.glob<-read.csv(variant.df.in.glob)
var.df.glob<-var.df.glob[,c("var.WHO","first.can.date")]
colnames(var.df.glob)<-c("var.WHO","first.can.date.new")
var.df<-left_join(var.df,var.df.glob,by="var.WHO")
var.df$first.can.date<-var.df$first.can.date.new #replaced!

var.df<-var.df[with(var.df, order(var.df$first.can.date, decreasing = F)),]

# use the first canadian sample date for variant to pull the delay in intervention from first canadian sample to int start
if(int.yn==T){
  firstsamp<-var.df$first.can.date[which(var.df$var.WHO %in% Focal.var)]
  cat(paste0("First Can sample: ", as.Date(firstsamp) ),
      paste0("Intervention delay = ", (int.start - as.Date(firstsamp)) ),
      file=text.out,sep="\n",append=T)
}

var.ord<-var.df$var.WHO
var.ord<-var.ord[-which(var.ord %in% c("Theta","Lambda","GH/490R"))]

varPalette<-read.table(variant.colors.in,sep="\t")

#varaint specific colors
focal.col<-rownames(varPalette)[which(tolower(varPalette$vars.ch)==focal.var)]
focal.pango<-var.df$var.lin[tolower(var.df$var.WHO)==focal.var]
if(str_detect(focal.pango,"\\+")){
  focal.pango<-unlist(str_split(focal.pango,"\\+"))[[1]]
}

## make color scheme
var.colz<-as.character(varPalette$vars.ch)
varPalette.ch<-row.names(varPalette)
names(varPalette.ch)<-var.colz

VOCColScale<-scale_colour_manual(name = "Variant",values = varPalette.ch,na.value=NULL)
VOCFillScale<-scale_fill_manual(name = "Variant",values = varPalette.ch,na.value=NULL)

VOCFillScale2<-scale_fill_manual(name = "Variant",values = varPalette.ch[match(var.ord,names(varPalette.ch))],na.value="grey60")
VOCColScale2<-scale_colour_manual(name = "Variant",values = varPalette.ch[match(var.ord,names(varPalette.ch))],na.value="grey60")

#MY FOCAL VAR COL
var.col<-varPalette.ch[which(names(varPalette.ch)==Focal.var)]

```

## Figure publication themes
```{r}
#panel.grid.major = element_blank(), 
pubTheme<-theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(),
                panel.background=element_rect("grey95"), 
                axis.line = element_line(colour = "black"),
                legend.key.size = unit(0.5,"line"),
                text=element_text(size=10,face="bold"),
                legend.text=element_text(size=7))

#panel.grid.major = element_blank(), 
pubThemeDate<-theme(panel.grid.major = element_blank(), 
                    panel.grid.minor = element_blank(),
                    panel.background=element_rect("grey95"), 
                    axis.line = element_line(colour = "black"),
                    text=element_text(size=10,face="bold"),
                    legend.key.size = unit(0.4,"line"),
                    legend.text=element_text(size=7),
                    axis.text.x=element_text(angle = 45,hjust=1,size=rel(1)))


scaleDate<-scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", date_labels = "%b %Y",limits=as.Date(c(start.date,"2022-03-22")),expand=c(0,0))

xdates<-ymd(c("2020-12-01","2021-03-01","2021-06-01","2021-09-01","2021-12-01","2022-03-01"))
scaleDateBusy<-scale_x_date(breaks=xdates, date_labels = "%b %Y",
                            limits=as.Date(c(start.date,"2022-03-22")),
                            expand=c(0,0))

scaleDateFlex<-scale_x_date(date_breaks = "1 month", date_labels = "%b %Y",expand=c(0,0))
scaleDateFlexLess<-scale_x_date(date_breaks = "1 month", date_labels = "%b %Y",expand=c(0,0),
                                limits=c(fig.start.date, fig.end.date))
scaleDateFlexMore<-scale_x_date(date_breaks = "1 month", date_labels = "%b %Y",expand=c(0,0),
                                limits=c(fig.start.date, "2021-08-01"))
scaleMonthChar<-scale_x_discrete(breaks=c("2020-12","2021-03","2021-06","2021-09","2021-12","2022-03"),labels=c("Dec 2020","Mar 2021","Jun 2021","Sep 2021","Dec 2021","Mar 2022"))
```

## setup bootstraps and inputs, outputs
```{r}
#Input folders of trees
dir.exists(ft_trees)
#input trees 
trees.in.nex <- list.files(ft_trees, pattern="timetree.nex$",full.names = T) %>% mixedsort()
# trees.in.nex
if(length(trees.in.nex)<1) print("where are the trees??")

#boots aka trees to operate over
n.B<-length(trees.in.nex)
BOOTS<-1:n.B
print(paste("There are ",n.B," trees",sep=""))

#Input folders of ace output
dir.exists(ft_in)

#Input folders of input to lsd
dir.exists(ft_trees_pre)

# input DFs from ace
all.anc.in<-list.files(ft_in,pattern="all.anc.csv",full.names = T) %>% mixedsort() #for the internal transitions
# can.anc.in<-list.files(ft_in,pattern="can.anc.csv",full.names = T) %>% mixedsort()
sum.in<-list.files(ft_in,pattern="sublin.csv",full.names = T)  %>% mixedsort()
meta.in<-list.files(ft_in,pattern="meta.csv",full.names = T) %>% mixedsort()
can.states.in<-list.files(ft_in,pattern="tip.anc.csv",full.names = T) %>% mixedsort()
dates.in<-str_replace_all(trees.in.nex,ft_trees,ft_trees_pre) %>%
  str_replace_all(".fasta.timetree.nex","_dates.txt")
all(file.exists(dates.in))
all(file.exists(meta.in))

#check lengths
# length(can.states.in)==length(trees.in.nex)
length(meta.in)

##OUTPUTS
#main output directory 
sum.out<-str_replace_all(sum.in, ".sublin.csv",".sublin.firstCanDate.csv")
sing.out<-str_replace_all(can.states.in, "_mask.tip.anc.csv","_singletons.csv")

#sublineage epi plots
f.sublin.out<-paste(f.out,"/Sublin.Out",sep="")
if(!dir.exists(f.sublin.out)){dir.create(f.sublin.out)}

#add to summary file
cat(paste0("n.boots = ",n.B),
    paste0("folder.out = ",f.out),file=text.out,sep="\n",append=TRUE)
```

## setup summary functions
```{r}
# function to spit out mean and 95% CI (for small n, t-dsitribution) for a vector containing n observations/boots with X digits
mean.95ci.X<-function(vector, digits){
  n<-length(vector)
  m<-mean(vector)
  sd<-sd(vector)
  t<-qt(0.025,(n-1),lower.tail=F)
  up<-m+(sd/sqrt(n)*t)
  low<-m-(sd/sqrt(n)*t)
  return(paste(round(m,digits=digits), " (",round(low,digits=digits)," - ",round(up,digits=digits),")",sep=""))
}

#scientific notation, digits=x
mean.95ci.SC<-function(vector,digits){ 
  n<-length(vector)
  m<-mean(vector)
  sd<-sd(vector)
  t<-qt(0.025,(n-1),lower.tail=F)
  up<-m+(sd/sqrt(n)*t)
  low<-m-(sd/sqrt(n)*t)
  return(paste(formatC(m,format = "e", digits = digits), " (",
              formatC(low,format = "e", digits = digits)," - ",
              formatC(up,format = "e", digits = digits),")",sep=""))
}

mean.95ci.givenmsd<-function(m,sd,n, digits){
  t<-qt(0.025,(n-1),lower.tail=F)
  up<-m+(sd/sqrt(n)*t)
  low<-m-(sd/sqrt(n)*t)
  return(paste(round(m,digits=digits), " (",round(low,digits=digits)," - ",round(up,digits=digits),")",sep=""))
}

#upperCI only with X and N input
upper.ci.X<-function(vector, digits){ 
  n<-length(vector)
  m<-mean(vector,na.rm = T)
  sd<-sd(vector,na.rm = T)
  t<-qt(0.025,(n-1),lower.tail=F)
  up<-m+(sd/sqrt(n)*t)
  return(round(up,digits=digits))
}

#lowerCI only with X and N input
lower.ci.X<-function(vector, digits){
  n<-length(vector)
  m<-mean(vector,na.rm = T)
  sd<-sd(vector,na.rm = T)
  t<-qt(0.025,(n-1),lower.tail=F)
  low<-m-(sd/sqrt(n)*t)
  return(round(low,digits=digits))
}

#width of the CI
width.ci.X<-function(vector, digits){
  n<-length(vector)
  m<-mean(vector,na.rm = T)
  sd<-sd(vector,na.rm = T)
  t<-qt(0.025,(n-1),lower.tail=F)
  width<-2*(sd/sqrt(n)*t)
  return(round(width,digits=digits))
}

#as above, but for larger n using the normal distribution instead of t distribution
#upperCI only with X and N input
upper.ci.big<-function(vector, digits){ 
  n<-length(vector)
  m<-mean(vector,na.rm = T)
  sd<-sd(vector,na.rm = T)
  t<-qnorm(0.025,(n-1),lower.tail=F)
  up<-m+(sd/sqrt(n)*t)
  return(round(up,digits=digits))
}

#lowerCI only with X and N input
lower.ci.big<-function(vector, digits){
  n<-length(vector)
  m<-mean(vector,na.rm = T)
  sd<-sd(vector,na.rm = T)
  t<-qnorm(0.025,(n-1),lower.tail=F)
  low<-m-(sd/sqrt(n)*t)
  return(round(low,digits=digits))
}

#width of the CI
width.ci.big<-function(vector, digits){
  n<-length(vector)
  m<-mean(vector,na.rm = T)
  sd<-sd(vector,na.rm = T)
  t<-qnorm(0.025,(n-1),lower.tail=F)
  width<-2*(sd/sqrt(n)*t)
  return(round(width,digits=digits))
}

```

# Read in, clean, and merge dfs for each tree/subsample (individual boots)
```{r main chunk}
# read into lists of length trees.in
meta.boots<-replicate(n=n.B,vector()) 
cana.boots<-replicate(n=n.B,vector()) 
sum.boots<-replicate(n=n.B,vector())
tree.boots.nex<-replicate(n=n.B,vector())
sublin.long<-replicate(n=n.B,vector())
sublin.long.unq<-replicate(n=n.B,vector())
dates.samp<-replicate(n=n.B,vector())

can.sing.boots<-replicate(n.B,vector())
can.tip.states.boots<-replicate(n.B,vector())

# make an object of provs
provs<-c("Quebec","Ontario","Alberta","British Columbia","Manitoba","Nova Scotia","New Brunswick","Newfoundland and Labrador","Saskatchewan")
provs.long<-paste("Canada",provs,sep="_")

k=1
# Loop across all trees of any
for (k in 1:n.B) {
  print(paste("Reading boot",k,sep=" "))  
  #### Read in objects from ancestralreconstruction.rmd ####
  sum.boots[[k]]<-read.csv(sum.in[k])
  meta.boots[[k]]<-read.csv(meta.in[k])
  tree.boots.nex[[k]]<-read.beast(trees.in.nex[k])
  dates.samp[[k]]<-read.table(dates.in[[k]],sep = "\t") #CHAGNE THIS FOR FUTURE
  colnames(dates.samp[[k]])<-c("tip.label","date.lsd.full")
  meta.boots[[k]]<-left_join(meta.boots[[k]],dates.samp[[k]],by="tip.label")
  
  #subset meta to cana only
  cana.boots[[k]]<-meta.boots[[k]] [str_which(meta.boots[[k]]$country, "Canada"),]
  
  
  #Rename provinces so Canada not in label
  sum.boots[[k]]$Node.Location<-sapply(sum.boots[[k]]$Node.Location, function (x) str_replace_all(x, "Canada_", ""))
  
  #make sure no "." instead of space 
  sum.boots[[k]]$Node.Location<-str_replace_all(sum.boots[[k]]$Node.Location, "\\.", " ")
  sum.boots[[k]]$Parent.Location<-str_replace_all(sum.boots[[k]]$Parent.Location, "\\.", " ")
  sum.boots[[k]]$Descendant.Location<-str_replace_all(sum.boots[[k]]$Descendant.Location, "\\.", " ")
  
  ## APPLY the lookup table to change names
  #Add fiji
  lookup.geo<-bind_rows(lookup.geo,data.frame(og.loc="Fiji",new.loc="Oceania"))
  ## GROUP together if applicable
  for (i in 1:nrow(lookup.geo)){
    if(lookup.geo$new.loc[i] != lookup.geo$og.loc[i]){ #if they don't match, replace 
      #find matches in node loc, and replace with new loc
      pr.ma<-which(sum.boots[[k]]$Node.Location==lookup.geo$og.loc[i])
      if (length(pr.ma)>0) {sum.boots[[k]]$Node.Location[pr.ma]<-lookup.geo$new.loc[i]; next}
      par.ma<-which(sum.boots[[k]]$Parent.Location==lookup.geo$og.loc[i])
      if (length(par.ma)>0) {sum.boots[[k]]$Parent.Location[par.ma]<-lookup.geo$new.loc[i]; next}
    }
  }
 
  #Remove any sublineages with >30000 descenndants, this is likely spurious
  biggie<-which(sum.boots[[k]]$Number.Descendants>30000)
  if(length(biggie)>0){
      sum.boots[[k]]<-sum.boots[[k]][-biggie,]
  }
  
  ## Make list of sublineages, where each one is a df in order to map descendants
  n.sl<-nrow(sum.boots[[k]])
  desc.df.list<-replicate(n=n.sl,vector())
  for (i in 1:n.sl){
    #name it by defining internal node
    names(desc.df.list)[[i]]<-sum.boots[[k]]$Node[i] 
    #extract accession ID (to lookup date in next chunk)
    accessions<-unlist(strsplit(sum.boots[[k]]$Descendant.AccessionID[i], ", "))
    #add each desc as a row in the df
    desc.df.list[[i]]<-data.frame(Lineage=sum.boots[[k]]$Lineage[i],
                                  Node=sum.boots[[k]]$Node[i], #use node to match back later
                                  Descendant.AccessionIDs=accessions,
                                  State=NA,
                                  Date=NA) 
  } 
  
  ## Look up each accession number for its sampling location and LSD-inferred date
  ## Also this fixes the mismatch issue between location and date
  for (i in 1:n.sl){
    # i=10
    metaRows<-match (desc.df.list[[i]]$Descendant.AccessionIDs, meta.boots[[k]]$GISAID_ID) # in order
    #extract location and dates
    desc.df.list[[i]]$State<-meta.boots[[k]]$state [metaRows]
    desc.df.list[[i]]$Date<-as.Date(meta.boots[[k]]$date.lsd.full[metaRows])
  }

  ## Add the LSD dates onto the sumboots object
  ## for each introduction, find the earliest sample collection date and where sampled
  sum.boots[[k]]$FirstCanDateLSD<-as.Date(NA)
  sum.boots[[k]]$FirstCanLocLSD<-NA
  sum.boots[[k]]$FirstCanGISAID<-NA 
  #repeat for lasts
  sum.boots[[k]]$LastCanDateLSD<-as.Date(NA)
  sum.boots[[k]]$LastCanLocLSD<-NA
  sum.boots[[k]]$LastCanGISAID<-NA 
    
  for (i in 1:nrow(sum.boots[[k]])){
    #list of desc is ordered same as sum.boot, subset to the descendants in Canada
    mtchCan<-desc.df.list[[i]] [str_detect(desc.df.list[[i]]$State, "Canada") ,]
    
    #order the subsetted dataframe by date
    mtchCan<-mtchCan[order(mtchCan$Date),]
    
    sum.boots[[k]]$FirstCanDateLSD[i]<-as.Date(first(mtchCan$Date))
    sum.boots[[k]]$FirstCanLocLSD[i]<-first(mtchCan$State)
    sum.boots[[k]]$FirstCanGISAID[i]<-first(mtchCan$Descendant.AccessionIDs)
    
    sum.boots[[k]]$LastCanDateLSD[i]<-as.Date(last(mtchCan$Date))
    sum.boots[[k]]$LastCanLocLSD[i]<-last(mtchCan$State)
    sum.boots[[k]]$LastCanGISAID[i]<-last(mtchCan$Descendant.AccessionIDs)
  }

  ## Name the sublineages ##
  #as Lineage.canX, where X is the order of first sample detection for a given lineage
  sum.boots[[k]]<-with(sum.boots[[k]],sum.boots[[k]][(order(sum.boots[[k]]$FirstCanDateLSD)),])
  
  #make a new column 
  sum.boots[[k]]$Sublineage<-NA 
  sublins<-c()
  for (i in 1:nrow(sum.boots[[k]])){
    subl<-paste(sum.boots[[k]]$Lineage[i],".can",sep="")
    ss<-0
    subl2<-str_replace_all(subl,"\\.","\\\\.") #weird R shit
    ss<-length(str_which(sublins,subl2)) #how many sublins already designated?
    sum.boots[[k]]$Sublineage[i]<-paste(subl,(ss+1),sep="")
    sublins<-c(sublins,subl)
  }

  ## ANOTHER sublinaege name that incorps the first gisaid id
  sum.boots[[k]]$Sublineage2<-NA 
  for (i in 1:nrow(sum.boots[[k]])){
    subl<-paste(sum.boots[[k]]$Lineage[i],".can.",sum.boots[[k]]$FirstCanGISAID[i],sep="")
    sum.boots[[k]]$Sublineage2[i]<-subl
  }
  #check unique
  length(sum.boots[[k]]$Sublineage2)==unique(length(sum.boots[[k]]$Sublineage2))

  ## Add sublineage back into the desc.df.list object, use the node column to matchy
  for (i in 1:length(desc.df.list)){
    match<-which(sum.boots[[k]]$Node== desc.df.list[[i]]$Node[1])
    desc.df.list[[i]]$Sublineage<-sum.boots[[k]]$Sublineage[match]
    desc.df.list[[i]]$Sublineage2<-sum.boots[[k]]$Sublineage2[match]
  }
   
  #rbind them all into one df to refer to later
  sublin.long[[k]]<-bind_rows(desc.df.list)
  
  #### Pull tmrca ####
  ## Find MRCA node in time tree linking the descendent accession IDs in each sublineage 
  tmrca.df<-data.frame(Node=sum.boots[[k]]$Node, #node in the binarized tree
                       mrca_nexus=NA, #mrca node in the non-binarized nexus tree with accurate dates
                       tmrca=NA, tmrca.dt=NA, tmrca.upper.dt=NA, tmrca.lower.dt=NA) #date format
  
  #pull information from the nexus tree (accurate dates, non-binarized)
  td<-get.data(tree.boots.nex[[k]])
  tp<-get.tree(tree.boots.nex[[k]])
  
  #pull the root date as a decimal
  myroot<-decimal_date(as.Date(first(sort(td$date))))
    
  for (i in 1:nrow(tmrca.df)){
    #find tip labels matching any descendant ID in the sublineage
    tip.match.j<-str_which(tp$tip.label,
                           str_replace_all(sum.boots[[k]]$Descendant.AccessionID[i],
                                           ", ", "\\|"))
    mrca.j<-getMRCA(tp,tip.match.j)
    tmrca.df$mrca_nexus[i]<-mrca.j
    tmrca.j<-nodeheight(tp,mrca.j)
    tmrca.df$tmrca[i]<-tmrca.j #node height rel. to root
    
    #pull the TMRCA as a calendar date (mean)
    # TMRCA.dt<-td$date[td$node==mrca.j]
    # if(TMRCA.dt=="2020"|TMRCA.dt=="2021"|TMRCA.dt=="2022"){TMRCA.dt<-format(date_decimal(myroot+tmrca.j),"%Y-%m-%d")}
    # if(is.na(TMRCA.dt)){
    TMRCA.dt<-format(date_decimal(myroot+tmrca.j),"%Y-%m-%d")
      # }
    tmrca.df$tmrca.dt[i]<-TMRCA.dt
    
    # pull the confidence interval for TRMCA
    if(any(td$node==mrca.j)){
      if(!is.na(td$CI_date[td$node==mrca.j])){
        LOWER.dt<-unlist(td$CI_date[td$node==mrca.j])[1]
        tmrca.df$tmrca.lower.dt[i]<-LOWER.dt
        UPPER.dt<-unlist(td$CI_date[td$node==mrca.j])[2]
        if(UPPER.dt=="2021"){UPPER.dt<-as.Date(TMRCA.dt)}
        tmrca.df$tmrca.upper.dt[i]<-UPPER.dt
      }
    }
  }  #end of loop over i rows
  
  ## connect TMRCA back to sum.boots[[k]] object 
  # all(sum.boots[[k]]$Node %in% tmrca.df$Node)
  sum.boots[[k]]<-left_join(sum.boots[[k]],tmrca.df,by="Node")
  
  ## Any unrealistic TMRCAs? 
  too.early<-which(sum.boots[[k]]$tmrca.dt<"2020-09-15")
  # print(sum.boots[[k]]$Sublineage[too.early])
  if(length(too.early)>0){ #replace with the upper limit
    sum.boots[[k]]$tmrca.dt[too.early]<-sum.boots[[k]]$tmrca.upper.dt[too.early]
  }
  #remove sublinages taht are still too early
  still.too.early<-which(sum.boots[[k]]$tmrca.dt<"2020-09-15")
  if(length(still.too.early)>0){
    sum.boots[[k]]<-sum.boots[[k]][-still.too.early, ]
  }

  
  #make year-month a new column
  meta.boots[[k]]$yearmonth<-as.character(NA)
  for (i in 1:nrow(meta.boots[[k]])){
    meta.boots[[k]]$yearmonth[i]<-format(as.Date(meta.boots[[k]]$date.lsd.full[i]), "%Y-%m")
  }
  
  #yearmonth
  sum.boots[[k]]$yearmonth<-as.character(NA)
  for (i in 1:nrow(sum.boots[[k]])){
    sum.boots[[k]]$yearmonth[i]<-format(as.Date(sum.boots[[k]]$tmrca.dt[i]), "%Y-%m")
  }
  
  ## add back some information onto sublin.long from sum.boots
  add.on<-sum.boots[[k]][,c("Sublineage","Node.Location","tmrca.dt","tmrca.upper.dt","tmrca.lower.dt")]
  sublin.long[[k]]<-left_join(sublin.long[[k]],add.on,by="Sublineage")

  # Consider some seqs have multiple sublineages - only take most recent sublin for unique desc 
  dupl.desc<-unique(sublin.long[[k]]$Descendant.AccessionIDs[which(duplicated(sublin.long[[k]]$Descendant.AccessionIDs ))]) #unique GISAID IDS that have duplicates (ie no duplicates of the duplicate IDs)
  l.d<-length(dupl.desc)
  dupz<-c()
  if(l.d<1){sublin.long.unq[[k]]<-sublin.long[[k]]} #if no duplicates, they are unique
  if(l.d>0){ #if there are duplicates, 
    for (j in 1:l.d){ #iterate through dups, find matches, accumulate dups to remove
    matchy<-which(sublin.long[[k]]$Descendant.AccessionIDs == dupl.desc[j]) #find all instances of the ID
    sublin.long[[k]]$Sublineage[matchy] #redundant sublineage calls
    rec<-max(as.Date(sublin.long[[k]]$tmrca.dt[matchy])) #find the most recent tmrca dt
    keep<-which(sublin.long[[k]]$tmrca.dt[matchy]==rec) #keep the instance matching most recent tmrca, 
    if(length(keep)>1){ #if multiple equally recent tmrcas....
      #keep it in the smallest sublin, b/c more specific
      szs<-sum.boots[[k]]$N.Desc.glob[sum.boots[[k]]$Sublineage %in%
                                          sublin.long[[k]]$Sublineage[matchy[keep]]]
      keep<-keep[which(szs==min(szs))]
    }
    dup<-matchy[-keep] #dup are all instances that we don't keep
    dupz<-c(dupz,dup) #add to running vector to remove
  }
  sublin.long.unq[[k]]<-sublin.long[[k]][-dupz, ]
  } #end of 'if dups'

  
 #note these have pre-lookup names ie Nova Scotia...
  provs.long2<-unique(meta.boots[[k]]$state[str_detect(meta.boots[[k]]$state,"Canada")])   
 
  ## Count the unique descendants global and canadian overall
  for (j in 1:nrow(sum.boots[[k]])){
    n.d.gl<-length(which(sublin.long.unq[[k]]$Sublineage==sum.boots[[k]]$Sublineage[j]))
    n.d.ca<-length(which(sublin.long.unq[[k]]$Sublineage==sum.boots[[k]]$Sublineage[j] &
                           sublin.long.unq[[k]]$State %in% provs.long2))
    sum.boots[[k]]$N.Desc.glob[j]<-n.d.gl
    sum.boots[[k]]$N.Desc.can[j]<-n.d.ca
  }
  # head(sum.boots[[k]][,c("Number.Descendants","N.Desc.glob","N.Desc.can")])
  
  #Need to UDPATE the first/last can date columns to reflect unique descendants
  for (i in 1:nrow(sum.boots[[k]])){
    #which sequences are unique descendants of the sublineage and sampled in Canada
    mtchCan<-sublin.long.unq[[k]] [which(sublin.long.unq[[k]]$Sublineage==sum.boots[[k]]$Sublineage[i] &
            sublin.long.unq[[k]]$State %in% provs.long2),]

    #order the subsetted dataframe by date
    mtchCan<-mtchCan[order(mtchCan$Date),]
    
    sum.boots[[k]]$FirstCanDateLSD[i]<-as.Date(first(mtchCan$Date))
    sum.boots[[k]]$FirstCanLocLSD[i]<-first(mtchCan$State)
    sum.boots[[k]]$FirstCanGISAID[i]<-first(mtchCan$Descendant.AccessionIDs)
    
    sum.boots[[k]]$LastCanDateLSD[i]<-as.Date(last(mtchCan$Date))
    sum.boots[[k]]$LastCanLocLSD[i]<-last(mtchCan$State)
    sum.boots[[k]]$LastCanGISAID[i]<-last(mtchCan$Descendant.AccessionIDs)
  }
  
  ## calculate the detection lag (FirstCanDate - tMRCA) using updated unique desc.
  sum.boots[[k]]<-sum.boots[[k]] %>% mutate(detection.lag = as.Date(FirstCanDateLSD) -as.Date(tmrca.dt))
  sum.boots[[k]]$detection.lag<-as.numeric(sum.boots[[k]]$detection.lag)
  sum.boots[[k]]$tmrca.dt<-as.Date(sum.boots[[k]]$tmrca.dt)
  if(any(sum.boots[[k]]$detection.lag<0)){  
    sum.boots[[k]]$tmrca.dt[sum.boots[[k]]$detection.lag<0]<-
      sum.boots[[k]]$tmrca.dt[sum.boots[[k]]$detection.lag<0] %m-% months(1)}
  
  #lower and upper detection lag (lower tmrca is a bigger lag)
     sum.boots[[k]]<-sum.boots[[k]] %>% mutate(detection.lag.upper = as.Date(FirstCanDateLSD) -as.Date(tmrca.upper.dt))
  sum.boots[[k]]$detection.lag.upper<-as.numeric(sum.boots[[k]]$detection.lag.upper)
  sum.boots[[k]]<-sum.boots[[k]] %>% mutate(detection.lag.lower = as.Date(FirstCanDateLSD) -as.Date(tmrca.lower.dt))
  sum.boots[[k]]$detection.lag.lower<-as.numeric(sum.boots[[k]]$detection.lag.lower)
  
  ## change the tmrcadt to something more likely if detection lag is negative or really big
  sum.boots[[k]]$tmrca.dt.likely<-sum.boots[[k]]$tmrca.dt
  sum.boots[[k]]$tmrca.lower.dt<-as.Date(sum.boots[[k]]$tmrca.lower.dt)
  sum.boots[[k]]$tmrca.upper.dt<-as.Date(sum.boots[[k]]$tmrca.upper.dt)

  
  for (i in 1:nrow(sum.boots[[k]])){
    if(sum.boots[[k]]$detection.lag[i]<0){sum.boots[[k]]$tmrca.dt.likely[i]<-sum.boots[[k]]$tmrca.lower.dt[i]}
    if(sum.boots[[k]]$detection.lag[i]>100){sum.boots[[k]]$tmrca.dt.likely[i]<-as.Date(sum.boots[[k]]$tmrca.upper.dt[i])+7}
  }
  sum.boots[[k]]<-sum.boots[[k]] %>% mutate(detection.lag.likely=as.Date(FirstCanDateLSD) -as.Date(tmrca.dt.likely))
  #replace these columns
  sum.boots[[k]]$tmrca.dt<-sum.boots[[k]]$tmrca.dt.likely
  sum.boots[[k]]$detection.lag<-sum.boots[[k]]$detection.lag.likely

  #apply lookup table on sublinlong.unq
  sublin.long.unq[[k]]$State<-str_replace_all(sublin.long.unq[[k]]$State, "Canada_","")
  
  #convert using lookup table
   for (i in 1:nrow(lookup.geo)){
      if(lookup.geo$new.loc[i] != lookup.geo$og.loc[i]){ 
        #if they don't match, go replace instances
        #find matches in node loc, and replace with new loc
        pr.ma<-which(sublin.long.unq[[k]]$State==lookup.geo$og.loc[i])
        if (length(pr.ma)>0) {sublin.long.unq[[k]]$State[pr.ma]<-lookup.geo$new.loc[i];
        next}
      }
   }
  ## Re-name the sublineages using only unique descendants in the Lin support
  # call lineage support with only unique descendants
  sum.boots[[k]]$LineageUnq<-NA
  sum.boots[[k]]$Lin.SupportUnq<-NA
  for (j in 1:nrow(sum.boots[[k]])){
    # epis<-unlist(strsplit(sum.boots[[k]]$Descendant.AccessionID[j],split=", "))
    episUnq<-sublin.long.unq[[k]]$Descendant.AccessionIDs [which(sublin.long.unq[[k]]$Sublineage==sum.boots[[k]][j,"Sublineage"])]
    t<-table(meta.boots[[k]]$Lineage [which(meta.boots[[k]]$GISAID_ID %in% episUnq)])
    all<-sum(t)
    t<-rev(sort(t)) #sort by most frequent
    des<-c()
    for (p in 1:length(t)){
      n<-paste(names(t)[p], ": ",round(t[p]/all*100,digits=2),"%",sep="")
      des<-c(des,n)
    }
    sum.boots[[k]]$Lin.SupportUnq[j]<-paste0(des,collapse="; ")
    sum.boots[[k]]$LineageUnq[j]<-names(rev(sort(t)))[1] 
  } # end of loop identifying new lin support
  
  #Name the sublineages based on the lineageunq, ordered by first sample date
  sum.boots[[k]]<-with(sum.boots[[k]],sum.boots[[k]][(order(sum.boots[[k]]$FirstCanDateLSD)),])
  sum.boots[[k]]$SublineageUnq<-NA 
  sublins<-c()
  for (j in 1:nrow(sum.boots[[k]])){
    subl<-paste(sum.boots[[k]]$LineageUnq[j],".can",sep="")
    ss<-0
    subl2<-str_replace_all(subl,"\\.","\\\\.") #weird R shit
    ss<-length(str_which(sublins,subl2)) #how many sublins already designated?
    sum.boots[[k]]$SublineageUnq[j]<-paste(subl,(ss+1),sep="")
    sublins<-c(sublins,subl)
  }
  #check
  sum.boots[[k]][which(sum.boots[[k]]$Sublineage != sum.boots[[k]]$SublineageUnq),c("Sublineage","SublineageUnq")]

  #after checking these, they make more sense. Converting old sublin name
  sublin.long.unq[[k]] <- sublin.long.unq[[k]] %>% 
    left_join(sum.boots[[k]][,c("Sublineage","SublineageUnq","LineageUnq")],
              by="Sublineage")
  sublin.long.unq[[k]]$Sublineage<-sublin.long.unq[[k]]$SublineageUnq
  sublin.long.unq[[k]]$Lineage<-sublin.long.unq[[k]]$LineageUnq

  sum.boots[[k]]$Sublineage<-sum.boots[[k]]$SublineageUnq
  sum.boots[[k]]$Lineage<-sum.boots[[k]]$LineageUnq
  sum.boots[[k]]$Lin.Support<-sum.boots[[k]]$Lin.SupportUnq
  
  #calculate sublienage longevity (first to last canadian sample date)
  #change: had previously been going to last sample overall, now focusing on Ca
  sum.boots[[k]]<- sum.boots[[k]] %>% mutate(longevitySample=LastCanDateLSD-FirstCanDateLSD,
                             longevityTMRCA=LastCanDateLSD-tmrca.dt)
  
  #add some colnames to this df
  if(!any(colnames(sublin.long.unq[[k]])=="FirstCanDateLSD")){
    sublin.long.unq[[k]]<-left_join(sublin.long.unq[[k]], sum.boots[[k]][,c("Sublineage", "FirstCanDateLSD", "LastCanDateLSD")],  by="Sublineage")
  }
  
  ## ANY unrealistic sizes?
  too.big<-which(sum.boots[[k]]$N.Desc.glob>15000)
  # if(length(too.big)>1){
  #   sum.boots[[k]]<-sum.boots[[k]][-too.big, ]
  # }
  print(c(length(too.big),length(still.too.early)))
  
 #write this for future use 
  write.csv(sum.boots[[k]], sum.out[k])

  ### SINGLETONS ###
# for (k in 1:n.B){
#   td<-get.data(tree.boots.nex[[k]])
#   tp<-get.tree(tree.boots.nex[[k]])
#   myroot<-decimal_date(as.Date(first(sort(td$date))))
  
  #all can tip's states
  can.tip.states.boots[[k]]<-read.csv(can.states.in[k])

  #join canadian metadata by tiplabel to can.tip.states.boots[[k]] 
  if (!"par.lik" %in% colnames(cana.boots[[k]])){ #this is a check to make sure don't do it twice
    cana.boots[[k]]<-left_join(cana.boots[[k]],can.tip.states.boots[[k]],by="tip.label")
  }

  #change names
  cana.boots[[k]]$par.state<-str_replace_all(cana.boots[[k]]$par.state,"Canada_","")
  cana.boots[[k]]$tip.state<-str_replace_all(cana.boots[[k]]$tip.state,"Canada_","")
  
  ##def'n of singleton: tips with non-Canadian origin; can be inside a sublineage
  can.sing.boots[[k]]<-cana.boots[[k]][which(!cana.boots[[k]]$par.state %in% provs),]
  
  ## add TMRCA dt to singletons
  can.sing.boots[[k]]$tmrca.dt<-as.Date(NA)
  for (i in 1:nrow(can.sing.boots[[k]])){
    tip.match.j<-str_which(tp$tip.label, can.sing.boots[[k]]$tip.label[i])
    mrca.j<-getParent(tp,tip.match.j)
    tmrca.j<-nodeheight(tp,mrca.j)
    TMRCA.dt<-format(date_decimal(myroot+tmrca.j),"%Y-%m-%d")
    can.sing.boots[[k]]$tmrca.dt[i]<-TMRCA.dt
  }
  #make a halfway b/w tmrcadt and sample date
  ### THIS IS ACTUALLY 3/5 NOW, b/c was biasing too early
  can.sing.boots[[k]]$date.lsd.full<-as.Date(can.sing.boots[[k]]$date.lsd.full)
  can.sing.boots[[k]]<-mutate(can.sing.boots[[k]],tmrca.dt.half=
                                tmrca.dt+(((date.lsd.full-tmrca.dt)/5)*3), 
                              .keep=T)
  can.sing.boots[[k]]$yearmonth.half<-format(can.sing.boots[[k]]$tmrca.dt.half, "%Y-%m")
  
  #write for future use
  write.csv(can.sing.boots[[k]], sing.out[k])
  
} #end of k loop across bootstraps

```

## Make lineage colors based on what is in the meta
```{r}
#function to check colors in a vector
colortest<-function(vector){
  n<-length(vector)
  return(plot(1:n,1:n,col=vector,pch=15))
}

## copy meta and sum boots
meta.b1<-meta.boots[[1]]
sum.b1<-sum.boots[[1]]

#### make lineage colors #####
print('making lineage color scheme')

#identify lineages that were found in Canada
linz<-unique(meta.b1$Lineage[meta.b1$country=="Canada"])
linz2<-unique(sum.boots[[1]]$Lineage) #check if any extras
if(any(!linz2 %in% linz)){ #if any extras found in sum.boots(possible if predominant called-lineage)in the clade is not the one sampled in canada
  linz<-unique(c(linz,linz2))
}
n.linz<-length(linz)

## sort out aliases in new table
aliases<-data.frame(lineage=linz, alias=NA)
if(any(is.na(aliases$lineage))){
  aliases<-aliases[-which(is.na(aliases$lineage)),]
}
# nrow(aliases)

#if A.* or B.*, then no alias
for (i in 1:nrow(aliases)){
  if (aliases$lineage[i]=="A"|aliases$lineage[i]=="B") {
    aliases$alias[i]<-aliases$lineage[i];next}
  if(str_detect(aliases$lineage[i],"B\\.")){aliases$alias[i]<-aliases$lineage[i];next}
  if(str_detect(aliases$lineage[i],"A\\.")){aliases$alias[i]<-aliases$lineage[i];next}
}

#find patterns
sh<-which(is.na(aliases$alias))
prefixes<-c()
for (i in sh){
  pre<-first(unlist(strsplit(aliases$lineage[i],"\\.")))
  prefixes<-c(prefixes,pre)
}
prefixes<-unique(prefixes)

#could add more here - make exhaustive
pref.exch<-c("AE\\." = "B.1.1.306.",
             "AL\\." = "B.1.1.231.",
             "AY\\." = "B.1.617.2.",
             "AZ\\." = "B.1.1.318.",
             "C\\." = "B.1.1.1.",
             "P\\." = "B.1.1.28.",
             "Q\\." = "B.1.1.7.", #changed a mistake here - need to apply to all scripts
             "R\\." = "B.1.1.316.")

#lookup aliases by prefix
if(length(sh)>0){
  al.sh<-aliases[sh,]
  al.sh$alias<-as.vector(al.sh$lineage) %>% str_replace_all(pref.exch)                                                  
  #join back onto aliases
  aliases[sh,]<-al.sh  
}

# aliases
## test to see if all lineages have an alias
if(any(!is.na(aliases$alias))){print("All lineages have an alias, go you.")} else
{print("add more aliases, foo")}#should be true

#order numerically
aliases<-aliases[with(aliases, order(aliases$alias)),]
#add lineage group and color
aliases$group<-NA
aliases$col<-NA

#assign parent type/lineage group
for (i in 1:nrow(aliases)){
  if(str_detect(aliases$alias[i],"B.1.1\\.")){ty<-"B.1.1"} else
  {if(str_detect(aliases$alias[i],"B\\.")){ty<-"B.1"} else
    if(str_detect(aliases$alias[i],"A\\.")){ty<-"A"} }
  aliases$group[i]<-ty
}

#all lineages within the variant
id.varsub<-str_which(aliases$alias,focal.pango) 
#some general rules
if (length(id.varsub)>1){
  varsubcol<-rev(colorRampPalette(brewer.pal(n=9, "BuGn")[4:9])(length(id.varsub))) 
}
if(length(id.varsub)==1){
  varsubcol<-focal.col
}
aliases$col[id.varsub]<-varsubcol
colortest(aliases$col)

#export a tsv of name, alias, and hex color
write.table(aliases,paste(f.out,"lineagecolors.tsv",sep=""),sep="\t",row.names=T)

## make color scheme
lin.colz<-aliases$lineage
linPalette.ch<-as.character(aliases$col)
# linPalette.ch<-as.character(aliases$col)
names(linPalette.ch)<-lin.colz
LinColScale<-scale_colour_manual(name = "Location",values =linPalette.ch,na.value="grey60")
LinFillScale<-scale_fill_manual(name = "Location",values = linPalette.ch,na.value="grey60")

## Test colz
k=1
ggplot(sum.boots[[k]])+
  geom_point(aes(x=1:nrow(sum.boots[[k]]), y=1:nrow(sum.boots[[k]]), color=Lineage))+
  LinColScale+
  theme(legend.position = "none")
```

# overall number of sublins, descendants
```{r}
## export these into the text files

#number of sublins
sublin.tot<-c()
for (k in 1:n.B){
  sublin.tot<-c(sublin.tot,nrow(sum.boots[[k]]))
}
cat(paste0('Mean (95%CI) # of sublins = ',
       mean.95ci.X(sublin.tot,0)), 
    file=text.out,sep="\n",append=T)

# number unique pango lineages
un.lins<-c()
for (k in 1:n.B){
    un.lins<-c(un.lins, length(unique(sum.boots[[k]]$Lineage)))
  }

cat(paste0('Mean (95%CI) # of unique pango lins = ',
       mean.95ci.X(un.lins) ), 
    file=text.out,sep="\n",append=T)
#ALL DESC
desc.tot<-c()
for (k in 1:n.B){
  desc<-length(unique(sublin.long.unq[[k]]$Descendant.AccessionIDs))
  desc.tot<-c(desc.tot,desc)
}
cat(paste0('Mean (95%CI) # of descendants = ',
       mean.95ci.X(desc.tot,0)), 
    file=text.out,sep="\n",append=T)

#INTERNATIONAL DESC
desc.global.tot<-c()
for (k in 1:n.B){
  desc<-length(unique(sublin.long.unq[[k]]$Descendant.AccessionIDs [!sublin.long.unq[[k]]$State %in% provs]))
  desc.global.tot<-c(desc.global.tot,desc)
}
cat(paste0('Mean (95%CI) # of intl descendants = ',
       mean.95ci.X(desc.global.tot,0) ), 
    file=text.out,sep="\n",append=T)

#CANADIAN DESC
desc.can.tot<-c()
for (k in 1:n.B){
  desc<-length(unique(sublin.long.unq[[k]]$Descendant.AccessionIDs [sublin.long.unq[[k]]$State %in% provs]))
  desc.can.tot<-c(desc.can.tot,desc)
}
cat(paste0('Mean (95%CI) # of Can descendants = ',
       mean.95ci.X(desc.can.tot,0)  ), 
    file=text.out,sep="\n",append=T)

#look at tables of parental location
sort(table(sum.boots[[k]]$Parent.Location),decreasing = T)
range(sum.boots[[k]]$tmrca.dt,na.rm=T) #2020-12-04 to 2021-10-05

#sublins by location
sublin.focal.source<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]] %>%
    filter(Parent.Location%in%focal.source) %>%
    nrow()
  sublin.focal.source<-c(sublin.focal.source,x)}

cat(paste0('Mean (95% CI) # of ', focal.source.brf, '-origin variant sublineages = ',
       mean.95ci.X(sublin.focal.source,0) ), 
    file=text.out,sep="\n",append=T)

cat(paste0('Percent of ', focal.source.brf, '-origin variant sublineages = ',
       round((mean(sublin.focal.source)/mean(sublin.tot)*100),digits=1)  ), 
    file=text.out,sep="\n",append=T)

## HOW Many seqs were there from the focal source in the global context seqs
focal.seq<-c()
for (i in 1:length(sum.boots)){
  focal<-length(which(meta.boots[[i]]$country%in%focal.source))
  intl<-length(which(meta.boots[[i]]$country!="Canada"))
  focal.seq<-c(focal.seq,focal/intl*100)
}

cat(paste0("% of intl sequences from ",focal.source,": ",mean.95ci.X(focal.seq,1)),
    file=text.out,sep="\n",append=T)

#sublineage from USA
sublin.usa<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]] %>%
    filter(Parent.Location=="USA") %>%
    nrow()
  sublin.usa<-c(sublin.usa,x)}
cat(paste0('Mean (95% CI) # of USA-origin variant sublineages = ',
      mean.95ci.X(sublin.usa,0) ), 
    file=text.out,sep="\n",append=T)
cat(paste0('Percent of USA-origin variant sublineages = ',
      round((mean(sublin.usa)/mean(sublin.tot)*100 ),digits=2) ), 
    file=text.out,sep="\n",append=T)

## HOW Many seqs were there from the usa in the global context seqs
usa.seq<-c()
for (i in 1:length(sum.boots)){
  usa<-length(which(meta.boots[[i]]$country=="USA"))
  intl<-length(which(meta.boots[[i]]$country!="Canada"))
  usa.seq<-c(usa.seq,usa/intl*100)
}

cat(paste0("% of intl sequences from ","USA",": ",mean.95ci.X(usa.seq,1)),
    file=text.out,sep="\n",append=T)

## IF there was a variant-specific intervention (specified above)...
if(int.yn==T){
  #how many sublineages introduced before intervention
  sublin.preban<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt<as.Date(int.start)))
    sublin.preban<-c(sublin.preban,x)}
  cat(paste0('Mean (95%CI) # of sublins pre-intervention = ',
       mean.95ci.X(sublin.preban,0) ), 
    file=text.out,sep="\n",append=T)
  
  #how many sublineages introduced before intervention from focal
  sublin.preban1<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt<as.Date(int.start) &
                      sum.boots[[k]]$Parent.Location %in%focal.source))
    sublin.preban1<-c(sublin.preban1,x) }
   cat(paste0('Mean (95%CI) # of sublins pre-intervention from focal = ',
         mean.95ci.X(sublin.preban1,0)  ), 
    file=text.out,sep="\n",append=T) 
   
  ## What proportion of sublins from focal before the ban
  sublin.preban.prop<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt<as.Date(int.start) &
                      sum.boots[[k]]$Parent.Location %in%focal.source))
    y<-length(which(sum.boots[[k]]$tmrca.dt<as.Date(int.start)))
    prop<-x/y
    sublin.preban.prop<-c(sublin.preban.prop, prop) }
  cat(paste0('Proportion of sublins pre-intervention from focal = ',
         mean.95ci.X(sublin.preban.prop,2)  ), 
  file=text.out,sep="\n",append=T) 
   
  #DURING BAN
  sublin.duringban<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.start) &
                    sum.boots[[k]]$tmrca.dt<as.Date(int.end)  ))
    sublin.duringban<-c(sublin.duringban,x)}
   cat(paste0('Mean (95%CI) # of sublins during intervention = ',
        mean.95ci.X(sublin.duringban,0) ), 
    file=text.out,sep="\n",append=T) 
  
  #DURING BAN from focal.source
  sublin.duringban1<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.start) &
                    sum.boots[[k]]$tmrca.dt<as.Date(int.end) &
                      sum.boots[[k]]$Parent.Location %in%focal.source))
    sublin.duringban1<-c(sublin.duringban1,x) }
   cat(paste0('Mean (95%CI) # of sublins during intervention from focal = ',
         mean.95ci.X(sublin.duringban1,0)  ), 
    file=text.out,sep="\n",append=T) 
   
   #proportion of sublin during ban from focal
    sublin.durban.prop<-c()
    for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.start) &
                    sum.boots[[k]]$tmrca.dt<as.Date(int.end) &
                      sum.boots[[k]]$Parent.Location %in%focal.source))
    y<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.start) &
                    sum.boots[[k]]$tmrca.dt<as.Date(int.end) ))
    prop<-x/y
    sublin.durban.prop<-c(sublin.durban.prop, prop) }
   cat(paste0('Proportion of sublins during-intervention from focal = ',
         mean.95ci.X(sublin.durban.prop,2)  ), 
    file=text.out,sep="\n",append=T) 
   
   
  #AFTER BAN
  sublin.afterban<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.end)  ))
    sublin.afterban<-c(sublin.afterban,x)}
    cat(paste0('Mean (95%CI) # of sublins after intervention = ',
           mean.95ci.X(sublin.afterban,0)  ), 
    file=text.out,sep="\n",append=T) 
    
  #AFTER BAN from focal.source
  sublin.afterban1<-c()
  for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.end)  &
                      sum.boots[[k]]$Parent.Location %in%focal.source))
    sublin.afterban1<-c(sublin.afterban1,x)}
  cat(paste0('Mean (95%CI) # of sublins after intervention = ',
          mean.95ci.X(sublin.afterban1,0) ), 
    file=text.out,sep="\n",append=T) 
  
  #proportion of sublin after ban form focal
  sublin.afterban.prop<-c()
    for (k in 1:n.B){
    x<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.end) &
                      sum.boots[[k]]$Parent.Location %in%focal.source))
    y<-length(which(sum.boots[[k]]$tmrca.dt>=as.Date(int.end) ))
    prop<-x/y
    sublin.afterban.prop<-c(sublin.afterban.prop, prop) }
   cat(paste0('Proportion of sublins post-intervention from focal = ',
         mean.95ci.X(sublin.afterban.prop,2)  ), 
    file=text.out,sep="\n",append=T) 
   
  ###Fold change in proportion focal pre vs during
  sublin.fc.prop<-c()
  for (k in 1:n.B){
    x<-sublin.preban.prop[k]
    y<-sublin.durban.prop[k]
    fc<-x/y #if pre=0.5 and during=0.1, 0.5/0.1 = 5-fold reduction 
    sublin.fc.prop<-c(sublin.fc.prop, fc) }
  cat(paste0('Fold reduction in proportion of sublins pre vs during-intervention from focal = ',
       mean.95ci.X(sublin.fc.prop,2)  ), 
  file=text.out,sep="\n",append=T) 

  
} #end of if loop for int.yn=T


```

## closer look at the biggest sublineage 
```{r}
biggest.sub<-c()
for(k in 1:n.B){
  biggest.sub<-c(biggest.sub, sum.boots[[k]]$Sublineage2[which(sum.boots[[k]]$Number.Descendants==
                                                   max(sum.boots[[k]]$Number.Descendants))])
}
biggest<-first(names(sort(table(biggest.sub),decreasing = T)))

#simpler sublin name
biggest.sub.sh<-c()
for(k in 1:n.B){
  biggest.sub.sh<-c(biggest.sub.sh, sum.boots[[k]]$Sublineage[which(sum.boots[[k]]$Sublineage2==biggest)])
}
biggest.sh<-first(names(sort(table(biggest.sub.sh),decreasing = T)))
#this name is more variable across boots

#origin location
og.loc<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$Parent.Location[sum.boots[[k]]$Sublineage2==biggest]
  og.loc<-c(og.loc,x)}
og.loc.tab<-as.data.frame(table(og.loc) )
og.loc.tab<-og.loc.tab[with(og.loc.tab,order(og.loc.tab$Freq,decreasing = T)),]
og.loc.tab<-og.loc.tab%>% mutate(Perc=Freq/n.B*100)

#likelihood of origin
og.lik<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$Parent.Likelihood[sum.boots[[k]]$Sublineage2==biggest]
  og.lik<-c(og.lik,x)}

#calculate the mean lik for each origin location
og.loc.tab$MeanLik<-NA
for(i in 1:nrow(og.loc.tab)){
  og.loc.tab$MeanLik[i]<-mean(og.lik[which(og.loc==og.loc.tab$og.loc[i]) ])
}
og.loc.tab<-og.loc.tab %>% mutate(SupportLik=MeanLik*Perc/100)

#TMRCA
og.tmrca<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$tmrca.dt[sum.boots[[k]]$Sublineage2==biggest]
  og.tmrca<-c(og.tmrca,as.character(x))}
mean.95ci.X(as.Date(og.tmrca)) #""2021-02-03 (2021-02-01 - 2021-02-06)"

#first can sample date
og.first<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$FirstCanDateLSD[sum.boots[[k]]$Sublineage2==biggest]
  og.first<-c(og.first,as.character(x))}
mean.95ci.X(as.Date(og.first)) #"2021-06-29 (2021-06-20 - 2021-07-09)"

#introduced to:
dest.loc<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$Node.Location[sum.boots[[k]]$Sublineage2==biggest]
  dest.loc<-c(dest.loc,x)}
dest.loc.tab<-as.data.frame(table(dest.loc) )#focal.source 7, europe 3
dest.loc.tab<-dest.loc.tab[with(dest.loc.tab,order(dest.loc.tab$Freq,decreasing = T)),]
dest.loc.tab<-dest.loc.tab%>% mutate(Perc=Freq/n.B*100)

#likelihood of dest
dest.lik<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$Node.Likelihood[sum.boots[[k]]$Sublineage2==biggest]
  dest.lik<-c(dest.lik,x)}
# mean.95ci.X(dest.lik,2)
dest.loc.tab$MeanLik<-NA
for(i in 1:nrow(dest.loc.tab)){
  dest.loc.tab$MeanLik[i]<-mean(dest.lik[which(dest.loc==dest.loc.tab$dest.loc[i]) ])
}
dest.loc.tab<-dest.loc.tab %>% mutate(SupportLik=MeanLik*Perc/100)

#lienage support (% lineage composition) - hard to average across boots
sum.boots[[k]]$Lin.Support[sum.boots[[k]]$Sublineage2==biggest]
sum.boots[[k]]$Descendant.Location[sum.boots[[k]]$Sublineage2==biggest]

#detection lag
det.lag<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$detection.lag[sum.boots[[k]]$Sublineage2==biggest]
  det.lag<-c(det.lag,x)}
mean.95ci.X(det.lag,2)

#number desec
num.des<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$N.Desc.glob[sum.boots[[k]]$Sublineage2==biggest]
  num.des<-c(num.des,x)}
mean.95ci.X(num.des,0) 

#number desc can
num.des.c<-c()
for (k in 1:n.B){
  x<-sum.boots[[k]]$N.Desc.can[sum.boots[[k]]$Sublineage2==biggest]
  num.des.c<-c(num.des.c,x)}
mean.95ci.X(num.des.c,0) 

#what were the lineages of the first members in Canada
first.lin.can<-c()
for(k in 1:n.B){
  first<-first(sort(sublin.long.unq[[k]]$Date[sublin.long.unq[[k]]$Sublineage2==biggest]))
  lin<-sublin.long.unq[[k]]$Lineage[which(sublin.long.unq[[k]]$Sublineage2==biggest &
                                       sublin.long.unq[[k]]$Date == first)]
  first.lin.can<-c(first.lin.can,lin)
}
table(first.lin.can) #suggests it was p.1.14 before it came to can

#how many USA desc
us.desc<-c()
for(k in 1:n.B){
  x<-length(which(sublin.long.unq[[k]]$Sublineage2==biggest &
                 sublin.long.unq[[k]]$State=="USA"))
  us.desc<-c(us.desc,x)
}
mean.95ci.X(us.desc,0) 

focal.source.desc<-c()
for(k in 1:n.B){
  x<-length(which(sublin.long.unq[[k]]$Sublineage2==biggest &
                 sublin.long.unq[[k]]$State%in%focal.source))
  focal.source.desc<-c(focal.source.desc,x)
}
mean.95ci.X(focal.source.desc,0)

#what about if we pull unique gisaid IDs found in any of the sublin long for thsi sublin? How many desc
all.desc_biggest<-sublin.long.unq[[1]][sublin.long.unq[[1]]$Sublineage2==biggest,] #start with these
for(k in 2:n.B){
  #if unique, add to the data.frame
  df<-sublin.long.unq[[k]][sublin.long.unq[[k]]$Sublineage2==biggest,]
  new<-which(!df$Descendant.AccessionIDs %in% all.desc_biggest$Descendant.AccessionIDs)
  if(length(new>0)){
    all.desc_biggest<-bind_rows(all.desc_biggest,df[new,])
  }
}
nrow(all.desc_biggest) #1231

# length(unique(all.desc_biggest$Descendant.AccessionIDs))
sort(table(all.desc_biggest$State))
(sort(all.desc_biggest$Date))[1]
last(sort(all.desc_biggest$Date))
all.desc_biggest$State[all.desc_biggest$Date==(sort(all.desc_biggest$Date))[1]]
all.desc_biggest$Lineage[all.desc_biggest$Date==(sort(all.desc_biggest$Date))[1]]

## EXPORT this info
cat(paste0("Biggest sublineage = ",biggest), 
    paste0("Biggest sublineage, short = ",biggest.sh), 
    paste0("Most likely origin: ",og.loc.tab$og.loc[1],
           ", % support: ",round(og.loc.tab$Perc[1],2),
            ", Mean lik: ",round(og.loc.tab$MeanLik[1],2),
            ", Support*lik: ",round(og.loc.tab$SupportLik[1],2)),
    paste0("Origin TMRCA mean (95%CI): ", mean.95ci.X(as.Date(og.tmrca)) ),
    paste0("First sample in Can, mean (95%CI): ", mean.95ci.X(as.Date(og.first))),
    paste0("Most likely destination: ",dest.loc.tab$dest.loc[1],
           ", % support: ",round(dest.loc.tab$Perc[1],2),
            ", Mean lik: ",round(dest.loc.tab$MeanLik[1],2),
            ", Support*lik: ",round(dest.loc.tab$SupportLik[1],2)),
    paste0("Detection lag [days]: ", mean.95ci.X(det.lag,2)),
    paste0("Number desc in boot: ", mean.95ci.X(num.des,0) ),
    paste0("Number desc in boot in Can: ", mean.95ci.X(num.des.c,0) ),
    paste0("Number desc in boot in US: ", mean.95ci.X(us.desc,0) ),
    paste0("Number desc in boot in focal source: ", mean.95ci.X(focal.source.desc,0)),
    paste0("n unique desc across boots: ", nrow(all.desc_biggest)),
    file=text.out, append=T,sep="\n")
```

# Sublineage summaries: Alluvial, mosaic, sizes
## summarize the total number and percent of sublins by 1) par. loc, 2) prov of intro, 3) lineage, 4) period
```{r}
#prep list item
sum.Par.Prov.l<-replicate(n.B,vector())

#go through each list item/subsample
## tabulate instances of location pairs
for (k in 1:n.B){
  sum.Par.Prov.l[[k]]<-sum.boots[[k]] %>% dplyr::group_by (Parent.Location, Node.Location) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)

  ## add a column for subsample
  sum.Par.Prov.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])

}

## rbind the summaries
sum.Par.Prov<-bind_rows(sum.Par.Prov.l) 

#summarize the mean and range for each of 1), 2) and 3)
sum.Par.Prov.summary<-sum.Par.Prov %>% dplyr::group_by(Parent.Location, Node.Location) %>%
  dplyr::summarize(.groups="rowwise",
                   mean.n=round(mean(n.Par)),
                   sd.n=round(sd(n.Par,na.rm=T),digits=2),
                   mean.perc=round(mean(perc.Par),digits=2),
                   sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
  as.data.frame()

tot.Par<-sum.Par.Prov.summary %>% dplyr::group_by(Parent.Location) %>%
  dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
  as.data.frame()

## Alluvial plot for Figure 2
#make a "subject column"
sum.Par.Prov.summary$subject<-1:nrow(sum.Par.Prov.summary)
# sum.Par.Prov.summary<-sum.Par.Prov.summary[-which(sum.Par.Prov.summary$mean.n<1),]

#order the geos
ord.count<-sum.Par.Prov.summary%>% dplyr::group_by(Parent.Location) %>%
  dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
sum.Par.Prov.summary$Parent.Location<-factor(sum.Par.Prov.summary$Parent.Location,levels=ord.count)

ord.prov<-sum.Par.Prov.summary%>% dplyr::group_by(Node.Location) %>%
  dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
sum.Par.Prov.summary$Node.Location<-factor(sum.Par.Prov.summary$Node.Location,levels=ord.prov)


#make it long
sum.Par.Prov.summary.long<-sum.Par.Prov.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
#order the geo types
sum.Par.Prov.summary.long$geo.type<-factor(sum.Par.Prov.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))

# sum.Par.Prov.summary.long$geo[which(!sum.Par.Prov.summary.long$geo %in% names(globalPalette.ch))]

#BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
sum.Par.Prov.summary.long$metric<-sum.Par.Prov.summary.long$mean.perc
matchy.origin<-which(sum.Par.Prov.summary.long$geo.type=="Origin")
sum.origin<-sum(sum.Par.Prov.summary.long$mean.perc[matchy.origin])
sum.Par.Prov.summary.long$metric[matchy.origin]<-sum.Par.Prov.summary.long$mean.perc[matchy.origin]/sum.origin*100
# sum(sum.Par.Prov.summary.long$metric[matchy.origin]) #should be 100
matchy.prov<-which(sum.Par.Prov.summary.long$geo.type=="Destination")
sum.prov<-sum(sum.Par.Prov.summary.long$mean.perc[matchy.prov])
sum.Par.Prov.summary.long$metric[matchy.prov]<-sum.Par.Prov.summary.long$mean.perc[matchy.prov]/sum.prov*100
# sum(sum.Par.Prov.summary.long$metric[matchy.prov]) 

```

## Alluvial plot: sublineages
```{r}
P1<- ggplot(sum.Par.Prov.summary.long,
       aes(x = geo.type, stratum = geo, alluvium = subject,
           y = metric,
           fill = geo, label = geo)) +
  scale_x_discrete(expand = c(0.01,0.01)) +
  scale_y_continuous(expand = c(0,0)) +
  geom_flow(alpha = 0.6,width=0.45) +
  geom_stratum(alpha = 0.9,width=0.45) +
  geom_text(stat = "stratum", size = 3.4,min.y=1,fontface="bold") +
  pubTheme+
  theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
        axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)))+
  labs(x=NULL,y=paste0("% sublineages"))+
  GlobFillScale
P1
ggsave(paste(f.out,"Alluvial.percent.Parent.Node.png",sep=""),height=5,width=5,units="in")

P1.count<- ggplot(sum.Par.Prov.summary.long,
       aes(x = geo.type, stratum = geo, alluvium = subject,
           y = mean.n,
           fill = geo, label = geo)) +
  scale_x_discrete(expand = c(0.01,0.01)) +
  scale_y_continuous(expand = c(0,0)) +
  geom_flow(alpha = 0.6,width=0.45) +
  geom_stratum(alpha = 0.9,width=0.45) +
  geom_text(stat = "stratum", size = 3.4,min.y=1,fontface="bold") +
  pubTheme+
  theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
        axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)))+
  labs(x=NULL,y=paste0("# sublineages"))+
  GlobFillScale
P1.count
ggsave(paste(f.out,"Alluvial.Count.Parent.Node.png",sep=""),height=5,width=5,units="in")

```

## If there was an intervention: Repeat with pre-; during-; post-intervention
print the csv for these proportions 
```{r}
if(int.yn==T){
  
  #prep list item
  sum.Par.Prov.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.l[[k]]<-sum.boots[[k]] %>%
      filter(tmrca.dt<as.Date(int.start)) %>%
      dplyr::group_by (Parent.Location, Node.Location) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  ## rbind the summaries
  sum.Par.Prov<-bind_rows(sum.Par.Prov.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.summary<-sum.Par.Prov %>% dplyr::group_by(Parent.Location, Node.Location) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  tot.Par<-sum.Par.Prov.summary %>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
    as.data.frame()
  
  ## Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.summary$subject<-1:nrow(sum.Par.Prov.summary)
  # sum.Par.Prov.summary<-sum.Par.Prov.summary[-which(sum.Par.Prov.summary$mean.n<1),]
  
  sum.Par.Prov.summary$Node.Location<-str_replace_all(sum.Par.Prov.summary$Node.Location,"British Columbia","British\nColumbia")
  sum.Par.Prov.summary$Parent.Location<-str_replace_all(sum.Par.Prov.summary$Parent.Location,focal.source,focal.source)
  
  #order the geos
  ord.count<-sum.Par.Prov.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.summary$Parent.Location<-factor(sum.Par.Prov.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.summary$Node.Location<-factor(sum.Par.Prov.summary$Node.Location,levels=ord.prov)
  
  #make it long
  sum.Par.Prov.summary.long<-sum.Par.Prov.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sum.Par.Prov.summary.long$geo.type<-factor(sum.Par.Prov.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  
  
  # sum.Par.Prov.summary.long$geo[which(!sum.Par.Prov.summary.long$geo %in% names(globalPalette.ch))]
  
  #BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
  sum.Par.Prov.summary.long$metric<-sum.Par.Prov.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.summary.long$metric[matchy.origin]<-sum.Par.Prov.summary.long$mean.perc[matchy.origin]/sum.origin*100
  # sum(sum.Par.Prov.summary.long$metric[matchy.origin]) #should be 100
  matchy.prov<-which(sum.Par.Prov.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.summary.long$metric[matchy.prov]<-sum.Par.Prov.summary.long$mean.perc[matchy.prov]/sum.prov*100
  # sum(sum.Par.Prov.summary.long$metric[matchy.prov])
  #Modified palette with weird names TODO
  globalPalette.ch.mod<-globalPalette.ch
  names(globalPalette.ch.mod)<-str_replace_all(names(globalPalette.ch.mod),c("British Columbia"="British\nColumbia",focal.source=focal.source))
  GlobFillScale.mod<-scale_fill_manual(name = "Location",values = globalPalette.ch.mod,na.value="grey60")
  GlobColScale.mod<-scale_color_manual(name = "Location",values = globalPalette.ch.mod,na.value="grey60")
  
  
  ## Alluvial plot
  P1.wave1<- ggplot(sum.Par.Prov.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = mean.n,
             fill = geo, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(alpha = 0.6,width=0.55) +
    geom_stratum(alpha = 0.8,width=0.55) +
    geom_text(stat = "stratum", size = 3.4,min.y=2,fontface="bold") +
    pubTheme+
    theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
          plot.margin=margin(4,4,4,4,unit="pt"))+
    labs(x=NULL,y=paste0("# sublineages pre-intervention"))+
    GlobFillScale.mod
  P1.wave1
  # ggsave(paste(f.out,"WAVE1.Alluvial.percent.Parent.Node.png",sep=""),height=5,width=5,units="in")
  
  ### WAVE 2 DURING BAN####
  #prep list item
  sum.Par.Prov.2.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.2.l[[k]]<-sum.boots[[k]] %>%
      filter(tmrca.dt>=as.Date(int.start)) %>%
      filter(tmrca.dt<=as.Date(int.end)) %>%
      dplyr::group_by (Parent.Location, Node.Location) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.2.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  ## rbind the summaries
  sum.Par.Prov<-bind_rows(sum.Par.Prov.2.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.2.summary<-sum.Par.Prov %>% dplyr::group_by(Parent.Location, Node.Location) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  tot.Par<-sum.Par.Prov.2.summary %>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
    as.data.frame()
  
  ## Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.2.summary$subject<-1:nrow(sum.Par.Prov.2.summary)
  # sum.Par.Prov.2.summary<-sum.Par.Prov.2.summary[-which(sum.Par.Prov.2.summary$mean.n<1),]
  
  
  sum.Par.Prov.2.summary$Node.Location<-str_replace_all(sum.Par.Prov.2.summary$Node.Location,"British Columbia","British\nColumbia")
  sum.Par.Prov.2.summary$Parent.Location<-str_replace_all(sum.Par.Prov.2.summary$Parent.Location,focal.source,focal.source)
  
  #order the geos
  ord.count<-sum.Par.Prov.2.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.2.summary$Parent.Location<-factor(sum.Par.Prov.2.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.2.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.2.summary$Node.Location<-factor(sum.Par.Prov.2.summary$Node.Location,levels=ord.prov)
  
  
  #make it long
  sum.Par.Prov.2.summary.long<-sum.Par.Prov.2.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sum.Par.Prov.2.summary.long$geo.type<-factor(sum.Par.Prov.2.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  
  # sum.Par.Prov.2.summary.long$geo[which(!sum.Par.Prov.2.summary.long$geo %in% names(globalPalette.ch))]
  
  #BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
  sum.Par.Prov.2.summary.long$metric<-sum.Par.Prov.2.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.2.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.2.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.2.summary.long$metric[matchy.origin]<-sum.Par.Prov.2.summary.long$mean.perc[matchy.origin]/sum.origin*100
  # sum(sum.Par.Prov.2.summary.long$metric[matchy.origin]) #should be 100
  matchy.prov<-which(sum.Par.Prov.2.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.2.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.2.summary.long$metric[matchy.prov]<-sum.Par.Prov.2.summary.long$mean.perc[matchy.prov]/sum.prov*100
  # sum(sum.Par.Prov.2.summary.long$metric[matchy.prov])
  ## Alluvial plot
  
  P1.wave2<- ggplot(sum.Par.Prov.2.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = mean.n,
             fill = geo, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(alpha = 0.6,width=0.55) +
    geom_stratum(alpha = 0.8,width=0.55) +
    geom_text(stat = "stratum", size = 3.4,min.y=2,fontface="bold") +
    pubTheme+
    theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
                  plot.margin=margin(4,4,4,4,unit="pt"))+
    labs(x=NULL,y=paste0("# sublineages during intervention"))+
    GlobFillScale.mod
  P1.wave2
  # ggsave(paste(f.out,"WAVE2.Alluvial.percent.Parent.Node.png",sep=""),height=8,width=5,units="in")
  
  
  ### WAVE 3 AFTER BAN####
  #prep list item
  sum.Par.Prov.3.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.3.l[[k]]<-sum.boots[[k]] %>%
      filter(tmrca.dt>as.Date(int.end)) %>%
      dplyr::group_by (Parent.Location, Node.Location) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.3.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  ## rbind the summaries
  sum.Par.Prov<-bind_rows(sum.Par.Prov.3.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.3.summary<-sum.Par.Prov %>% dplyr::group_by(Parent.Location, Node.Location) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  tot.Par<-sum.Par.Prov.3.summary %>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
    as.data.frame()
  
  ## Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.3.summary$subject<-1:nrow(sum.Par.Prov.3.summary)
  # sum.Par.Prov.3.summary<-sum.Par.Prov.3.summary[-which(sum.Par.Prov.3.summary$mean.n<1),]
  
  
  sum.Par.Prov.3.summary$Node.Location<-str_replace_all(sum.Par.Prov.3.summary$Node.Location,"British Columbia","British\nColumbia")
  sum.Par.Prov.3.summary$Parent.Location<-str_replace_all(sum.Par.Prov.3.summary$Parent.Location,focal.source,focal.source)
  
  #order the geos
  ord.count<-sum.Par.Prov.3.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.3.summary$Parent.Location<-factor(sum.Par.Prov.3.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.3.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.3.summary$Node.Location<-factor(sum.Par.Prov.3.summary$Node.Location,levels=ord.prov)
  
  
  #make it long
  sum.Par.Prov.3.summary.long<-sum.Par.Prov.3.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sum.Par.Prov.3.summary.long$geo.type<-factor(sum.Par.Prov.3.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  
  # sum.Par.Prov.3.summary.long$geo[which(!sum.Par.Prov.3.summary.long$geo %in% names(globalPalette.ch))]
  
  #BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
  sum.Par.Prov.3.summary.long$metric<-sum.Par.Prov.3.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.3.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.3.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.3.summary.long$metric[matchy.origin]<-sum.Par.Prov.3.summary.long$mean.perc[matchy.origin]/sum.origin*100
  # sum(sum.Par.Prov.3.summary.long$metric[matchy.origin]) #should be 100
  matchy.prov<-which(sum.Par.Prov.3.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.3.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.3.summary.long$metric[matchy.prov]<-sum.Par.Prov.3.summary.long$mean.perc[matchy.prov]/sum.prov*100
  # sum(sum.Par.Prov.3.summary.long$metric[matchy.prov])
  ## Alluvial plot
  
  P1.wave3<- ggplot(sum.Par.Prov.3.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = mean.n,
             fill = geo, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(alpha = 0.6,width=0.55) +
    geom_stratum(alpha = 0.8,width=0.55) +
    geom_text(stat = "stratum", size = 3.4,min.y=2,fontface="bold") +
    pubTheme+
    theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
                  plot.margin=margin(4,4,4,4,unit="pt"))+
    labs(x=NULL,y=paste0("# sublineages post-intervention"))+
    GlobFillScale.mod
  P1.wave3

  #grob plot of both waves
  plot_grid(P1.wave1,P1.wave2,P1.wave3,nrow=1,labels=c("A","B","C"))
  ggsave(paste(f.out,"SepWaves.Alluvial.percent.Parent.Node.png",sep=""),height=5,width=11,units="in")
}
```

## Alluvial stratified by lineage, then by intervention period
```{r}
#if there are more than one unique pango lineage among sublin designations...  
if(mean(un.lins)>1){
  #prep list item
  sum.Par.Prov.L.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.L.l[[k]]<-sum.boots[[k]] %>% 
      dplyr::group_by (Parent.Location, Node.Location, Lineage) %>% 
      dplyr::summarize (.groups="rowwise", n.Par= n()) %>% 
      as.data.frame() %>% 
      mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.L.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  sum.Par.Prov.L<-bind_rows(sum.Par.Prov.L.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.L.summary<-sum.Par.Prov.L %>% dplyr::group_by(Parent.Location, Node.Location, Lineage) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  # head(sum.Par.Prov.L.summary)
  ### Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.L.summary$subject<-1:nrow(sum.Par.Prov.L.summary)
  sum.Par.Prov.L.summary$Node.Location<-str_replace_all(sum.Par.Prov.L.summary$Node.Location,"British Columbia","British\nColumbia")
  
  #order the geos by frequency
  ord.count<-sum.Par.Prov.L.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.L.summary$Parent.Location<-factor(sum.Par.Prov.L.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.L.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.L.summary$Node.Location<-factor(sum.Par.Prov.L.summary$Node.Location,levels=ord.prov)
  
  #make it long
  sum.Par.Prov.L.summary.long<-sum.Par.Prov.L.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo") %>% as.data.frame()
  
  #order the geo types and lineages
  sum.Par.Prov.L.summary.long$geo.type<-factor(sum.Par.Prov.L.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  sum.Par.Prov.L.summary.long$Lineage<-factor(sum.Par.Prov.L.summary.long$Lineage,levels=aliases$lineage)
  
  #re-scale to perfect 100
  sum.Par.Prov.L.summary.long$metric<-sum.Par.Prov.L.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.L.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.L.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.L.summary.long$metric[matchy.origin]<-sum.Par.Prov.L.summary.long$mean.perc[matchy.origin]/sum.origin*100
  sum(sum.Par.Prov.summary.long$metric[matchy.origin],na.rm=T) #should be 100
  matchy.prov<-which(sum.Par.Prov.L.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.L.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.L.summary.long$metric[matchy.prov]<-sum.Par.Prov.L.summary.long$mean.perc[matchy.prov]/sum.prov*100
  
  #plot it
  alluv.lin<-ggplot(sum.Par.Prov.L.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    geom_stratum(alpha = 0.4,width=0.4 )+
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.3,min.y=2,label.padding=unit(0.1, "lines")) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.5,min.y=1,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
          axis.title.y=element_text(vjust=-2),
          plot.margin=margin(4,4,4,1,unit="pt"))+
    labs(x=NULL,y=paste0("% sublineages"))
  alluv.lin
  
  ggsave(paste(f.out,"/Alluvial.Perc.Parent.Node.Lineage.png",sep=""),height=5,width=5,units="in")
  
  #plot the count instead of perc
  alluv.lin.n<-ggplot(sum.Par.Prov.L.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = mean.n, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    geom_stratum(alpha = 0.4,width=0.41 )+
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.3,min.y=1,label.padding=unit(0.1, "lines")) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.5,min.y=1,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
          axis.title.y=element_text(vjust=-2),
          plot.margin=margin(4,4,4,1,unit="pt"))+
    labs(x=NULL,y=paste0("# sublineages "))
  alluv.lin.n
  ggsave(paste(f.out,"/Alluvial.Count.Parent.Node.Lin.png",sep=""),height=5,width=5,units="in")

}

#and if there was a var-specific intervention, stratify by intervetion period
if(mean(un.lins)>1 & int.yn==T){
  ### BEFORE ####
  #summarize the total number and percent of intros by 1) par. loc, 2) prov of intro, maybe also 3) lineage
  sum.Par.Prov.L1.l<-replicate(n.B,vector())
  
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.L1.l[[k]]<-sum.boots[[k]] %>%
      filter(tmrca.dt<as.Date(int.start)) %>%
      dplyr::group_by (Parent.Location, Node.Location, Lineage) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.L1.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  }
  
  sum.Par.Prov.L<-bind_rows(sum.Par.Prov.L1.l)

  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.L1.summary<-sum.Par.Prov.L %>% dplyr::group_by(Parent.Location, Node.Location, Lineage) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  ### Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.L1.summary$subject<-1:nrow(sum.Par.Prov.L1.summary)
  sum.Par.Prov.L1.summary$Node.Location<-str_replace_all(sum.Par.Prov.L1.summary$Node.Location,"British Columbia","British\nColumbia")

  #order the geos by frequency
  ord.count<-sum.Par.Prov.L1.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.L1.summary$Parent.Location<-factor(sum.Par.Prov.L1.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.L1.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.L1.summary$Node.Location<-factor(sum.Par.Prov.L1.summary$Node.Location,levels=ord.prov)
  
  #make it long
  sum.Par.Prov.L1.summary.long<-sum.Par.Prov.L1.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sum.Par.Prov.L1.summary.long$geo.type<-factor(sum.Par.Prov.L1.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  sum.Par.Prov.L1.summary.long$Lineage<-factor(sum.Par.Prov.L1.summary.long$Lineage,levels=aliases$lineage)
  
  #re-scale
  sum.Par.Prov.L1.summary.long$metric<-sum.Par.Prov.L1.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.L1.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.L1.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.L1.summary.long$metric[matchy.origin]<-sum.Par.Prov.L1.summary.long$mean.perc[matchy.origin]/sum.origin*100
  sum(sum.Par.Prov.summary.long$metric[matchy.origin],na.rm=T) #should be 100
  matchy.prov<-which(sum.Par.Prov.L1.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.L1.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.L1.summary.long$metric[matchy.prov]<-sum.Par.Prov.L1.summary.long$mean.perc[matchy.prov]/sum.prov*100
  
  #plot it
  alluv.w1.lin<-ggplot(sum.Par.Prov.L1.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    # new_scale("fill")+ #sneaky
    geom_stratum(alpha = 0.4,width=0.4) +
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.1,min.y=2) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.5,min.y=1,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.55,0.7)),
          axis.title.y=element_text(vjust=-2),
          plot.margin=margin(4,4,4,1,unit="pt"))+ #accommodate the lineage group image
    labs(x=NULL,y=paste0("% sublineages before intervention"))
  alluv.w1.lin
  ggsave(paste(f.out,"/Alluvial.Sublin.Before.Parent.Node.Lin.png",sep=""),height=5,width=5,units="in")
  
  ### DURING ###
  #summarize the total number and percent of intros by 1) par. loc, 2) prov of intro, maybe also 3) lineage
  #prep list item
  sum.Par.Prov.L2.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.L2.l[[k]]<-sum.boots[[k]] %>%
      filter(tmrca.dt>=as.Date(int.start)) %>%
      filter(tmrca.dt<=as.Date(int.end)) %>%
      dplyr::group_by (Parent.Location, Node.Location, Lineage) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.L2.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  }
  sum.Par.Prov.L<-bind_rows(sum.Par.Prov.L2.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.L2.summary<-sum.Par.Prov.L %>% dplyr::group_by(Parent.Location, Node.Location, Lineage) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  ### Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.L2.summary$subject<-1:nrow(sum.Par.Prov.L2.summary)
  # sum.Par.Prov.L2.summary<-sum.Par.Prov.L2.summary[-which(sum.Par.Prov.L2.summary$mean.n<1),]
  
  sum.Par.Prov.L2.summary$Node.Location<-str_replace_all(sum.Par.Prov.L2.summary$Node.Location,"British Columbia","British\nColumbia")
  sum.Par.Prov.L2.summary$Parent.Location<-str_replace_all(sum.Par.Prov.L2.summary$Parent.Location,focal.source,focal.source)
  
  #order the geos by frequency
  ord.count<-sum.Par.Prov.L2.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.L2.summary$Parent.Location<-factor(sum.Par.Prov.L2.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.L2.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.L2.summary$Node.Location<-factor(sum.Par.Prov.L2.summary$Node.Location,levels=ord.prov)
  
  #make it long
  sum.Par.Prov.L2.summary.long<-sum.Par.Prov.L2.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sum.Par.Prov.L2.summary.long$geo.type<-factor(sum.Par.Prov.L2.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  sum.Par.Prov.L2.summary.long$Lineage<-factor(sum.Par.Prov.L2.summary.long$Lineage,levels=aliases$lineage)
  
  #re-scale
  sum.Par.Prov.L2.summary.long$metric<-sum.Par.Prov.L2.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.L2.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.L2.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.L2.summary.long$metric[matchy.origin]<-sum.Par.Prov.L2.summary.long$mean.perc[matchy.origin]/sum.origin*100
  sum(sum.Par.Prov.summary.long$metric[matchy.origin],na.rm=T) #should be 100
  matchy.prov<-which(sum.Par.Prov.L2.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.L2.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.L2.summary.long$metric[matchy.prov]<-sum.Par.Prov.L2.summary.long$mean.perc[matchy.prov]/sum.prov*100
  
  #plot it
  alluv.w2.lin<-ggplot(sum.Par.Prov.L2.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    # new_scale("fill")+ #sneaky
    geom_stratum(alpha = 0.4,width=0.4) +
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.1,min.y=2) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.5,min.y=1,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.55,0.7)),
          axis.title.y=element_text(vjust=-2),
          plot.margin=margin(4,4,4,1,unit="pt"))+ #accommodate the lineage group image
    labs(x=NULL,y=paste0("% sublineages during intervention"))
  alluv.w2.lin
  #
  ggsave(paste(f.out,"/Alluvial.Sublin.during.Parent.Node.Lin.png",sep=""),height=5,width=5,units="in")

  ### after ###
  #summarize the total number and percent of intros by 1) par. loc, 2) prov of intro, maybe also 3) lineage
  #prep list item
  sum.Par.Prov.L3.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sum.Par.Prov.L3.l[[k]]<-sum.boots[[k]] %>%
      filter(tmrca.dt>as.Date(int.end)) %>%
      dplyr::group_by (Parent.Location, Node.Location, Lineage) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sum.Par.Prov.L3.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  }
  sum.Par.Prov.L<-bind_rows(sum.Par.Prov.L3.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sum.Par.Prov.L3.summary<-sum.Par.Prov.L %>% dplyr::group_by(Parent.Location, Node.Location, Lineage) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  ### Alluvial plot for Figure 2
  #make a "subject column"
  sum.Par.Prov.L3.summary$subject<-1:nrow(sum.Par.Prov.L3.summary)
  # sum.Par.Prov.L3.summary<-sum.Par.Prov.L3.summary[-which(sum.Par.Prov.L3.summary$mean.n<1),]
  
  sum.Par.Prov.L3.summary$Node.Location<-str_replace_all(sum.Par.Prov.L3.summary$Node.Location,"British Columbia","British\nColumbia")
  sum.Par.Prov.L3.summary$Parent.Location<-str_replace_all(sum.Par.Prov.L3.summary$Parent.Location,focal.source,focal.source)
  
  #order the geos by frequency
  ord.count<-sum.Par.Prov.L3.summary%>% dplyr::group_by(Parent.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
  sum.Par.Prov.L3.summary$Parent.Location<-factor(sum.Par.Prov.L3.summary$Parent.Location,levels=ord.count)
  
  ord.prov<-sum.Par.Prov.L3.summary%>% dplyr::group_by(Node.Location) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
  sum.Par.Prov.L3.summary$Node.Location<-factor(sum.Par.Prov.L3.summary$Node.Location,levels=ord.prov)
  
  #make it long
  sum.Par.Prov.L3.summary.long<-sum.Par.Prov.L3.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sum.Par.Prov.L3.summary.long$geo.type<-factor(sum.Par.Prov.L3.summary.long$geo.type,levels=c("Parent.Location","Node.Location"),labels=c("Origin","Destination"))
  sum.Par.Prov.L3.summary.long$Lineage<-factor(sum.Par.Prov.L3.summary.long$Lineage,levels=aliases$lineage)
  
  #re-scale
  sum.Par.Prov.L3.summary.long$metric<-sum.Par.Prov.L3.summary.long$mean.perc
  matchy.origin<-which(sum.Par.Prov.L3.summary.long$geo.type=="Origin")
  sum.origin<-sum(sum.Par.Prov.L3.summary.long$mean.perc[matchy.origin])
  sum.Par.Prov.L3.summary.long$metric[matchy.origin]<-sum.Par.Prov.L3.summary.long$mean.perc[matchy.origin]/sum.origin*100
  sum(sum.Par.Prov.summary.long$metric[matchy.origin],na.rm=T) #should be 100
  matchy.prov<-which(sum.Par.Prov.L3.summary.long$geo.type=="Destination")
  sum.prov<-sum(sum.Par.Prov.L3.summary.long$mean.perc[matchy.prov])
  sum.Par.Prov.L3.summary.long$metric[matchy.prov]<-sum.Par.Prov.L3.summary.long$mean.perc[matchy.prov]/sum.prov*100
  
  #plot it
  alluv.w3.lin<-ggplot(sum.Par.Prov.L3.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    # new_scale("fill")+ #sneaky
    geom_stratum(alpha = 0.4,width=0.4) +
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.1,min.y=2) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.5,min.y=1,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.55,0.7)),
          axis.title.y=element_text(vjust=-2),
          plot.margin=margin(4,4,4,1,unit="pt"))+ #accommodate the lineage group image
    labs(x=NULL,y=paste0("% sublineages after intervention"))
  alluv.w3.lin
  #
  ggsave(paste(f.out,"/Alluvial.Sublin.after.Parent.Node.Lin.png",sep=""),height=5,width=5,units="in")  
  
  #composite of both lineage-spec wave-spec flows
  linwaves<-plot_grid(alluv.w1.lin,alluv.w2.lin,alluv.w3.lin,labels=c("A","B","C"),nrow=1)
  linwaves
  ggsave(paste(f.out,"/Alluvial.Sublin.AllPeriods.Parent.Node.Lin.png",sep=""),height=5,width=8,units="in")  
}
```

## Summary of sublineage size, global and Can desc, histogram
```{r}
#first need a way of comparing sublineages... cross reference to see how common
sublin.df<-data.frame(Sublineage=NA,Sublineage2=NA,Lineage=NA,N.Desc.glob=NA,
                      SampleSet=NA)
for (k in 1:n.B){
  sum.boots[[k]]$SampleSet<-k
  #add sublineages to the dataframe
  sublin.df<-rbind(sublin.df,sum.boots[[k]][,c("Sublineage","SublineageUnq","Lineage","N.Desc.glob", "SampleSet")])
}
sublin.df<-sublin.df[-1,] #NA row from rbind

k=1 
#Bin sublineage size by frequency
sum.boots.binSize<-sum.boots[[k]] %>% 
  group_by(N.Desc.glob,Lineage) %>% 
  dplyr::summarize(.groups="rowwise",n=n())
#bin the larger sizes
bin.sublinsize<-function(df){
  df$Number.Desc.Factor<-as.character("NA")
  for (i in 1:nrow(df)){
    if (df$N.Desc.glob[i]<10){
      df$Number.Desc.Factor[i]<-as.character(df$N.Desc.glob[i]);next}
    if (df$N.Desc.glob[i]>=10 & df$N.Desc.glob[i]<20){
      df$Number.Desc.Factor[i]<-as.character("10-19");next}
    if (df$N.Desc.glob[i]>=20 &df$N.Desc.glob[i]<30){
      df$Number.Desc.Factor[i]<-as.character("20-29");next}
    if (df$N.Desc.glob[i]>=30 & df$N.Desc.glob[i]<40){
      df$Number.Desc.Factor[i]<-as.character("30-39");next}
    if (df$N.Desc.glob[i]>=40 &df$N.Desc.glob[i]<50){
      df$Number.Desc.Factor[i]<-as.character("40-49");next}
    if (df$N.Desc.glob[i]>=50 & df$N.Desc.glob[i]<100){
      df$Number.Desc.Factor[i]<-as.character("50-99");next}
    if (df$N.Desc.glob[i]>=100 &df$N.Desc.glob[i]<1000){
      df$Number.Desc.Factor[i]<-as.character("100-999");next}
    if (df$N.Desc.glob[i]>=1000 & df$N.Desc.glob[i]<5000){
      df$Number.Desc.Factor[i]<-as.character("1000-4999");next}
    if (df$N.Desc.glob[i]>=5000){
      df$Number.Desc.Factor[i]<-as.character("5000+");next}
  }
  df$Number.Desc.Factor<-factor(df$Number.Desc.Factor, levels=c(as.character(2:9),'10-19','20-29','30-39','40-49','50-99','100-999','1000-4999',"5000+"))
  df$Lineage<-factor(df$Lineage,levels=rev(aliases$lineage))
  return(df)
}

#use function:
sum.boots.binSize<-bin.sublinsize(sum.boots.binSize)

#make a summary df of n in each category for text and pos
sum.boots.binSize.summ<-sum.boots.binSize%>% dplyr::group_by(Number.Desc.Factor) %>%
  dplyr::summarise(.groups="rowwise",total.subs=sum(n)) %>% as.data.frame()

#PLOT IT 
# sum(sum.boots.binSize$n) #685
P3<-ggplot(data=sum.boots.binSize)+
  geom_bar(aes(x=as.factor(Number.Desc.Factor),y=n, group=Lineage,fill=Lineage),
               position="stack",stat="identity",alpha=0.9)+
  pubTheme+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_text(angle=45,vjust=1,hjust=1,size=rel(1.2)),
        # legend.position = c(0.7,0.8),
        legend.position = "none",
        legend.text = element_text(size=9),
        # axis.title.y=element_text(vjust=4),       
        axis.title.y=element_text(vjust=1,size=rel(1.2)), 
        plot.margin=unit(c(4,4,4,4),"pt"))+
  LinFillScale+
  labs(x="Sublineage size",y="# sublineages")+
  guides(fill=guide_legend(ncol= 1,title="Lineage",title.position="top",reverse = T, keywidth = 1.2,keyheight = 1.2))+
  geom_text(data=sum.boots.binSize.summ,aes(y=total.subs,x=Number.Desc.Factor,label=paste(total.subs,sep=""),hjust=0.5,vjust=-0.3),size=2.7)+  
  scale_y_continuous(limits=c(0,71),expand=c(0.02,0))
  # annotate("rect",xmin=2.5,xmax=16.4,ymin=166,ymax=202, fill="white")

P3
ggsave(paste(f.out,"/Sublineagesizes.bylin.Frequencies.png",sep=""),width=4,height=4,units="in")

```

## Alternative figure for sublineage size using treemap tiles
```{r}
k=1
sb<-sum.boots[[k]][,c("Lineage","Sublineage","Number.Descendants","N.Desc.glob","N.Desc.can")]
# sb$Sublineage3<-NA
# sb$Sublineage3[sb$N.Desc.glob>20]<-sb$Sublineage[sb$N.Desc.glob>20]
p.size<-ggplot(sb, aes(area = N.Desc.glob, fill = Lineage, 
                       label=paste(Sublineage,"\n","n=",N.Desc.glob,sep=""))) +
  geom_treemap()+
  LinFillScale+
  geom_treemap_text(colour = "white", place = "centre",grow = TRUE,reflow=T,
                    min.size = 0.4, padding.x=unit(1,"mm"),padding.y=unit(1,"mm"))+
  theme(legend.position="none",
        plot.margin=margin(4,4,4,4,unit="pt"))
p.size
ggsave(paste(f.out,"/SublineageSizeTreeMap.Lineage.png",sep=""),height=2,width=3,unit="in")

p.size.can<-ggplot(sb, aes(area = N.Desc.can, fill = Lineage, 
                           label=paste(Sublineage,"\n","n=",N.Desc.can,sep=""))) +
  geom_treemap()+
  LinFillScale+
  geom_treemap_text(colour = "white", place = "centre",grow = TRUE,reflow=T,
                    min.size = 0.4, padding.x=unit(1,"mm"),padding.y=unit(1,"mm"))+
  theme(legend.position="none",
        plot.margin=margin(4,4,4,4,unit="pt"))
p.size.can
ggsave(paste(f.out,"/SublineageSizeCANADATreeMap.Lineage.png",sep=""),height=2,width=3,unit="in")
```

## Sublin sizes (Can desc) by period
```{r}
## if there are var-spec interventions...
if(int.yn==T){
  #stratify this by wave, only for Canadian descendants,
  # need to calculate number of descendants in each wave
  sb2<-sum.boots[[k]][,c("Lineage","Sublineage","Number.Descendants","N.Desc.glob","N.Desc.can")]
  #wave 1 = before intervention
  wave1.desc<-sublin.long.unq[[k]] %>% ### CHANGED TO UNIQUE
    filter(Date<as.Date(int.end)) %>%
    filter(State %in% provs) %>% #only canadians allowed
    dplyr::count(Sublineage)
  colnames(wave1.desc)[2]<-"wave1.Desc"
  
  #wave 2 = during intervention
  wave2.desc<-sublin.long.unq[[k]] %>%
    filter(Date>=as.Date(int.start)) %>%
    filter(Date<=as.Date(int.end)) %>%
      filter(State %in% provs) %>%
    dplyr::count(Sublineage)
  colnames(wave2.desc)[2]<-"wave2.Desc"
  
  #wave 3 = after intervention
  wave3.desc<-sublin.long.unq[[k]] %>%
    filter(Date>as.Date(int.end)) %>%
      filter(State %in% provs) %>%
    dplyr::count(Sublineage)
  colnames(wave3.desc)[2]<-"wave3.Desc"
  
  sb2<-sb2%>%
    left_join(wave1.desc,by="Sublineage") %>%
    left_join(wave2.desc,"Sublineage") %>%
    left_join(wave3.desc,"Sublineage")
  
  # head(sb2)
  p.size.w1<-ggplot(sb2, aes(area = wave1.Desc, fill = Lineage,
                             label=paste(Sublineage,"\n","n=",wave1.Desc,sep=""))) +
    geom_treemap()+
    LinFillScale+
    geom_treemap_text(colour = "white", place = "centre",
                      grow = TRUE,reflow=T,min.size = 0.4,
                      padding.x=unit(1,"mm"),padding.y=unit(1,"mm"))+
    theme(legend.position="none",
          plot.margin=margin(4,4,4,4,unit="pt"))
  p.size.w1
  ggsave(file=paste(f.out,"/Wave1_SublinSizesUnq.png",sep=""),width=3,height=2,units = "in")
  
  p.size.w2<-ggplot(sb2, aes(area = wave2.Desc, fill = Lineage,
                             label=paste(Sublineage,"\n","n=",wave2.Desc,sep=""))) +
    geom_treemap()+
    LinFillScale+
    geom_treemap_text(colour = "white", place = "centre",
                      grow = TRUE,reflow=T,min.size = 0.4,
                      padding.x=unit(1,"mm"),padding.y=unit(1,"mm"))+
    theme(legend.position="none",
          plot.margin=margin(4,4,4,4,unit="pt"))
  p.size.w2
  ggsave(file=paste(f.out,"/Wave2_SublinSizesUnq.png",sep=""),width=3,height=2,units = "in")
  
  p.size.w3<-ggplot(sb2, aes(area = wave3.Desc, fill = Lineage,
                             label=paste(Sublineage,"\n","n=",wave3.Desc,sep=""))) +
    geom_treemap()+
    LinFillScale+
    geom_treemap_text(colour = "white", place = "centre",
                      grow = TRUE,reflow=T,min.size = 0.4,
                      padding.x=unit(1,"mm"),padding.y=unit(1,"mm"))+
    theme(legend.position="none",
          plot.margin=margin(4,4,4,4,unit="pt"))
  p.size.w3
  ggsave(file=paste(f.out,"/Wave3_SublinSizesUnq.png",sep=""),width=3,height=2,units = "in")
  
  
  ## Could make this animated so rects change over time
  #see https://github.com/wilkox/treemapify/tree/3debdbf2350e6d44a2ac4f7eb5e4d7f8c74ab210
  
  #descendants row
  size.row<-plot_grid(p.size.w1,p.size.w2,p.size.w3,
                      ncol=3, align="h")
  size.row
  ggsave(file=paste(f.out,"/WavesSublinSize.png",sep=""),width=8.5,height=4,units = "in")
}
```

# Sublineage importations over time
## Calculate rolling mean of importations by 1) origin, 2) destination using tmrca
```{r}
Parent.Location.sum.boots<-replicate(n=n.B,vector())
Node.Location.sum.boots<-replicate(n=n.B,vector())
Parent.Location.sum.boots.full<-replicate(n=n.B,vector())
Node.Location.sum.boots.full<-replicate(n=n.B,vector())

for (k in 1:n.B){
  #make sure this a date
  sum.boots[[k]]$tmrca.dt<-as.Date(sum.boots[[k]]$tmrca.dt)
  
  #### Calculate a rolling 7-day mean for origins ####
  ## count the importations by origin location over time
  Parent.Location.sum.boots[[k]]<-sum.boots[[k]] %>%
    dplyr::select(Parent.Location, Lineage,tmrca.dt) %>%
    dplyr::group_by(tmrca.dt, Parent.Location) %>%
    dplyr::summarize(.groups="rowwise", total=n()) %>%
    dplyr::arrange(desc(Parent.Location)) %>% 
    dplyr::group_by(Parent.Location) 
  
  #need to add rows for missing dates
  alldays<-seq(ymd(first(sort(Parent.Location.sum.boots[[k]]$tmrca.dt))),
      ymd(last(sort(Parent.Location.sum.boots[[k]]$tmrca.dt))),
      by='1 day')
  
  #make a empty df in same structure as above then populate it
  nL<-length(unique(Parent.Location.sum.boots[[k]]$Parent.Location))
  nD<-length(alldays)
  Parent.Location.sum.boots.full[[k]]<-data.frame(tmrca.dt=rep(alldays,times=nL),
Parent.Location = sort(rep(unique(Parent.Location.sum.boots[[k]]$Parent.Location),times=nD)),
                                 total=0)

  #populate it
  for (i in 1:nrow(Parent.Location.sum.boots.full[[k]])){
    #look for a match
    match<-which(Parent.Location.sum.boots[[k]]$Parent.Location==Parent.Location.sum.boots.full[[k]]$Parent.Location[i] &
            Parent.Location.sum.boots[[k]]$tmrca.dt==Parent.Location.sum.boots.full[[k]]$tmrca.dt[i])
    if(length(match)==0) next #no match, no change
    #else, replace:
    Parent.Location.sum.boots.full[[k]]$total[i]<-Parent.Location.sum.boots[[k]]$total[match]
  }

  Parent.Location.sum.boots.full[[k]]<-Parent.Location.sum.boots.full[[k]] %>% 
    dplyr::mutate(intros_mean7d = zoo::rollmean(total, k = 7, fill = NA),
                  intros_mean14d = zoo::rollmean(total, k = 14, fill = NA),
                  intros_median7d = zoo::rollmedian(total, k = 7, fill = NA),
                  intros_sum7d = zoo::rollsum(total, k = 7, fill = NA,align="right")) %>% #right align to sum all prev
    #rolling mean and median weekly importation rate
    dplyr::mutate(intros_meansum7d = zoo::rollmean(intros_sum7d, k = 7, fill = NA), 
                  intros_mediansum7d = zoo::rollmedian(intros_sum7d, k = 7, fill = NA),) %>%
    dplyr::ungroup()

  ## count the importations by node.location over time
  Node.Location.sum.boots[[k]]<-sum.boots[[k]] %>%
    dplyr::select(Node.Location, Lineage,tmrca.dt) %>%
    group_by(tmrca.dt, Node.Location) %>%
    dplyr::summarize(.groups="rowwise", total=n()) %>%
    dplyr::arrange(desc(Node.Location)) %>% 
    dplyr::group_by(Node.Location) 
  
  #need to add rows for missing dates
  alldays<-seq(ymd(first(sort(Node.Location.sum.boots[[k]]$tmrca.dt))),
      ymd(last(sort(Node.Location.sum.boots[[k]]$tmrca.dt))),
      by='1 day')
  
  #make a empty df in same structure as above then populate it
  nL<-length(unique(Node.Location.sum.boots[[k]]$Node.Location))
  nD<-length(alldays)
  Node.Location.sum.boots.full[[k]]<-data.frame(tmrca.dt=rep(alldays,times=nL),
                                 Node.Location=sort(rep(unique(Node.Location.sum.boots[[k]]$Node.Location),times=nD)),
                                 total=0)
  # nrow(Node.Location.sum.boots[[k]].empty)==nD*nL      
  
  #populate it
  for (i in 1:nrow(Node.Location.sum.boots.full[[k]])){
    #look for a match
    match<-which(Node.Location.sum.boots[[k]]$Node.Location==Node.Location.sum.boots.full[[k]]$Node.Location[i] &
            Node.Location.sum.boots[[k]]$tmrca.dt==Node.Location.sum.boots.full[[k]]$tmrca.dt[i])
    if(length(match)==0) next #no match, no change
    #else, replace:
    Node.Location.sum.boots.full[[k]]$total[i]<-Node.Location.sum.boots[[k]]$total[match]
  }

  Node.Location.sum.boots.full[[k]]<-Node.Location.sum.boots.full[[k]] %>% 
    dplyr::mutate(intros_mean7d = zoo::rollmean(total, k = 7, fill = NA),
                  intros_mean14d = zoo::rollmean(total, k = 14, fill = NA),
                  intros_median7d = zoo::rollmedian(total, k = 7, fill = NA),
                  intros_sum7d = zoo::rollsum(total, k = 7, fill = NA,align="right")) %>% 
    #rolling mean and median weekly importation rate
    dplyr::mutate(intros_meansum7d = zoo::rollmean(intros_sum7d, k = 7, fill = NA), 
                  intros_mediansum7d = zoo::rollmedian(intros_sum7d, k = 7, fill = NA),) %>%
    dplyr::ungroup()

}

## Summarize the rolling means overall

#go through each list item/subsample
for (k in 1:n.B){
  ## add a column for subsample
  Parent.Location.sum.boots.full[[k]]$SampleSet<-paste("Sample",BOOTS[k])
}

## rbind the summaries
sum.Par.Roll<-bind_rows(Parent.Location.sum.boots.full)
#summarize the mean and confint
sum.Par.Roll.summary<-sum.Par.Roll %>% dplyr::group_by(tmrca.dt, Parent.Location) %>%
  dplyr::summarize(.groups="rowwise",
                   intros_meansum7d.mean=(mean(intros_meansum7d,na.rm=T)),
                   intros_meansum7d.sd=sd(intros_meansum7d,na.rm=T)) %>%
  as.data.frame()

#order the geos
ord.count<-sum.Par.Roll.summary%>% dplyr::group_by(Parent.Location) %>%
  dplyr::summarise(.groups="rowwise",n=sum(intros_meansum7d.mean,na.rm=T)) %>% as.data.frame()
# ord.count<-ord.count[-which(ord.count$n=="NaN"),]
ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
sum.Par.Roll.summary$Parent.Location<-factor(sum.Par.Roll.summary$Parent.Location,levels=ord.count)

#Global specific fill 
GlobRegFillScale<-scale_fill_manual(name = "Location",values = globalPalette.ch[match(ord.count,names(globalPalette.ch))], na.value="grey60")
GlobRegColScale<-scale_color_manual(name = "Location",values = globalPalette.ch[match(ord.count,names(globalPalette.ch))], na.value="grey60")

## BY PROVINCE
for (k in 1:n.B){
  Node.Location.sum.boots.full[[k]]$SampleSet<-paste("Sample",BOOTS[k])
}

sum.Prov.Roll<-bind_rows(Node.Location.sum.boots.full)

#summarize the mean and confint
sum.Prov.Roll.summary<-sum.Prov.Roll %>% dplyr::group_by(tmrca.dt, Node.Location) %>%
  dplyr::summarize(.groups="rowwise",
                   intros_meansum7d.mean=(mean(intros_meansum7d,na.rm=T)),
                   intros_meansum7d.sd=(sd(intros_meansum7d,na.rm=T))) %>%
  as.data.frame()

#make sure proper order
ord.prov<-sum.Prov.Roll.summary%>% dplyr::group_by(Node.Location) %>%
  dplyr::summarise(.groups="rowwise",n=sum(intros_meansum7d.mean,na.rm = T)) %>% as.data.frame()
ord.prov<-ord.prov[rev(order(ord.prov$n)),'Node.Location']
sum.Prov.Roll.summary$Node.Location<-factor(sum.Prov.Roll.summary$Node.Location,levels=ord.prov)

## Add confidence upper and lower
t.here<-qt(0.025,(n.B-1),lower.tail=F)
sum.Par.Roll.summary<-sum.Par.Roll.summary %>% mutate(upper.CI=(intros_meansum7d.mean)+(intros_meansum7d.sd/sqrt(10)*t.here),
       lower.CI=(intros_meansum7d.mean)-(intros_meansum7d.sd/sqrt(10)*t.here))

## Add an indidicator for source country vs not to add alpha channel to other lines
sum.Par.Roll.summary$focal<-0
sum.Par.Roll.summary$focal[which(sum.Par.Roll.summary$Parent.Location%in%focal.source)]<-1

#PRovince specific fill 
ProvFillScale<-scale_fill_manual(name = "Location",values = globalPalette.ch[match(ord.prov,names(globalPalette.ch))], na.value="grey60")

ProvColScale<-scale_color_manual(name = "Location",values = globalPalette.ch[match(ord.prov,names(globalPalette.ch))], na.value="grey60")

write.csv(sum.Par.Roll.summary, paste0(f.out,"sum.par.roll.summary.csv"))
```

## Plot sublineages over time using rolling rates, by tmrca, by origin or dest
```{r}
#set some limits
all.y<-sum.Par.Roll.summary %>% dplyr::group_by(tmrca.dt) %>% dplyr::summarize(allsum=sum(intros_meansum7d.mean,na.rm=T))
y.max.all<-ceiling(max(all.y$allsum,na.rm = T))
y.jumps1<-ceiling(y.max.all/5)
y.max.singles<-ceiling(max(sum.Par.Roll.summary$intros_meansum7d.mean,na.rm=T))
y.jumps<-ceiling(y.max.singles/5)
y.lim<-c(0,y.max.all)

p1<-sum.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Parent.Location,fill=Parent.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = "top",
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim[1], ymax =y.lim[2], color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = as.Date(int.start),y=y.lim[2], vjust=1, hjust=0,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
    geom_density(stat="identity", position="stack",lwd=0,alpha=0.9)+
  GlobRegFillScale+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"), fill="Origin Location")+
  scaleDateFlex+
  scale_y_continuous(expand=c(0,0),limits=y.lim,breaks=seq(0,max(y.lim),y.jumps1))+
  guides(fill = guide_legend(keywidth = 0.6,keyheight=0.6,title.position = "top",title="Global region",legend.spacing=0,nrow=2))
# p1
# ggsave(paste(f.out,"/Rolling-sublin-importRate.png",sep=""),height=4,width=6)

##PLOT AS LINE WITH CONFIDENCE INTERVALS
alphavals<-c(0.3,0.9)
names(alphavals)<-c("0","1")  
sum.Par.Roll.summary$focal<-as.factor(sum.Par.Roll.summary$focal)
y.lim2<-c(0,(y.max.singles+2))

p1.line.ci<-sum.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Parent.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = c(0.8,0.7),
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = as.Date(int.start), 
           xmax = as.Date(int.end),
             ymin = y.lim2[1], ymax =y.lim2[2], 
           color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = as.Date(int.start)+int.duration/2,
           y=y.lim2[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_ribbon(aes(ymin=lower.CI,ymax=upper.CI, 
                  fill=Parent.Location, alpha=focal))+
  geom_line(stat="identity",lwd=1,aes(alpha=focal,color=Parent.Location))+
  GlobRegColScale+
  GlobRegFillScale+
  scale_alpha_manual(values=alphavals)+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"), fill="Origin Location")+
  scaleDateFlex+
  scale_y_continuous(expand=c(0,0),breaks=seq(0,y.max.singles,y.jumps))+
 guides(fill = guide_legend(keywidth = 0.6,keyheight=0.6,
                              title.position = "top",title="Global origin",
                              legend.spacing=0,ncol=1),
         alpha="none", color="none")+
  coord_cartesian(ylim=y.lim2)
p1.line.ci
ggsave(paste(f.out,"/Rolling-sublin-importRate_line_confint.png",sep=""),height=3,width=3.5)


#Line no CI
p1.line<-sum.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Parent.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = c(0.8,0.7),
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim2[1], ymax =y.lim2[2], color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = as.Date(int.start)+int.duration/2,y=y.lim2[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_line(stat="identity",lwd=1,aes(alpha=focal,color=Parent.Location))+
  GlobRegColScale+
  # GlobRegFillScale+
  scale_alpha_manual(values=alphavals)+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"), fill="Origin Location")+
  scaleDateFlex+
  scale_y_continuous(expand=c(0,0),breaks=seq(0,y.max.singles,y.jumps))+
  guides(color = guide_legend(keywidth = 0.6,keyheight=0.6,
                              title.position = "top",title="Global origin",
                              legend.spacing=0,ncol=1),
         alpha="none")+
  coord_cartesian(ylim=y.lim2)
p1.line
ggsave(paste(f.out,"/Rolling-sublin-importRate_line.png",sep=""),height=3,width=3.5)

#rolling importation rate, by province destination
p2<-sum.Prov.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Node.Location,fill=Node.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = "top",
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim[1], ymax =y.lim[2], color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = as.Date(int.start),y=y.lim[2], vjust=1, hjust=0,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  scaleDateFlex+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"))+
  geom_density(stat="identity", position="stack",lwd=0,alpha=0.9)+
  ProvFillScale+
  scale_y_continuous(expand=c(0,0),limits=y.lim,breaks=seq(0,y.max.all,y.jumps1))+
  guides(fill = guide_legend(keywidth = 0.6,keyheight=0.6,title.position = "top",title="Province",legend.spacing=0,nrow=2))
# p2
# ggsave(paste(f.out,"/Rolling-sublin-importRate-byprovince.png",sep=""),height=4,width=6)

y.lim3<-c(0,y.max.singles)
p2.line<-sum.Prov.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Node.Location,color=Node.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = "top",
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
    annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim3[1], ymax =y.lim3[2], color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = as.Date(int.start),y=y.lim3[2], vjust=1, hjust=0,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  scaleDateFlex+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"))+
  geom_line(stat="identity", lwd=1,alpha=0.9)+
  ProvColScale+
  scale_y_continuous(expand=c(0,0),limits=y.lim3,breaks=seq(0,y.max.singles,y.jumps))+
  guides(color = guide_legend(keywidth = 0.6,keyheight=0.6,title.position = "top",title="Province",legend.spacing=0,nrow=2))
p2.line
ggsave(paste(f.out,"/Rolling-sublin-importRate-byprovince_line.png",sep=""),height=4,width=6)

```

## re-generate this plot using just sum.par.roll.summary
```{r}
# sum.Par.Roll.summary<-read.csv("2023-07-06_alpha_analysis/sum.par.roll.summary.csv")

# all.y<-sum.Par.Roll.summary %>% dplyr::group_by(tmrca.dt) %>% dplyr::summarize(allsum=sum(intros_meansum7d.mean,na.rm=T))
# y.max.all<-ceiling(max(all.y$allsum,na.rm = T))
# y.jumps1<-ceiling(y.max.all/5)
y.max.singles<-ceiling(max(sum.Par.Roll.summary$intros_meansum7d.mean,na.rm=T))
y.jumps<-ceiling(y.max.singles/5)
# y.lim<-c(0,y.max.all)

##PLOT AS LINE WITH CONFIDENCE INTERVALS
alphavals<-c(0.3,0.9)
names(alphavals)<-c("0","1")  
sum.Par.Roll.summary$focal<-as.factor(sum.Par.Roll.summary$focal)
y.lim2<-c(0,(y.max.singles+2))

##set up colors
#order the geos
ord.count<-sum.Par.Roll.summary%>% dplyr::group_by(Parent.Location) %>%
  dplyr::summarise(.groups="rowwise",n=sum(intros_meansum7d.mean,na.rm=T)) %>% as.data.frame()
# ord.count<-ord.count[-which(ord.count$n=="NaN"),]
ord.count<-ord.count[rev(order(ord.count$n)),'Parent.Location']
sum.Par.Roll.summary$Parent.Location<-factor(sum.Par.Roll.summary$Parent.Location,levels=ord.count)

#Global specific fill 
GlobRegFillScale<-scale_fill_manual(name = "Location",values = globalPalette.ch[match(ord.count,names(globalPalette.ch))], na.value="grey60")
GlobRegColScale<-scale_color_manual(name = "Location",values = globalPalette.ch[match(ord.count,names(globalPalette.ch))], na.value="grey60")

#remove the confidence intervals for any but the focal source
# sum.Par.Roll.summary$upper.CI[which(sum.Par.Roll.summary$focal==0)]<-NA
# sum.Par.Roll.summary$lower.CI[which(sum.Par.Roll.summary$focal==0)]<-NA

sum.Par.Roll.summary$tmrca.dt<-as.Date(sum.Par.Roll.summary$tmrca.dt)
p1.line.ci<-sum.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Parent.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = c(1,1),
        legend.justification=c(1,1), 
        legend.text=element_text(size=rel(0.8)),
        legend.title=element_text(size=rel(0.9)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = as.Date(int.start), 
           xmax = as.Date(int.end),
             ymin = y.lim2[1], ymax =y.lim2[2], 
           color="grey85",size=0,  fill="grey80",alpha=0.8)+
  annotate(geom="text",x = as.Date(int.start)+int.duration/2,
           y=y.lim2[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=4)+
  geom_ribbon(aes(ymin=lower.CI,ymax=upper.CI, 
                  fill=Parent.Location, alpha=focal))+
  geom_line(stat="identity",lwd=1,aes(alpha=focal,color=Parent.Location))+
  GlobRegColScale+
  GlobRegFillScale+
  scale_alpha_manual(values=alphavals)+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"), fill="Origin Location")+
  scaleDateFlexLess+
  scale_y_continuous(expand=c(0,0),breaks=seq(0,y.max.singles,y.jumps))+
 guides(fill = guide_legend(keywidth = 0.6,keyheight=0.6,
                              title.position = "top",title="Global origin",
                              legend.spacing=0,ncol=1,
                              # override.aes = list(alpha = c(0.9,0.3,0.3,0.3,0.3,0.3,0.3,0.3))
                            ),
        color="none",
        alpha="none")+
  coord_cartesian(ylim=y.lim2)
p1.line.ci
ggsave(paste(f.out,"/Rolling-sublin-importRate_line_confintALL.png",sep=""),height=3,width=3.5)


##Plot as above, but no legend, limited scale to figure end date
# direct labels for big contributors
y.lim2<-c(0,(y.max.singles+2))

p1.line.ci.noleg<-sum.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt,y=intros_meansum7d.mean,group=Parent.Location))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = "none",#c(0.8,0.7),
        # legend.text=element_text(size=rel(0.7)),
        # legend.title=element_text(size=rel(0.8)),
        plot.margin=margin(20,4,1,4,"pt"),
        ### ADDITION TO line up axes text for single digit scales
        axis.title.y=element_text(vjust=2))+
  annotate(geom="rect",xmin = as.Date(int.start), 
           xmax = as.Date(int.end),
             ymin = y.lim2[1], ymax =y.lim2[2], 
           color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = as.Date(int.start)+int.duration/2,
           y=y.lim2[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_ribbon(aes(ymin=lower.CI,ymax=upper.CI, 
                  fill=Parent.Location, alpha=focal))+
  geom_line(stat="identity",lwd=1,aes(alpha=focal,color=Parent.Location))+
  GlobRegColScale+
  GlobRegFillScale+
  scale_alpha_manual(values=alphavals)+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"), fill="Origin Location")+
  scaleDateFlexLess+
  scale_y_continuous(expand=c(0,0),breaks=seq(0,y.max.singles+2,y.jumps))+
 guides(fill = guide_legend(keywidth = 0.6,keyheight=0.6,
                              title.position = "top",title="Global origin",
                              legend.spacing=0,ncol=1),
         alpha="none", color="none")+
  coord_cartesian(ylim=y.lim2)+
  annotate(geom="text",x=as.Date("2020-12-07"),y=6,label="UK",color=globalPalette.ch[which(names(globalPalette.ch)=="United Kingdom")],size=4)+
  annotate(geom="text",x=as.Date("2021-05-03"),y=4.1,label="USA",color=globalPalette.ch[which(names(globalPalette.ch)=="USA")],alpha=0.8,size=2.5)+
  annotate(geom="text",x=as.Date("2021-02-11"),y=7.4,label="Europe",color=globalPalette.ch[which(names(globalPalette.ch)=="Europe")],alpha=0.8,size=2.5)

p1.line.ci.noleg
ggsave(paste(f.out,"/Rolling-sublin-importRate_line_confint_noleg.png",sep=""),height=3,width=3.5)

## version with A)
plot_grid(p1.line.ci.noleg, labels="A) Alpha",label_x=-0.1)
ggsave(paste(f.out,"/Rolling-sublin-importRate_line_confint_noleg_A.png",sep=""),height=3,width=3.5)

```


```{r}
#make a list of grobs
## add in the Canadian province representation plots
plot.row<-plot_grid(p1,p2,ncol=2,rel_widths = c(1,1),labels=c("A" ,"B"))
png(file=paste(f.out,"/Sublin_OverTime_OriginsDestinations.png",sep=""),
          width=9,height=4,units = "in", bg = "white",res=200)
print(plot.row)
dev.off()
```

## summarize Maximum rolling rates
```{r}
#overall sum of new sublineages/week in Canada max
sum.Roll.total<-sum.Prov.Roll.summary %>% dplyr::group_by(tmrca.dt) %>%
  dplyr::summarize(.groups="rowwise",
                   total_meansum7d.mean=(sum(intros_meansum7d.mean,na.rm=T)),
                   total_meansum7d.sd=(sum(intros_meansum7d.sd,na.rm=T))) %>%
  as.data.frame()
max.overall<-head(na.omit(sum.Roll.total[rev(order(sum.Roll.total$total_meansum7d.mean)),]),n=1)
dt.temp<-max.overall[1,1]
cat(paste0("Max weekly sublin importation rate overall on ", dt.temp,": ",
           mean.95ci.givenmsd(m=max.overall[2],sd=max.overall[3],n=n.B,2)),
    file=text.out,sep="\n",append=T)

#Max from focal.source
max.mean.focal<-max(sum.Par.Roll.summary$intros_meansum7d.mean[sum.Par.Roll.summary$Parent.Location%in%focal.source],na.rm=T) 
max.sd.focal<-sum.Par.Roll.summary$intros_meansum7d.sd[which(sum.Par.Roll.summary$intros_meansum7d.mean ==max(sum.Par.Roll.summary$intros_meansum7d.mean[sum.Par.Roll.summary$Parent.Location%in%focal.source],na.rm=T) )]
dt.focal<-sum.Par.Roll.summary$tmrca.dt[which(sum.Par.Roll.summary$intros_meansum7d.mean==max(sum.Par.Roll.summary$intros_meansum7d.mean[sum.Par.Roll.summary$Parent.Location%in%focal.source],na.rm=T)) ] 

cat(paste0("Max weekly sublin importation rate from ",focal.source, " on ",dt.focal,
           ": ", mean.95ci.givenmsd(m=max.mean.focal,sd=max.sd.focal, n.B, 2) ),
    file=text.out,sep="\n",append=T)

## Max from USA 
max.mean.usa<-max(sum.Par.Roll.summary$intros_meansum7d.mean[sum.Par.Roll.summary$Parent.Location=="USA"],na.rm=T) 
max.sd.usa<-sum.Par.Roll.summary$intros_meansum7d.sd[which(sum.Par.Roll.summary$intros_meansum7d.mean ==max.mean.usa )]
dt.usa<-sum.Par.Roll.summary$tmrca.dt[which(sum.Par.Roll.summary$intros_meansum7d.mean==max.mean.usa) ] 
cat(paste0("Max weekly sublin importation rate from ","USA", " on ",dt.usa,
           ": ", mean.95ci.givenmsd(m=max.mean.usa,sd=max.sd.usa, n.B, 2) ),
    file=text.out,sep="\n",append=T)

## IF there was a var-specifc intervention, calculate sublin import on date implemented vs two weeks after from focal source
if(int.yn==T){
   rate.pre<-sum.Par.Roll.summary %>% filter(Parent.Location%in%focal.source) %>% filter(tmrca.dt==int.start)
   
   rate.2wk<-sum.Par.Roll.summary %>% filter(Parent.Location%in%focal.source) %>% filter(tmrca.dt==(int.start+14))
   
    rate.4wk<-sum.Par.Roll.summary %>% filter(Parent.Location%in%focal.source) %>% filter(tmrca.dt==(int.start+28))
   
   cat(paste0("Weekly sublin importation rate pre-int from ",focal.source, 
              " on ",int.start,": ", 
              mean.95ci.givenmsd(m=rate.pre$intros_meansum7d.mean,
                                    sd=rate.pre$intros_meansum7d.sd, n.B, 2) ),
    file=text.out,sep="\n",append=T)
   
   cat(paste0("Weekly sublin importation rate 2-weeks post-int from ",focal.source, 
              " on ",int.start+14, ": ", 
              mean.95ci.givenmsd(m=rate.2wk$intros_meansum7d.mean,
                                    sd=rate.2wk$intros_meansum7d.sd, n.B, 2) ),
    file=text.out,sep="\n",append=T)
   
   
   ## FOLD DECREASE from pre to 2wk
   fc2wk.focal<-c()
   fc4wk.focal<-c()

   for (k in 1:n.B){
     temp<-Parent.Location.sum.boots.full[[k]] %>% 
      dplyr::filter(Parent.Location%in%focal.source) %>%
      as.data.frame()
     temp.pre<-temp %>% filter(tmrca.dt==int.start)
     temp.2wk<-temp %>% filter(tmrca.dt==(int.start+14))
     temp.4wk<-temp %>% filter(tmrca.dt==(int.start+28))

     fc2wk<-temp.pre$intros_meansum7d/temp.2wk$intros_meansum7d #fold change 2wk/pre (ex: 10/1)
     fc2wk.focal<-c(fc2wk.focal, fc2wk)
     
     fc4wk<-temp.pre$intros_meansum7d/temp.4wk$intros_meansum7d #fold change 2wk
     fc4wk.focal<-c(fc4wk.focal, fc4wk)
   }
   fc2wk.focal<-fc2wk.focal[is.finite(fc2wk.focal)]
   fc4wk.focal<-fc4wk.focal[is.finite(fc4wk.focal)]
   
   mean.fc.2wk<-mean.95ci.X(fc2wk.focal,2)
   mean.fc.4wk<-mean.95ci.X(fc4wk.focal,2)

   cat(paste0("Fold decrease sublin rate pre-int to 2-weeks after int from ",focal.source, 
              ": ", mean.fc.2wk),    file=text.out,sep="\n",append=T)
   
   cat(paste0("Fold decrease sublin rate pre-int to 4-weeks after int from ",focal.source, 
              ": ", mean.fc.4wk),    file=text.out,sep="\n",append=T)
   
}

```

## how many sublineages had >100 or >500 descendants?
```{r}
hund<-c()
hund.perc<-c()
hund.des<-c()
for (k in 1:n.B){ 
  big<-which(sum.boots[[k]]$N.Desc.glob>99)
  hund<-c(hund, length(big))
  #what percent of sublins were big (>99)
  hund.perc<-c(hund.perc, length(big)/nrow(sum.boots[[k]])*100)
  
  # What percent of sampled cases in Canada were downstream of these sublineages?
  restr<-sublin.long.unq[[k]] [ sublin.long.unq[[k]]$Sublineage %in% sum.boots[[k]]$Sublineage[big] ,]
  can.des<-length(which(restr$State %in% provs ))
  hund.des<-c(hund.des, can.des/nrow(meta.boots[[k]][meta.boots[[k]]$country=="Canada",])*100)
}
mean.95ci.X(hund) 
mean.95ci.X(hund.perc,1) 
mean.95ci.X(hund.des,1)  

# how many sublineagse had >500 descendants?
fvhund<-c()
fvhund.perc<-c()
fvhund.des<-c()
for (k in 1:n.B){ 
  big<-which(sum.boots[[k]]$N.Desc.glob>499)
  fvhund<-c(fvhund, length(big))
  fvhund.perc<-c(fvhund.perc, length(big)/nrow(sum.boots[[k]])*100)
  # What percent of sampled cases in Canada were downstream of these sublineages?
  restr<-sublin.long.unq[[k]] [ sublin.long.unq[[k]]$Sublineage %in% sum.boots[[k]]$Sublineage[big] ,]
  # restr<-restr[-which(duplicated(restr$Descendant.AccessionIDs)),]
  can.des<-length(which(restr$State %in% provs))
  fvhund.des<-c(fvhund.des, can.des/nrow(meta.boots[[k]][meta.boots[[k]]$country=="Canada",])*100)
}
mean.95ci.X(fvhund)
mean.95ci.X(fvhund.perc,1)
mean.95ci.X(fvhund.des,1) 

## ADD TO TEXT EXPORT 
cat(paste0("# sublineages with >=100 desc: ",mean.95ci.X(hund), 
           ", accounting for ",mean.95ci.X(hund.perc,1) ,
           "% of sublineages and ",mean.95ci.X(hund.des,1) ,"% of Canadian descendants"),
    file=text.out,sep="\n",append=T)

cat(paste0("# sublineages with >=500 desc: ",mean.95ci.X(fvhund), 
           ", accounting for ",mean.95ci.X(fvhund.perc,1) ,
           "% of sublineages and ",mean.95ci.X(fvhund.des,1) ,"% of Canadian descendants"),
    file=text.out,sep="\n",append=T)
```

## Sublineage longevity and recency analysis
```{r }
# head(sum.boots[[k]]$longevityTMRCA)

#distrib of longevity (life span)
k=1
P3<-ggplot(sum.boots[[k]])+
  geom_density(aes(x=longevityTMRCA, color="deepskyblue3"),alpha=0.5)+ 
  scale_color_manual(values=c("deepskyblue3"))+ 
  pubTheme+
  labs(x="Sublineage longevity (days)",y="Density")+
  annotate(geom="text",x=100,y=0.007,label=paste("Median longevity =\n", median(sum.boots[[k]]$longevityTMRCA)," days",sep=""),hjust=0,color="deepskyblue3")+
  theme(legend.position="none")
P3
ggsave(paste(f.out,"/SublineagelongevityDensity.png",sep=""),height=4,width=4, units="in")


#is it associated with tmrca (did they reduce over time?)
ggplot(sum.boots[[k]])+
  geom_point(aes(y=longevityTMRCA, x=tmrca.dt))

#looks like yes
mod.long<-lm(as.numeric(longevityTMRCA) ~ tmrca.dt ,data=sum.boots[[k]])
# summary(mod.long)
newdata2 <- data.frame(
  tmrca.dt = rep(seq(from = min(sum.boots[[k]]$tmrca.dt), to = max(sum.boots[[k]]$tmrca.dt), length.out = 100), 2))

newdata2 <- cbind(newdata2, predict(mod.long, newdata2, type = "response", se.fit=TRUE))
newdata2 <- within(newdata2, {
  longevityTMRCA <- (fit)
  LL <- (fit - 1.96 * se.fit)
  UL <- (fit + 1.96 * se.fit)
})


#plot again
P5<-ggplot(sum.boots[[k]])+
  geom_point(aes(y=longevityTMRCA, x=tmrca.dt),alpha=0.6)+
  scale_color_manual(values=c("deepskyblue3"))+
  geom_ribbon(data=newdata,aes(x=tmrca.dt, ymin=(LL),ymax=(UL)),fill="deepskyblue3",alpha = 0.25,) +
  geom_line(data=newdata,aes(x=tmrca.dt, y=longevityTMRCA), color="deepskyblue3",size = 0.5,alpha=0.5,) +
  pubThemeDate+
  theme(legend.position="none")+
  labs(x="Date of most recent common ancestor",y="Sublineage longevity (days)")+
  scaleDateFlex+
  scale_y_continuous(expand=c(0.01,0), limits=c(-50,390))+
    coord_cartesian(clip="on",ylim=c(0,390))

P5
ggsave(paste(f.out,"/longevityOverTime.png",sep=""),height=4,width=4, units="in")

## Composite figure of transmission longevity
plot_grid(P3,P5,ncol=2,labels=c("A" ,"B"))
ggsave(paste(f.out,"/Sublineagelongevity.png",sep=""),height=4,width=8, units="in")
```

## Number descendent vs tmrca, negative binomial
```{r}
#### # descendant vs tmrca.dt ######
k=1

#basic Y ~ X
mod.desc<-glm.nb((N.Desc.glob) ~ as.Date(tmrca.dt), data=sum.boots[[k]])
summary(mod.desc)

#adjust for node location
mod.desc2<-glm.nb((N.Desc.glob) ~ as.Date(tmrca.dt) + Node.Location, data=sum.boots[[k]])
summary(mod.desc2)

#compare to model 1
# anova(mod.desc,mod.desc2,test="lrt")
# BIC(mod.desc);BIC(mod.desc2) #modest improvement
# plot(mod.desc2) #residuals and QQ plot don't look great...

# go with model 3, not overfit
mod.desc<-mod.desc3

#summarize coeff, exponentiated because negbin
nb.summ<-as.data.frame(cbind(exp(mod.desc$coefficients),exp(confint(mod.desc))))
pvalz<-as.data.frame(summary(mod.desc)[12])[,4]
nb.summ$pvalue<-pvalz
colnames(nb.summ)<-c("Estimate","Lower 95% bound","Upper 95% bound","p-value")
nb.summ[,1:3]<-format(nb.summ[,1:3],scientific=T,digits=3)
nb.summ[,4]<-format(nb.summ[,4], scientific = TRUE,digits = 3)
rownames(nb.summ)<-str_replace_all(rownames(nb.summ),"Node.Location","")
# rownames(nb.summ)<-str_replace_all(rownames(nb.summ),"Newfoundland\nand Labrador","Newfoundland and Labrador")
rownames(nb.summ)[1:2]<-c("Intercept","TMRCA")
# write.csv(nb.summ,paste(f.out,"/tmrca_vs_logNumberDescendants_byLocation_NEGBIN.csv",sep = ""))
# nb.summ

newdata2 <- data.frame(
  tmrca.dt = rep(seq(from = min(sum.boots[[k]]$tmrca.dt), to = max(sum.boots[[k]]$tmrca.dt), length.out = 100), 2))

newdata2 <- cbind(newdata2, predict(mod.desc, newdata2, type = "link", se.fit=TRUE))
newdata2 <- within(newdata2, {
  N.Desc.glob <- exp(fit)
  LL <- exp(fit - 1.96 * se.fit)
  UL <- exp(fit + 1.96 * se.fit)
})

#PPLOT WITH NEGATIVE BINOMIAL FITS 
p3.v1<-ggplot(data=sum.boots[[k]],aes(x=as.Date(tmrca.dt),y=log10(N.Desc.glob)))+
  geom_point(alpha=0.8,color="deepskyblue3")+
  geom_ribbon(data=newdata2,aes(x=as.Date(tmrca.dt),ymin=log10(LL),ymax=log10(UL)),alpha = 0.15,
              fill="deepskyblue3") +
  geom_line(data=newdata2,aes(x=as.Date(tmrca.dt)), size = 0.5,alpha=0.5,fill="deepskyblue3") +
  coord_cartesian(ylim=c(0.2,5))+
  pubThemeDate+
  theme(legend.position=c(0.8,0.9),
        legend.text = element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)))+
  scaleDateFlex+
  scale_y_continuous(expand=c(0,0))+
  labs(x="Date of most recent common ancestor",y="log10(total sampled descendants)")+
  scale_color_manual(values=c("deepskyblue3"),guide=NULL)+
  scale_fill_manual(values=c("deepskyblue3"),guide=NULL)
  # guides(color=guide_legend(title = "Sublineage activity"))
p3.v1
ggsave(paste(f.out,"/tmrca_vs_logNumberDescendants_NEGBIN.png",sep=""),height=4,width=4,units="in")

```

## Time since importation
```{r}
sublin.long2<-replicate(n.B,vector())
k=1
#only doing this for one bootstrap for now
# sublin.long[[k]]

#join select columns from sum.boots onto sublin.long by sublineage2
#CHANGED TO the uniq
sublin.long2[[k]]<-sublin.long.unq[[k]]

# what is the average time since importation for sampled cases over time?
# for each sampled descendant of a sublineage, how long since the tmrca?
sublin.long2[[k]]$tmrca.dt<-as.Date(sublin.long2[[k]]$tmrca.dt)
sublin.long2[[k]]<- sublin.long2[[k]] %>% mutate(sublinAge=Date - tmrca.dt)

#only keep canadian descendants
# table(sublin.long2[[k]]$State)
# sublin.long2[[k]]$State<-str_replace_all(sublin.long2[[k]]$State,"Canada_","")
keepers<-which(sublin.long2[[k]]$State %in% provs)
sublin.long.Can<-sublin.long2[[k]] [keepers,] 

#calculate this as a rolling mean over time
summAge<-sublin.long.Can %>% dplyr::group_by(Date) %>%
  dplyr::summarize(.groups="rowwise",
                   meanAge=mean(sublinAge,na.rm=T),
                   sdAge=as.numeric(sd(sublinAge,na.rm=T)),
                   n=n())
# head(summAge)
summAge[is.na(summAge)]<-0 #get rid of NAs
summAge2<- summAge %>% as.data.frame() %>%
  dplyr::summarize(.groups="keep",
                   Date=Date,
                   meanAge2=meanAge,
                   #note that qt(.025, n, lower.tail=F) looks up the critical value for a given n
                   upperAge=as.numeric(meanAge)+( (sdAge/sqrt(n-1))*qt(.025, n, lower.tail=F) ), 
                   lowerAge=as.numeric(meanAge)-( (sdAge/sqrt(n-1))*qt(.025, n, lower.tail=F) ) ) %>% 
  dplyr::ungroup() %>% as.data.frame()

#if we can't calculate a sd, just make the upper/lower the mean
for (i in 1:nrow(summAge2)){
  if (is.na(summAge2$upperAge[i])){
    summAge2$lowerAge[i]<-summAge2$meanAge2[i]
    summAge2$upperAge[i]<-summAge2$meanAge2[i]
  }
  #if negative, make it zero
  if (summAge2$lowerAge[i]<0){summAge2$lowerAge[i]<-0}
}

summAge3<- summAge2 %>% as.data.frame() %>%
  dplyr::mutate(meanAge7d = zoo::rollmean(as.numeric(meanAge2), k = 7, fill=NA,align="center"),
                upperAge7d = zoo::rollmean(as.numeric(upperAge),k=7, fill=NA,align="center"),
                lowerAge7d = zoo::rollmean(as.numeric(lowerAge),k=7, fill=NA,align="center"),
                meanAge14d = zoo::rollmean(as.numeric(meanAge2), k = 14, fill=NA,align="center"),
                upperAge14d = zoo::rollmean(as.numeric(upperAge),k=14, fill=NA,align="center"),
                lowerAge14d = zoo::rollmean(as.numeric(lowerAge),k=14, fill=NA,align="center")) %>% 
  dplyr::ungroup() %>% as.data.frame()

#Add to the model
ord.prov2<-str_replace_all(ord.prov,"British\nColumbia","British Columbia")
ProvColScale2<-scale_color_manual(name = "Location",values = globalPalette.ch[match(ord.prov2,names(globalPalette.ch))], na.value="grey60")

p4<-sublin.long2[[k]] %>% filter(State %in% provs) %>%
  ggplot()+
  geom_point(aes(x=Date,y=sublinAge,color=Lineage),alpha=0.3,size=0.6)+
  geom_line(data=summAge3,aes(x=Date,y=meanAge14d),color="grey10",alpha=0.7,size=1)+
  geom_ribbon(data=summAge3,aes(x=Date,ymin=lowerAge14d,ymax=upperAge14d),fill="grey10",alpha=0.5)+
  # geom_line(data=pred.val,aes(x=Date,y=fit),color="grey10",alpha=0.8,size=1.5)+
  pubThemeDate+
  LinColScale+
  # ProvColScale2+
  theme(#legend.position=c(0.3,0.7), 
        legend.position="none",
        legend.title = element_text(size=rel(0.8)))+
  guides(color=guide_legend(title="Sampling location",
                            override.aes = list(alpha=1)))+
  labs(x="Canadian descendant sampling date",y="Days since importation")+
  # annotate(geom="text",x=as.Date("2020-09-08"),y=210,label="Rolling mean",hjust=1,size=3)+
  scaleDateFlex+
  scale_y_continuous(limits=c(0,300),expand=c(0.01,0))
  # annotate(geom="rect",xmin=as.Date("2020-01-15"),xmax=as.Date("2020-03-15"),
           # ymin=100,ymax=410,color="white",fill="white")
p4
ggsave(paste(f.out,"/TimeSinceImportationVsSampleDate.png",sep=""),height=4,width=5,units="in")
```

## descendant locations
```{r}
#summarize the total descendants by location

## tabulate instances of location pairs
sum.Desc.l<-replicate(n.B,vector())
for (k in 1:n.B){
  sum.Desc.l[[k]]<-sublin.long.unq[[k]] %>% dplyr::group_by (State) %>% 
    dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() 
  sum.Desc.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
}

## rbind the summaries
sum.Desc<-bind_rows(sum.Desc.l)

#summarize the mean and range for each 
sum.Desc.summary<-sum.Desc %>% dplyr::group_by(State) %>%
  dplyr::summarize(.groups="rowwise",
                   mean.n=round(mean(n.Par)),
                   sd.n=round(sd(n.Par,na.rm=T),digits=2)) %>%
  as.data.frame()

## plot it
sum.Desc.summary<-sum.Desc.summary[rev(order(sum.Desc.summary$mean.n)),]
sum.Desc.summary$State<-factor(sum.Desc.summary$State,
                               levels=sum.Desc.summary$State)
#big ones
ylim.up<-max(sum.Desc.summary$mean.n)+30
sum.Desc.summary %>%
      ggplot(aes(x=State,y=mean.n,group=State,fill=State))+
        geom_bar(stat="identity")+
        geom_errorbar(stat="identity",aes(ymin=mean.n-sd.n, ymax=mean.n+sd.n),color="grey50",lwd=0.1)+
        pubThemeDate+
        theme(legend.position = "none")+
        labs(x="Sublineage descendant location", y="# sampled descendants")+
        GlobFillScale+
  scale_y_continuous(limits=c(0,ylim.up), expand=expansion(0,0))
ggsave(paste(f.out,"/DescendantsBarPlot.png",sep=""),height=4, width=6,units="in")
```

# Detection lag modelling
```{r}
#write a generic lm function
make.lm.eqn<-function(lm){
  slope<-coef(lm)[2]
  yint<-coef(lm)[1]
  pval<-anova(lm)$'Pr(>F)'[1]
  radj<-summary(lm)$adj.r.squared
  eqn<-paste("y = ",signif(slope,digits=3)," x + ",signif(yint,digits=3),"\n",
             "adj. R^2 = ",signif(radj,digits = 2),"\n",
             "p-value = ", signif(pval,digits = 2), sep="")
  return(eqn)
}
```

## Plot number of descendants by detection lag, colored by prov, linear model
```{r}
k=1

mod.9<-lm(log(N.Desc.glob) ~ detection.lag,data=sum.boots[[k]])
summary(mod.9) #N/S

#SETUP FOR PLOT
mod.9.text<-make.lm.eqn(mod.9)
#set positions
max.x<-max(sum.boots[[k]]$detection.lag)
max.y<-max(log(sum.boots[[k]]$N.Desc.glob))
pred.mod.9<-predict(mod.9,interval="confidence")
dat.mod.9 <-cbind(sum.boots[[k]], pred.mod.9)

#### Plot number of descendants by detection lag, colored by prov, 
###with eqn for line and line
ggplot(dat.mod.9,aes(x=detection.lag))+
  geom_point(aes(y=log(N.Desc.glob), color=Node.Location))+
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"), panel.background=element_rect("grey95"),
        legend.key.size = unit(0.5,"line"),text=element_text(size=10,face="bold"),
        legend.text=element_text(size=8),axis.text.x=element_text(angle=45,hjust = 1))+
  # scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", date_labels = "%b %Y")+
  labs(x="Detection lag (days)",y="log(Number Sampled Descendants)",color="Province\nof introduction")+
  ProvColScale2+
  # annotate("text",label=mod.9.text,x=max.x,y=max.y,hjust=1,color="black",size=3,vjust=1)+
  geom_line(aes(y=fit),color="darkblue",lwd=0.4)+
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill="lightblue",alpha = .5) 

ggsave(paste(f.out,"DetectionLag_vs_logNumberDescendants_byProv_LM.png",sep=""),height=4,width=6,units="in")

#look at adjusting for tmrca.dt here
mod.9b<-lm(log(N.Desc.glob) ~ detection.lag+as.Date(tmrca.dt),data=sum.boots[[k]])
# summary(mod.9)
# summary(mod.9b)
# AIC(mod.9b);AIC(mod.9) #lower AIC for first model
# BIC(mod.9b);BIC(mod.9) #lower BIC for first model
# anova(mod.9,mod.9b,test="LRT") #p=0.0006576 ***
```

## Detection lag over time
```{r}
k=1

#make a model and add the lm fit to the plot
dl.mod<-lm(as.numeric(detection.lag) ~ as.Date(tmrca.dt), data=sum.boots[[k]])
dl.mod.2<-lm(as.numeric(detection.lag) ~ as.Date(tmrca.dt)+Node.Location, data=sum.boots[[k]])
# anova(dl.mod,dl.mod.2) #p=0.03364 *
# summary(dl.mod.2)
# summary(dl.mod) #0.616

dl.mod.3<-lm(as.numeric(detection.lag) ~ Node.Location, data=sum.boots[[k]])
# summary(dl.mod.3)
# BIC(dl.mod);BIC(dl.mod.2) #lower for simpler
# AIC(dl.mod);AIC(dl.mod.2) #support for including node
#support for including node location
sum.boots[[k]]$Node.Location<-factor(sum.boots[[k]]$Node.Location)
newdata2 <- data.frame(
  tmrca.dt = rep(seq(from = min(sum.boots[[k]]$tmrca.dt), to = max(sum.boots[[k]]$tmrca.dt), length.out = 100), 8),
  Node.Location = factor(rep(1:8, each = 100), levels = 1:8, labels =
  levels(sum.boots[[k]]$Node.Location)))

newdata2 <- cbind(newdata2, predict(dl.mod.2, newdata2, type = "response", se.fit=TRUE))
newdata2 <- within(newdata2, {
  detection.lag<- (fit)
  LL <- (fit - 1.96 * se.fit)
  UL <- (fit + 1.96 * se.fit)
})

#Color by province and add linear fit overall
# ggplot(sum.boots[[k]],aes(x=as.Date(tmrca.dt),y=detection.lag))+
#   geom_point(aes(color=Node.Location,alpha=Node.Likelihood))+
#   theme_bw()+
#   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
#         axis.line = element_line(colour = "black"), panel.background=element_rect("grey95"),
#         legend.key.size = unit(0.5,"line"),text=element_text(size=10,face="bold"),
#         legend.text=element_text(size=8),axis.text.x=element_text(angle=45,hjust = 1))+
#   scaleDateBusy+
#   labs(x="Date of most recent common ancestor",y="Detection lag (days)",color="Province\nof introduction")+
#   theme(legend.position = "none")+
#   GlobColScale+
#   GlobFillScale+
#   # annotate("text",label=dl.mod.text,x=as.numeric(max.x),y=max.y,hjust=1,color="black",size=3,vjust=1)+
#   geom_line(data=newdata2,aes(y=detection.lag,color=Node.Location),lwd=0.4)+
#   geom_ribbon(data=newdata2,aes(ymin = UL, ymax = LL,fill=Node.Location),alpha = .5) 
#  
#repeat with one fit
# dl.mod
# summary(dl.mod) #  0.616
#as.Date(tmrca.dt)  -0.02144    0.00923  -2.323   0.0205 *
# -120*-0.02144 #120 days earlier, 2.5728 times longer the detection lag
newdata2 <- data.frame(
  tmrca.dt = rep(seq(from = min(sum.boots[[k]]$tmrca.dt), to = max(sum.boots[[k]]$tmrca.dt), length.out = 100), 8))

newdata2 <- cbind(newdata2, predict(dl.mod, newdata2, type = "response", se.fit=TRUE))
newdata2 <- within(newdata2, {
  detection.lag<- (fit)
  LL <- (fit - 1.96 * se.fit)
  UL <- (fit + 1.96 * se.fit)
})

#Color by province and add linear fit overall
px3<-ggplot(sum.boots[[k]],aes(x=as.Date(tmrca.dt),y=as.numeric(detection.lag)))+
  geom_point(aes(color=Lineage),alpha=0.6,size=1.5)+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"), panel.background=element_rect("grey95"),
        legend.key.size = unit(0.5,"line"),text=element_text(size=10,face="bold"),
        legend.text=element_text(size=8),axis.text.x=element_text(angle=45,hjust = 1))+
scaleDateFlex+
  scale_y_continuous(limits=c(0,205),expand=c(0.01,0))+
  labs(x="Date of most recent common ancestor",y="Detection lag (days)",color="Province\nof introduction")+
  theme(legend.position = "none")+
  LinColScale+
  geom_line(data=newdata2,aes(y=detection.lag),lwd=0.4,color="grey30")+
  geom_ribbon(data=newdata2,aes(ymin = UL, ymax = LL),fill="grey30",alpha = .5) 
px3
ggsave(paste(f.out,"DetectionLag_SampleDateLSD-tMRCA.png",sep=""),height=4,width=4,units="in")

```

## assemble compostite plot of sublineage size over time, longevity, detection lag and days since importation
```{r}
plot.modz<-plot_grid(p3,P5,p4,px3, ncol=2, labels=c("A","B","C","D"),align="hv")
plot.modz

plot.modz
ggsave(paste(f.out,"/SublineagesOvertime_mod.png",sep=""),height=7,width=8,units="in")

```

## plot distrib of detection lag by prov
```{r}
dl.summ<-sum.boots[[k]] %>% group_by(Node.Location) %>% 
  dplyr::summarize(.groups="rowwise",
                   mean.dl=mean(detection.lag), 
                   median.dl=median(detection.lag), 
                   total=n())
dl.summ
# are detection lags sign'ly diff b/w provinces
kruskal.test(as.numeric(sum.boots[[k]]$detection.lag), sum.boots[[k]]$Node.Location) # p-value = 0.001977
pairwise.wilcox.test(as.numeric(sum.boots[[k]]$detection.lag), sum.boots[[k]]$Node.Location,
                 p.adjust.method = "bonferroni") #n/s
# 0.00068 quebec vs BC
dunn.test::dunn.test(as.numeric(sum.boots[[k]]$detection.lag), sum.boots[[k]]$Node.Location, method="bonferroni")
# p-value =  0.0004*  b/w quebec and BC

#which ones are 
ord1<-dl.summ[rev(order(dl.summ$median.dl)),]$Node.Location
sum.boots[[k]]$Node.Location<-factor(sum.boots[[k]]$Node.Location,levels=ord1)
px2<-ggplot(sum.boots[[k]])+
  geom_boxplot(aes(x=Node.Location,group=Node.Location,y=detection.lag))+
  geom_point(aes(x=Node.Location,group=Node.Location,y=detection.lag,color=Node.Location),alpha=0.8)+
  GlobColScale+
  pubThemeDate+
  labs(x=NULL,y="Detection lag (days)")+
  theme(legend.position="none")+
  geom_text(data=dl.summ,aes(label=paste("n=",total,sep=""),x=Node.Location,y=-5),size=3,vjust=1)
  # geom_segment(aes(x=2, xend=5, y=310, yend=310),size=0.1)+
  # geom_text(aes(label="p = 0.0002",x=3.5,  y=314 ),size=2.5,fontface="plain",vjust=0)
px2
ggsave(paste(f.out,"DetectionLag_by_province_boxplotpoints.png",sep=""),width=4,height=4,units="in")
```

# sublineages' dates over time
```{r}
# head(sublin.long.unq[[k]])
k=1

# plot these dates over time for each sublinage
sublin.long.unq[[k]]$tmrca.dt<-as.Date(sublin.long.unq[[k]]$tmrca.dt)
sublin.long.unq[[k]]$Sublineage <- reorder(sublin.long.unq[[k]]$Sublineage, sublin.long.unq[[k]]$tmrca.dt)
sublin.long.unq[[k]]$tmrca.upper.dt<-as.Date(sublin.long.unq[[k]]$tmrca.upper.dt)
sublin.long.unq[[k]]$tmrca.lower.dt<-as.Date(sublin.long.unq[[k]]$tmrca.lower.dt)

## PLOT
sublin.long.unq[[k]] %>% 
  filter(State %in% provs) %>%
  ggplot(aes(y=Sublineage,color=Lineage))+
  geom_linerangeh(aes(xmin=tmrca.lower.dt,xmax=tmrca.upper.dt),lwd=0.1,linetype="dotted")+
  geom_linerangeh(aes(xmin=tmrca.dt,xmax=FirstCanDateLSD),linetype="44",lwd=0.1)+ #detection lag
  geom_linerangeh(aes(xmin=FirstCanDateLSD,xmax=LastCanDateLSD),lwd=0.1)+
  # geom_point(aes(x=Date),alpha=0.7,size=0.5)+
  # geom_point(aes(x=FirstCanDateLSD),shape=22,size=0.7)+
  geom_point(aes(x=tmrca.dt),shape=18,size=0.5)+
  theme(#axis.text.y=element_text(size=1.3),
        axis.ticks.y=element_blank(),
        #axis.text.y=element_blank(), 
        axis.text.x=element_text(angle=45,hjust = 1),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          panel.background=element_rect("grey95"),
          legend.key.size = unit(0.5,"line"),
          text=element_text(size=10,face="bold"),
          legend.text=element_text(size=8),
        legend.position="top")+
    # scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", date_labels = "%b %Y")+
  labs(x=NULL)+
  # ProvColScale2+
  LinColScale+
  scale_y_discrete(expand = c(0.01,0.01))+
  theme(legend.position = "none")+
  pubThemeDate
  
ggsave(paste(f.out,"/SamplesOverTimeForSublineages.png",sep=""),height=5,width=8,units="in")

## ANOTHER version, with only key sublineages shown
key.subs<-sum.boots[[k]]$Sublineage [sum.boots[[k]]$N.Desc.can>50]
sum.boots[[k]]$tmrca.upper.dt<-as.Date(sum.boots[[k]]$tmrca.upper.dt)
sum.boots[[k]]$tmrca.lower.dt<-as.Date(sum.boots[[k]]$tmrca.lower.dt)

sum.boots[[k]] %>% 
  filter(Sublineage %in% key.subs) %>%
  ggplot(aes(y=Sublineage,color=Lineage))+
  theme(legend.position = "none")+
  geom_linerangeh(aes(xmin=FirstCanDateLSD,xmax=LastCanDateLSD,lwd=0.5),fill=0.7)+
  geom_linerangeh(aes(xmin=tmrca.lower.dt,xmax=tmrca.upper.dt),lwd=0.5,linetype="dotted")+
  geom_linerangeh(aes(xmin=tmrca.dt,xmax=FirstCanDateLSD),linetype="44",lwd=0.3)+ #detection lag  
  theme(axis.text.y=element_text(size=5),axis.ticks.y=element_blank(),
        #axis.text.y=element_blank(), 
        axis.text.x=element_text(angle=45,hjust = 1),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          panel.background=element_rect("grey95"),
          legend.key.size = unit(0.5,"line"),
          text=element_text(size=10,face="bold"),
          legend.text=element_text(size=8),
        legend.position="top")+
    scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", 
                 date_labels = "%b %Y")+
  labs(x=NULL)+
  # ProvColScale2+
  LinColScale+
  scale_y_discrete(expand = c(0.01,0.01))+
  theme(legend.position = "none")+
  pubThemeDate

ggsave(paste(f.out,"/SublineageCartoonExpansion.png",sep=""),height=4,width=8.5,units="in")

```

## Main sublineages as ggridge plot
```{r}
# length(which(sum.boots[[k]]$N.Desc.can>100))
#key sublineages
key.subs<-sum.boots[[k]]$Sublineage [sum.boots[[k]]$N.Desc.can>100]

#order of these sublins by first can sample date LSD
sum.boots[[k]]$FirstCanDateLSD[match(key.subs,sum.boots[[k]]$Sublineage)] #these are the right order
sublin.ord<-sum.boots[[k]]$Sublineage[match(key.subs,sum.boots[[k]]$Sublineage)]

temp.df<-sublin.long.unq[[k]] %>%
  filter(Sublineage %in% key.subs)

#order by first can sample date
temp.df$Sublineage<-factor(temp.df$Sublineage, levels=sublin.ord)

temp.df.count <- temp.df %>%
  dplyr::group_by(Sublineage,Date) %>%
  dplyr::summarize(.groups="rowwise",count=n())
temp.df.count<-temp.df.count %>% left_join( temp.df[,c("Sublineage","Lineage","tmrca.dt")],by="Sublineage")

#filter to only canadian samples
temp.df <- temp.df %>%
  filter(State %in% provs)
temp.df$label.x<-temp.df$tmrca.dt
temp.df<-temp.df %>% left_join(sum.boots[[k]][,c("Sublineage","N.Desc.can")], by = "Sublineage")
temp.df$Sublineage<-factor(temp.df$Sublineage, levels=sublin.ord)

temp.df$tmrca.dt<-as.Date(temp.df$tmrca.dt)
temp.df$Date<-as.Date(temp.df$Date)
temp.df$label.x<-as.Date(temp.df$label.x)

p.ridge<-ggplot(temp.df, aes(x=Date, y=Sublineage,fill=Lineage),alpha=0.7) + 
  geom_point(aes(x=tmrca.dt,color=Lineage),shape=23,size=0.5)+
  # geom_linerangeh(aes(xmin=as.Date(tmrca.lower.dt),xmax=as.Date(tmrca.upper.dt),color=Lineage),
                  # lwd=0.5,linetype="dotted")+
  geom_density_ridges(color="white",lwd=0.1,rel_min_height = 0.01, scale = 2)+
  geom_text(aes(label=Sublineage,x=label.x,color=Lineage),hjust=1,size=3,vjust=-1.7)+
  pubThemeDate+
  theme(legend.position = "none",
        axis.text.y=element_blank(),
        axis.text.x=element_text(size=8),
        plot.margin = unit(c(14,4,4,4), "pt"),
        axis.ticks.y=element_blank())+
  LinFillScale+ 
  LinColScale+
  labs(x=NULL, y= "Sublineages with 100+ Canadian descendants")+
  scale_x_date(date_labels = "%b %Y",expand=c(0,0),
               limits=as.Date(c("2020-07-01","2021-12-01")))+
  scale_y_discrete(expand=c(0.01,0)) + 
  # add annotations for n Can desc
  geom_text(aes(label=paste("n = ",N.Desc.can),
                x=label.x,color=Lineage),hjust=1,size=2,vjust=-0.8)

p.ridge
ggsave(paste(f.out,"/SublineageCartoonDensity_100plus_n_sml.png",sep=""),
       height=8,width=4,units="in")
  
```

# SINGLETONS
## summarize singletons for results text
```{r}
#summary of singletons
sing.tot<-c()
for (k in 1:n.B){
   sing.tot<-c(sing.tot,nrow(can.sing.boots[[k]] ))
}
cat(paste0("Total singletons (mean, 95%CI)  = ",mean.95ci.X(sing.tot,0) ),
    file=text.out,sep="\n",append=T)

#what was the TOTAL number of intros (singles + sublins)
tot.tot<-c()
for (k in 1:n.B){
   tot.tot<-c(tot.tot,(nrow(can.sing.boots[[k]] ) + nrow(sum.boots[[k]])))
}
cat(paste0("Total intros, singles + sublins (mean, 95%CI)= ",mean.95ci.X(tot.tot,0)  ),
    file=text.out,sep="\n",append=T)

#what proportion were singeltons
cat(paste0("Proportion singletons of all intros = ",
           mean.95ci.X((sing.tot/tot.tot*100),1)),
    file=text.out,sep="\n",append=t)

#what percent did the few large intros account for?
cat(paste0("Proportion intros with >99 desc = ",
          mean.95ci.X( hund/tot.tot*100, 1)),
    file=text.out,sep="\n",append=T)

cat(paste0("Proportion intros with >=500 desc = ",
          mean.95ci.X( fvhund/tot.tot*100, 1)),
    file=text.out,sep="\n",append=T)

```

## PROPORTION OF importations leading to sublineage analysis
## proportion of importations from each country of origin leading to singleton vs an introduction BY COUNTRY
```{r}
allimportz.boots<-replicate(n.B,vector())
for (k in 1:n.B){
  #what proportion of importations from each country resulted in a singleton vs an introdctuion leading to ongoing transmission, by month?
  singz<-as.data.frame.matrix(table(can.sing.boots[[k]]$par.state,can.sing.boots[[k]]$yearmonth))
  subz<-as.data.frame.matrix(table(sum.boots[[k]]$Parent.Location,sum.boots[[k]]$yearmonth))
  
  #go from wide to long
  singz$country<-rownames(singz)
  singz.long<-singz %>% pivot_longer(1:(ncol(singz)-1),names_to = "yearmonth")
  colnames(singz.long)[3]<-"singz"
    
  subz$country<-rownames(subz)
  subz.long<-subz %>% pivot_longer(1:(ncol(subz)-1),names_to = "yearmonth")
  colnames(subz.long)[3]<-"subz"
  
  allimportz<-left_join(singz.long,subz.long,by=c("country","yearmonth"))
  allimportz.boots[[k]]<-allimportz %>% mutate(total=subz+singz, prop.subz=subz/total)
}

#summarize across boots
summ.allimportz.boots<-bind_rows(allimportz.boots)
summary.allimportz.boots<-summ.allimportz.boots %>% 
  dplyr::group_by (country, yearmonth) %>%
  dplyr::summarise(.groups="rowwise",
                   mean.propsubz=mean(prop.subz)) %>%
  dplyr::ungroup() %>% as.data.frame()

  # #statistical test (different over time) 0.4631
  kruskal.test(summary.allimportz.boots$yearmonth, summary.allimportz.boots$mean.propsubz) #not signf  0.4713
  kruskal.test(summary.allimportz.boots$country, summary.allimportz.boots$mean.propsubz) #not signif  0.4719
  
## consider showing as boxplot with points behind
#USE IN PUBLlCATION GROB BELOW
PX2<-summary.allimportz.boots  %>%
  filter(!yearmonth %in% c("2021-07","2021-08","2021-09")) %>% #empty
  ggplot()+
    geom_boxplot(aes(x=yearmonth,y=mean.propsubz))+
    geom_point(aes(x=yearmonth,y=mean.propsubz,group=country,color=country),position=position_dodge2(width=0.4),alpha=0.9)+
    pubThemeDate+
    theme(legend.position="top")+
  labs(x=NULL,y="Prop. importations resulting in sublineage")+
  CountryColScale+
  guides(color=guide_legend(title="Origin\nlocation"))+
  coord_cartesian(clip="off")
PX2
ggsave(paste(f.out,"Boxplot with POINT Prop. of importations resulting in sublineage BY ORIGIN.png",sep=""),width=5,height=4,units="in")

```

## proportion of importations into each province leading to singleton vs an introduction BY PROVINCE
```{r}
allimportz.boots<-replicate(n.B,vector())
for (k in 1:n.B){
  #what proportion of importations from each country resulted in a singleton vs an introdctuion leading to ongoing transmission, by month?
  singz<-as.data.frame.matrix(table(can.sing.boots[[k]]$tip.state,can.sing.boots[[k]]$yearmonth))
  subz<-as.data.frame.matrix(table(sum.boots[[k]]$Node.Location,sum.boots[[k]]$yearmonth))
  
  #go from wide to long
  singz$country<-rownames(singz)
  singz.long<-singz %>% pivot_longer(1:(ncol(singz)-1),names_to = "yearmonth")
  colnames(singz.long)[3]<-"singz"
    
  subz$country<-rownames(subz)
  subz.long<-subz %>% pivot_longer(1:(ncol(subz)-1),names_to = "yearmonth")
  colnames(subz.long)[3]<-"subz"
  
  allimportz<-left_join(singz.long,subz.long,by=c("country","yearmonth"))
  allimportz.boots[[k]]<-allimportz %>% mutate(total=subz+singz, prop.subz=subz/total)
}

#summarize across boots
summ.allimportz.boots<-bind_rows(allimportz.boots)
summary.allimportz.boots<-summ.allimportz.boots %>% 
  dplyr::group_by (country, yearmonth) %>%
  dplyr::summarise(.groups="rowwise",
                   mean.propsubz=mean(prop.subz)) %>%
  dplyr::ungroup() %>% as.data.frame()

# #statistical test (different over time)
kruskal.test(summary.allimportz.boots$yearmonth, summary.allimportz.boots$mean.propsubz) #0.3665
kruskal.test(summary.allimportz.boots$country, summary.allimportz.boots$mean.propsubz) #not signif = 0.5286
#compare means acros month
# summary.allimportz.boots %>% dplyr::group_by(yearmonth) %>%
#   dplyr::summarise(.groups="rowwise",mean(mean.propsubz,na.rm=T))
# summary.allimportz.boots %>% dplyr::group_by(country) %>%
#   dplyr::summarise(.groups="rowwise",mean(mean.propsubz,na.rm=T))

#boxplot with points
PX1<-summary.allimportz.boots %>%
    filter(!yearmonth %in% c("2021-07","2021-08","2021-09")) %>% #empty
  ggplot()+
    geom_boxplot(aes(x=yearmonth,y=mean.propsubz))+
    geom_point(aes(x=yearmonth,y=mean.propsubz,group=country,color=country),position=position_dodge2(width=0.4),alpha=0.9)+
  pubThemeDate+
  theme(legend.position="top")+
  labs(x=NULL,y="Prop. importations resulting in sublineage")+
  guides(color=guide_legend(title="Province",nrow=3))+
  ProvColScale2+
  coord_cartesian(clip="off")
PX1
ggsave(paste(f.out,"Boxplot with POINT Prop. of importations resulting in sublineage BY PROV.png",sep=""),width=5,height=4,units="in")
```

##figure of proportion of importations resulting in sublineage
```{r}
plot_grid(PX1, PX2, labels=c("A","B"))
ggsave(file=paste(f.out,"boxplot_PropImportSublin_BY PROV and ORIGIN.png",sep=""),width=9,height=4,units = "in")
```

## Plot singletons as in Figure 2A and 3A,B

## Simple alluvial overall for singletons
```{r}
#summarize the total number and percent of intros by 1) par. loc, 2) prov of intro, maybe also 3) lineage 
#prep list item
sing.Par.Prov.l<-replicate(n.B,vector())

#go through each list item/subsample
## tabulate instances of location pairs
for (k in 1:n.B){
  sing.Par.Prov.l[[k]]<-can.sing.boots[[k]] %>% dplyr::group_by (par.state, tip.state) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)

  ## add a column for subsample
  sing.Par.Prov.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])

}

## rbind the summaries

sum.Par.Prov<-bind_rows(sing.Par.Prov.l)

#summarize the mean and range for each of 1), 2) and 3)
sing.Par.Prov.summary<-sum.Par.Prov %>% dplyr::group_by(par.state, tip.state) %>%
  dplyr::summarize(.groups="rowwise",
                   mean.n=round(mean(n.Par)),
                   sd.n=round(sd(n.Par,na.rm=T),digits=2),
                   mean.perc=round(mean(perc.Par),digits=2),
                   sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
  as.data.frame()

tot.Par<-sing.Par.Prov.summary %>% dplyr::group_by(par.state) %>%
  dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.n)) %>%
  as.data.frame()

## Alluvial plot for Figure 2
#make a "subject column"
sing.Par.Prov.summary$subject<-1:nrow(sing.Par.Prov.summary)
# sing.Par.Prov.summary<-sing.Par.Prov.summary[-which(sing.Par.Prov.summary$mean.n<1),]

#order the geos
ord.count<-sing.Par.Prov.summary%>% dplyr::group_by(par.state) %>%
  dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
sing.Par.Prov.summary$par.state<-factor(sing.Par.Prov.summary$par.state,levels=ord.count)

ord.prov<-sing.Par.Prov.summary%>% dplyr::group_by(tip.state) %>%
  dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
ord.prov<-ord.prov[rev(order(ord.prov$n)),'tip.state']
sing.Par.Prov.summary$tip.state<-factor(sing.Par.Prov.summary$tip.state,levels=ord.prov)


#make it long
sing.Par.Prov.summary.long<-sing.Par.Prov.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
#order the geo types
sing.Par.Prov.summary.long$geo.type<-factor(sing.Par.Prov.summary.long$geo.type,levels=c("par.state","tip.state"),labels=c("Origin","Destination"))

## Alluvial plot
P1.singz<- ggplot(sing.Par.Prov.summary.long,
       aes(x = geo.type, stratum = geo, alluvium = subject,
           y = mean.n,
           fill = geo, label = geo)) +
  scale_x_discrete(expand = c(0.01,0.01)) +
  scale_y_continuous(expand = c(0,0)) +
  geom_flow(alpha = 0.6,width=0.45) +
  geom_stratum(alpha = 0.9,width=0.45) +
  geom_text(stat = "stratum", size = 3.4,min.y=10,fontface="bold") +
  pubTheme+
  theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
        axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)))+
  labs(x=NULL,y=paste0("# singletons"))+
  GlobFillScale
P1.singz
ggsave(paste(f.out,"alluvial.singletons.Parent.Node.png",sep=""),height=6,width=5,units="in")
```

## Repeat with pre-intervention; during intervention; post-intervention
```{r}
if(int.yn==T){
  sing.Par.Prov.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sing.Par.Prov.l[[k]]<-can.sing.boots[[k]] %>%
      filter(date<as.Date(int.start)) %>%
      dplyr::group_by (par.state, tip.state) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sing.Par.Prov.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  ## rbind the summaries
  sing.Par.Prov<-bind_rows(sing.Par.Prov.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sing.Par.Prov.summary<-sing.Par.Prov %>% dplyr::group_by(par.state, tip.state) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  tot.Par<-sing.Par.Prov.summary %>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
    as.data.frame()
  
  ## Alluvial plot for Figure 2
  #make a "subject column"
  sing.Par.Prov.summary$subject<-1:nrow(sing.Par.Prov.summary)
  # sing.Par.Prov.summary<-sing.Par.Prov.summary[-which(sing.Par.Prov.summary$mean.n<1),]
  
  sing.Par.Prov.summary$tip.state<-str_replace_all(sing.Par.Prov.summary$tip.state,"British Columbia","British\nColumbia")

  #order the geos
  ord.count<-sing.Par.Prov.summary%>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
  sing.Par.Prov.summary$par.state<-factor(sing.Par.Prov.summary$par.state,levels=ord.count)
  
  ord.prov<-sing.Par.Prov.summary%>% dplyr::group_by(tip.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'tip.state']
  sing.Par.Prov.summary$tip.state<-factor(sing.Par.Prov.summary$tip.state,levels=ord.prov)
  
  #make it long
  sing.Par.Prov.summary.long<-sing.Par.Prov.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sing.Par.Prov.summary.long$geo.type<-factor(sing.Par.Prov.summary.long$geo.type,levels=c("par.state","tip.state"),labels=c("Origin","Destination"))
  
  
  # sing.Par.Prov.summary.long$geo[which(!sing.Par.Prov.summary.long$geo %in% names(globalPalette.ch))]
  
  #BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
  sing.Par.Prov.summary.long$metric<-sing.Par.Prov.summary.long$mean.perc
  matchy.origin<-which(sing.Par.Prov.summary.long$geo.type=="Origin")
  sing.origin<-sum(sing.Par.Prov.summary.long$mean.perc[matchy.origin])
  sing.Par.Prov.summary.long$metric[matchy.origin]<-sing.Par.Prov.summary.long$mean.perc[matchy.origin]/sing.origin*100
  # sum(sing.Par.Prov.summary.long$metric[matchy.origin]) #should be 100
  matchy.prov<-which(sing.Par.Prov.summary.long$geo.type=="Destination")
  sing.prov<-sum(sing.Par.Prov.summary.long$mean.perc[matchy.prov])
  sing.Par.Prov.summary.long$metric[matchy.prov]<-sing.Par.Prov.summary.long$mean.perc[matchy.prov]/sing.prov*100
  # sum(sing.Par.Prov.summary.long$metric[matchy.prov])
  #Modified palette with weird names TODO
  globalPalette.ch.mod<-globalPalette.ch
  names(globalPalette.ch.mod)<-str_replace_all(names(globalPalette.ch.mod),c("British Columbia"="British\nColumbia",focal.source=focal.source))
  GlobFillScale.mod<-scale_fill_manual(name = "Location",values = globalPalette.ch.mod,na.value="grey60")
  GlobColScale.mod<-scale_color_manual(name = "Location",values = globalPalette.ch.mod,na.value="grey60")
  
  
  ## Alluvial plot
  P1.S.wave1<- ggplot(sing.Par.Prov.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric,
             fill = geo, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(alpha = 0.6,width=0.55) +
    geom_stratum(alpha = 0.8,width=0.55) +
    geom_text(stat = "stratum", size = 3.4,min.y=5,fontface="bold") +
    pubTheme+
    theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.7)),
          plot.margin=margin(4,4,4,4,unit="pt"))+
    labs(x=NULL,y="% singletons, before intervention")+
    GlobFillScale.mod
  P1.S.wave1
  # ggsave(paste(f.out,"WAVE1.Alluvial.percent.Parent.Node.png",sep=""),height=8,width=5,units="in")
  
  ### WAVE 2 DURING BAN####
  #prep list item
  sing.Par.Prov.2.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sing.Par.Prov.2.l[[k]]<-can.sing.boots[[k]] %>%
      filter(date>=as.Date(int.start)) %>%
      filter(date<as.Date(int.end)) %>%
      dplyr::group_by (par.state, tip.state) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sing.Par.Prov.2.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  ## rbind the summaries
  sing.Par.Prov<-bind_rows(sing.Par.Prov.2.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sing.Par.Prov.2.summary<-sing.Par.Prov %>% dplyr::group_by(par.state, tip.state) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  tot.Par<-sing.Par.Prov.2.summary %>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
    as.data.frame()
  
  ## Alluvial plot for Figure 2
  #make a "subject column"
  sing.Par.Prov.2.summary$subject<-1:nrow(sing.Par.Prov.2.summary)
  # sing.Par.Prov.2.summary<-sing.Par.Prov.2.summary[-which(sing.Par.Prov.2.summary$mean.n<1),]
  
  
  sing.Par.Prov.2.summary$tip.state<-str_replace_all(sing.Par.Prov.2.summary$tip.state,"British Columbia","British\nColumbia")
  sing.Par.Prov.2.summary$par.state<-str_replace_all(sing.Par.Prov.2.summary$par.state,focal.source,focal.source)
  
  #order the geos
  ord.count<-sing.Par.Prov.2.summary%>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
  sing.Par.Prov.2.summary$par.state<-factor(sing.Par.Prov.2.summary$par.state,levels=ord.count)
  
  ord.prov<-sing.Par.Prov.2.summary%>% dplyr::group_by(tip.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'tip.state']
  sing.Par.Prov.2.summary$tip.state<-factor(sing.Par.Prov.2.summary$tip.state,levels=ord.prov)
  
  
  #make it long
  sing.Par.Prov.2.summary.long<-sing.Par.Prov.2.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sing.Par.Prov.2.summary.long$geo.type<-factor(sing.Par.Prov.2.summary.long$geo.type,levels=c("par.state","tip.state"),labels=c("Origin","Destination"))
  
  # sing.Par.Prov.2.summary.long$geo[which(!sing.Par.Prov.2.summary.long$geo %in% names(globalPalette.ch))]
  
  #BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
  sing.Par.Prov.2.summary.long$metric<-sing.Par.Prov.2.summary.long$mean.perc
  matchy.origin<-which(sing.Par.Prov.2.summary.long$geo.type=="Origin")
  sing.origin<-sum(sing.Par.Prov.2.summary.long$mean.perc[matchy.origin])
  sing.Par.Prov.2.summary.long$metric[matchy.origin]<-sing.Par.Prov.2.summary.long$mean.perc[matchy.origin]/sing.origin*100
  # sum(sing.Par.Prov.2.summary.long$metric[matchy.origin]) #should be 100
  matchy.prov<-which(sing.Par.Prov.2.summary.long$geo.type=="Destination")
  sing.prov<-sum(sing.Par.Prov.2.summary.long$mean.perc[matchy.prov])
  sing.Par.Prov.2.summary.long$metric[matchy.prov]<-sing.Par.Prov.2.summary.long$mean.perc[matchy.prov]/sing.prov*100
  # sum(sing.Par.Prov.2.summary.long$metric[matchy.prov])
  ## Alluvial plot
  
  P1.S.wave2<- ggplot(sing.Par.Prov.2.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric,
             fill = geo, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(alpha = 0.6,width=0.55) +
    geom_stratum(alpha = 0.8,width=0.55) +
    geom_text(stat = "stratum", size = 3.4,min.y=5,fontface="bold") +
    pubTheme+
    theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.7)),
                  plot.margin=margin(4,4,4,4,unit="pt"))+
    labs(x=NULL,y="% singletons, during intervention")+
    GlobFillScale.mod
  P1.S.wave2
  # ggsave(paste(f.out,"WAVE2.Alluvial.percent.Parent.Node.png",sep=""),height=8,width=5,units="in")
  
  
  ### WAVE 3 AFTER BAN####
  #prep list item
  sing.Par.Prov.3.l<-replicate(n.B,vector())
  
  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    sing.Par.Prov.3.l[[k]]<-can.sing.boots[[k]] %>%
      filter(date>=as.Date(int.end)) %>%
      dplyr::group_by (par.state, tip.state) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)
  
    ## add a column for subsample
    sing.Par.Prov.3.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])
  
  }
  
  ## rbind the summaries
  sing.Par.Prov<-bind_rows(sing.Par.Prov.3.l)
  
  #summarize the mean and range for each of 1), 2) and 3)
  sing.Par.Prov.3.summary<-sing.Par.Prov %>% dplyr::group_by(par.state, tip.state) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()
  
  tot.Par<-sing.Par.Prov.3.summary %>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups = "rowwise", totalImports=sum(mean.perc)) %>%
    as.data.frame()
  
  ## Alluvial plot for Figure 2
  #make a "subject column"
  sing.Par.Prov.3.summary$subject<-1:nrow(sing.Par.Prov.3.summary)
  # sing.Par.Prov.3.summary<-sing.Par.Prov.3.summary[-which(sing.Par.Prov.3.summary$mean.n<1),]
  
  
  sing.Par.Prov.3.summary$tip.state<-str_replace_all(sing.Par.Prov.3.summary$tip.state,"British Columbia","British\nColumbia")
  sing.Par.Prov.3.summary$par.state<-str_replace_all(sing.Par.Prov.3.summary$par.state,focal.source,focal.source)
  
  #order the geos
  ord.count<-sing.Par.Prov.3.summary%>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
  sing.Par.Prov.3.summary$par.state<-factor(sing.Par.Prov.3.summary$par.state,levels=ord.count)
  
  ord.prov<-sing.Par.Prov.3.summary%>% dplyr::group_by(tip.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'tip.state']
  sing.Par.Prov.3.summary$tip.state<-factor(sing.Par.Prov.3.summary$tip.state,levels=ord.prov)
  
  
  #make it long
  sing.Par.Prov.3.summary.long<-sing.Par.Prov.3.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sing.Par.Prov.3.summary.long$geo.type<-factor(sing.Par.Prov.3.summary.long$geo.type,levels=c("par.state","tip.state"),labels=c("Origin","Destination"))
  
  # sing.Par.Prov.3.summary.long$geo[which(!sing.Par.Prov.3.summary.long$geo %in% names(globalPalette.ch))]
  
  #BECAUSE now showing a percent, rescale to equal 100 (means might not sum to 100)
  sing.Par.Prov.3.summary.long$metric<-sing.Par.Prov.3.summary.long$mean.perc
  matchy.origin<-which(sing.Par.Prov.3.summary.long$geo.type=="Origin")
  sing.origin<-sum(sing.Par.Prov.3.summary.long$mean.perc[matchy.origin])
  sing.Par.Prov.3.summary.long$metric[matchy.origin]<-sing.Par.Prov.3.summary.long$mean.perc[matchy.origin]/sing.origin*100
  # sum(sing.Par.Prov.3.summary.long$metric[matchy.origin]) #should be 100
  matchy.prov<-which(sing.Par.Prov.3.summary.long$geo.type=="Destination")
  sing.prov<-sum(sing.Par.Prov.3.summary.long$mean.perc[matchy.prov])
  sing.Par.Prov.3.summary.long$metric[matchy.prov]<-sing.Par.Prov.3.summary.long$mean.perc[matchy.prov]/sing.prov*100
  # sum(sing.Par.Prov.3.summary.long$metric[matchy.prov])
  ## Alluvial plot
  
  P1.S.wave3<- ggplot(sing.Par.Prov.3.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric,
             fill = geo, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(alpha = 0.6,width=0.55) +
    geom_stratum(alpha = 0.8,width=0.55) +
    geom_text(stat = "stratum", size = 3.4,min.y=5,fontface="bold") +
    pubTheme+
    theme(legend.position = "none", axis.line = element_blank(), text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.7)),
                  plot.margin=margin(4,4,4,4,unit="pt"))+
    labs(x=NULL,y="% singletons, after intervention")+
    GlobFillScale.mod
  P1.S.wave3
  # ggsave(paste(f.out,"WAVE2.Alluvial.percent.Parent.Node.png",sep=""),height=8,width=5,units="in")
  
  #grob plot of both waves
  plot_grid(P1.S.wave1,P1.S.wave2,P1.S.wave3,nrow=1,labels=c("A","B","C"))
  ggsave(paste(f.out,"SepWaves.Alluvial.percent.Parent.Node.png",sep=""),height=5,width=9,units="in")
}
```

## Alluvial stratified by lineage, by wave
```{r}
if(mean(un.lins)>1 & int.yn==T){
  #summarize the total number and percent of intros by 1) par. loc, 2) prov of intro, maybe also 3) lineage
  #prep list item
  sing.Par.Prov.L.l<-replicate(n.B,vector())

  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    can.sing.boots[[k]]$date<-as.Date(can.sing.boots[[k]]$tmrca.dt.half)
    sing.Par.Prov.L.l[[k]]<-can.sing.boots[[k]] %>%
      filter(tmrca.dt.half<as.Date(int.start)) %>%
      dplyr::group_by (par.state, tip.state, Lineage) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)

    ## add a column for subsample
    sing.Par.Prov.L.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])

  }

  sing.Par.Prov.L<-bind_rows(sing.Par.Prov.L.l)


  #summarize the mean and range for each of 1), 2) and 3)
  sing.Par.Prov.L.summary<-sing.Par.Prov.L %>% dplyr::group_by(par.state, tip.state, Lineage) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()

  ### Alluvial plot for Figure 2

  #make a "subject column"
  sing.Par.Prov.L.summary$subject<-1:nrow(sing.Par.Prov.L.summary)
  # sing.Par.Prov.L.summary<-sing.Par.Prov.L.summary[-which(sing.Par.Prov.L.summary$mean.n<1),]
  # sum(sing.Par.Prov.summary.long$metric[matchy.prov],na.rm=T) #should be 100
  sing.Par.Prov.L.summary$tip.state<-str_replace_all(sing.Par.Prov.L.summary$tip.state,"British Columbia","British\nColumbia")
  sing.Par.Prov.L.summary$par.state<-str_replace_all(sing.Par.Prov.L.summary$par.state,focal.source,focal.source)

  #order the geos by frequency
  ord.count<-sing.Par.Prov.L.summary%>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
  sing.Par.Prov.L.summary$par.state<-factor(sing.Par.Prov.L.summary$par.state,levels=ord.count)

  ord.prov<-sing.Par.Prov.L.summary%>% dplyr::group_by(tip.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.perc)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'tip.state']
  sing.Par.Prov.L.summary$tip.state<-factor(sing.Par.Prov.L.summary$tip.state,levels=ord.prov)

  #make it long
  sing.Par.Prov.L.summary.long<-sing.Par.Prov.L.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo") %>% as.data.frame()


  #order the geo types and lineages
  sing.Par.Prov.L.summary.long$geo.type<-factor(sing.Par.Prov.L.summary.long$geo.type,levels=c("par.state","tip.state"),labels=c("Origin","Destination"))
  sing.Par.Prov.L.summary.long$Lineage<-factor(sing.Par.Prov.L.summary.long$Lineage,levels=aliases$lineage)


  #re-scale to perfect 100
  sing.Par.Prov.L.summary.long$metric<-sing.Par.Prov.L.summary.long$mean.perc
  matchy.origin<-which(sing.Par.Prov.L.summary.long$geo.type=="Origin")
  sing.origin<-sum(sing.Par.Prov.L.summary.long$mean.perc[matchy.origin])
  sing.Par.Prov.L.summary.long$metric[matchy.origin]<-sing.Par.Prov.L.summary.long$mean.perc[matchy.origin]/sing.origin*100

  sum(sing.Par.Prov.L.summary.long$metric[matchy.origin],na.rm=T) #should be 100
  matchy.prov<-which(sing.Par.Prov.L.summary.long$geo.type=="Destination")
  sing.prov<-sum(sing.Par.Prov.L.summary.long$mean.perc[matchy.prov])
  sing.Par.Prov.L.summary.long$metric[matchy.prov]<-sing.Par.Prov.L.summary.long$mean.perc[matchy.prov]/sing.prov*100

  #plot it
  alluv.sing.w1.lin<-ggplot(sing.Par.Prov.L.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    geom_stratum(alpha = 0.4,width=0.4 )+
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.5,min.y=1,label.padding=unit(0.1, "lines")) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.6,min.y=0.5,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
          plot.margin=margin(6,4,28,4,unit="pt"))+
    labs(x=NULL,y="% singletons, before intervention")
  alluv.sing.w1.lin

  ggsave(paste(f.out,"/Singleton.Alluvial.Wave1.Lineage.Parent.Node.Lin.png",sep=""),height=8,width=5,units="in")

  ### SECOND WAVE ####
  #summarize the total number and percent of intros by 1) par. loc, 2) prov of intro, maybe also 3) lineage
  #prep list item
  sing.Par.Prov.L2.l<-replicate(n.B,vector())

  #go through each list item/subsample
  ## tabulate instances of location pairs
  for (k in 1:n.B){
    can.sing.boots[[k]]$tmrca.dt.half<-as.Date(can.sing.boots[[k]]$tmrca.dt.half)
    sing.Par.Prov.L2.l[[k]]<-can.sing.boots[[k]] %>%
      filter(tmrca.dt.half>=as.Date(int.start)) %>%
      dplyr::group_by (par.state, tip.state, Lineage) %>% dplyr::summarize (.groups="rowwise", n.Par= n()) %>% as.data.frame() %>% mutate(perc.Par=(n.Par/sum(n.Par))*100)

    ## add a column for subsample
    sing.Par.Prov.L2.l[[k]]$SampleSet<-paste("Sample",BOOTS[k])

  }

  sing.Par.Prov.L<-bind_rows(sing.Par.Prov.L2.l)


  #summarize the mean and range for each of 1), 2) and 3)
  sing.Par.Prov.L2.summary<-sing.Par.Prov.L %>% dplyr::group_by(par.state, tip.state, Lineage) %>%
    dplyr::summarize(.groups="rowwise",
                     mean.n=round(mean(n.Par)),
                     sd.n=round(sd(n.Par,na.rm=T),digits=2),
                     mean.perc=round(mean(perc.Par),digits=2),
                     sd.perc=round(sd(perc.Par,na.rm=T),digits=2)) %>%
    as.data.frame()

  ### Alluvial plot for Figure 2

  #make a "subject column"
  sing.Par.Prov.L2.summary$subject<-1:nrow(sing.Par.Prov.L2.summary)
  # sing.Par.Prov.L2.summary<-sing.Par.Prov.L2.summary[-which(sing.Par.Prov.L2.summary$mean.n<1),]

  sing.Par.Prov.L2.summary$tip.state<-str_replace_all(sing.Par.Prov.L2.summary$tip.state,"British Columbia","British\nColumbia")

  #order the geos by frequency
  ord.count<-sing.Par.Prov.L2.summary%>% dplyr::group_by(par.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
  sing.Par.Prov.L2.summary$par.state<-factor(sing.Par.Prov.L2.summary$par.state,levels=ord.count)

  ord.prov<-sing.Par.Prov.L2.summary%>% dplyr::group_by(tip.state) %>%
    dplyr::summarise(.groups="rowwise",n=sum(mean.n)) %>% as.data.frame()
  ord.prov<-ord.prov[rev(order(ord.prov$n)),'tip.state']
  sing.Par.Prov.L2.summary$tip.state<-factor(sing.Par.Prov.L2.summary$tip.state,levels=ord.prov)

  #make it long
  sing.Par.Prov.L2.summary.long<-sing.Par.Prov.L2.summary %>% pivot_longer(1:2, names_to = "geo.type", values_to = "geo")
  #order the geo types
  sing.Par.Prov.L2.summary.long$geo.type<-factor(sing.Par.Prov.L2.summary.long$geo.type,levels=c("par.state","tip.state"),labels=c("Origin","Destination"))

  sing.Par.Prov.L2.summary.long$Lineage<-factor(sing.Par.Prov.L2.summary.long$Lineage,levels=aliases$lineage)

  #re-scale
  sing.Par.Prov.L2.summary.long$metric<-sing.Par.Prov.L2.summary.long$mean.perc
  matchy.origin<-which(sing.Par.Prov.L2.summary.long$geo.type=="Origin")
  sing.origin<-sum(sing.Par.Prov.L2.summary.long$mean.perc[matchy.origin])
  sing.Par.Prov.L2.summary.long$metric[matchy.origin]<-sing.Par.Prov.L2.summary.long$mean.perc[matchy.origin]/sing.origin*100
  sum(sing.Par.Prov.L2.summary.long$metric[matchy.origin],na.rm=T) #should be 100
  matchy.prov<-which(sing.Par.Prov.L2.summary.long$geo.type=="Destination")
  sing.prov<-sum(sing.Par.Prov.L2.summary.long$mean.perc[matchy.prov])
  sing.Par.Prov.L2.summary.long$metric[matchy.prov]<-sing.Par.Prov.L2.summary.long$mean.perc[matchy.prov]/sing.prov*100

  #plot it
  alluv.sing.w2.lin<-ggplot(sing.Par.Prov.L2.summary.long,
         aes(x = geo.type, stratum = geo, alluvium = subject,
             y = metric, label = geo)) +
    scale_x_discrete(expand = c(0.01,0.01)) +
    scale_y_continuous(expand = c(0,0)) +
    geom_flow(aes(fill = Lineage),alpha = 1,width=0.4) +
    LinFillScale+
    # new_scale("fill")+ #sneaky
    geom_stratum(alpha = 0.4,width=0.4) +
    scale_color_manual(name = "geo",values = globalPalette.ch,na.value="grey60")+
    geom_text(stat = "stratum", size = 3.5,min.y=1) +
    geom_text(stat="flow",aes(label=Lineage,color=Lineage),size=1.6,min.y=0.5,color="white",
              nudge_x=-0.206,hjust=1)+
    pubTheme+
    theme(legend.position = "none",text =element_text(size=14),
          axis.ticks.x = element_blank(),axis.text.x = element_text(hjust=c(0.5,0.5)),
          plot.margin=margin(6,4,28,4,unit="pt"))+ #accommodate the lineage group image
    labs(x=NULL,y="% singletons, after intervention")
  alluv.sing.w2.lin

  ggsave(paste(f.out,"/Singleton.Alluvial.Wave2.Lineage.Parent.Node.Lin.png",sep=""),height=8,width=5,units="in")
}
```

## singletons: rolling rates
```{r}
#mid script lists for rolling means
par.state.sing.boots<-replicate(n=n.B,vector())
par.state.sing.boots.full<-replicate(n=n.B,vector())
tip.state.sing.boots<-replicate(n=n.B,vector())
tip.state.sing.boots.full<-replicate(n=n.B,vector())


for (k in 1:n.B){
  #make sure this a date
  can.sing.boots[[k]]$tmrca.dt.half<-as.Date(can.sing.boots[[k]]$tmrca.dt.half)
  
  #### Calculate a rolling 7-day mean for origins ####
  
  ## count the importations by origin location over time
  par.state.sing.boots[[k]]<-can.sing.boots[[k]] %>%
    dplyr::select(par.state, Lineage,tmrca.dt.half) %>%
    dplyr::group_by(tmrca.dt.half, par.state) %>%
    dplyr::summarize(.groups="rowwise", total=n()) %>%
    dplyr::arrange(desc(par.state)) %>% 
    dplyr::group_by(par.state) 
  
  #need to add rows for missing dates
  alldays<-seq(ymd(first(sort(par.state.sing.boots[[k]]$tmrca.dt.half))),
      ymd(last(sort(par.state.sing.boots[[k]]$tmrca.dt.half))),
      by='1 day')
  
  #make a empty df in same structure as above then populate it
  nL<-length(unique(par.state.sing.boots[[k]]$par.state))
  nD<-length(alldays)
  par.state.sing.boots.full[[k]]<-data.frame(tmrca.dt.half=rep(alldays,times=nL),
par.state = sort(rep(unique(par.state.sing.boots[[k]]$par.state),times=nD)),
                                 total=0)
  # nrow(par.state.sing.boots[[k]].empty)==nD*nL      
  
  #populate it
  for (i in 1:nrow(par.state.sing.boots.full[[k]])){
    #look for a match
    match<-which(par.state.sing.boots[[k]]$par.state==par.state.sing.boots.full[[k]]$par.state[i] &
            par.state.sing.boots[[k]]$tmrca.dt.half==par.state.sing.boots.full[[k]]$tmrca.dt.half[i])
    if(length(match)==0) next #no match, no change
    #else, replace:
    par.state.sing.boots.full[[k]]$total[i]<-par.state.sing.boots[[k]]$total[match]
  }
  
  # sum(par.state.sing.boots.full[[k]]$total[par.state.sing.boots.full[[k]]$par.state=="USA"])==sum(par.state.sing.boots[[k]]$total[par.state.sing.boots[[k]]$par.state=="USA"])
  
  par.state.sing.boots.full[[k]]<-par.state.sing.boots.full[[k]] %>% 
    dplyr::mutate(intros_mean7d = zoo::rollmean(total, k = 7, fill = NA),
                  intros_mean14d = zoo::rollmean(total, k = 14, fill = NA),
                  intros_median7d = zoo::rollmedian(total, k = 7, fill = NA),
                  intros_sum7d = zoo::rollsum(total, k = 7, fill = NA,align="right")) %>% #right align to sum all prev
    #rolling mean and median weekly importation rate
    dplyr::mutate(intros_meansum7d = zoo::rollmean(intros_sum7d, k = 7, fill = NA), 
                  intros_mediansum7d = zoo::rollmedian(intros_sum7d, k = 7, fill = NA),) %>%
    dplyr::ungroup()
   
  ## count the importations by tip.state over time
  tip.state.sing.boots[[k]]<-can.sing.boots[[k]] %>%
    dplyr::select(tip.state, Lineage,tmrca.dt.half) %>%
    group_by(tmrca.dt.half, tip.state) %>%
    dplyr::summarize(.groups="rowwise", total=n()) %>%
    dplyr::arrange(desc(tip.state)) %>% 
    dplyr::group_by(tip.state) 
  
  #need to add rows for missing dates
  alldays<-seq(ymd(first(sort(tip.state.sing.boots[[k]]$tmrca.dt.half))),
      ymd(last(sort(tip.state.sing.boots[[k]]$tmrca.dt.half))),
      by='1 day')
  
  #make a empty df in same structure as above then populate it
  nL<-length(unique(tip.state.sing.boots[[k]]$tip.state))
  nD<-length(alldays)
  tip.state.sing.boots.full[[k]]<-data.frame(tmrca.dt.half=rep(alldays,times=nL),
                                 tip.state=sort(rep(unique(tip.state.sing.boots[[k]]$tip.state),times=nD)),
                                 total=0)
  # nrow(tip.state.sing.boots[[k]].empty)==nD*nL      
  
  #populate it
  for (i in 1:nrow(tip.state.sing.boots.full[[k]])){
    #look for a match
    match<-which(tip.state.sing.boots[[k]]$tip.state==tip.state.sing.boots.full[[k]]$tip.state[i] &
            tip.state.sing.boots[[k]]$tmrca.dt.half==tip.state.sing.boots.full[[k]]$tmrca.dt.half[i])
    if(length(match)==0) next #no match, no change
    #else, replace:
    tip.state.sing.boots.full[[k]]$total[i]<-tip.state.sing.boots[[k]]$total[match]
  }
  
  # sum(tip.state.sing.boots.full[[k]]$total[tip.state.sing.boots.full[[k]]$tip.state=="USA"])==sum(tip.state.sing.boots[[k]]$total[tip.state.sing.boots[[k]]$tip.state=="USA"])
  
  tip.state.sing.boots.full[[k]]<-tip.state.sing.boots.full[[k]] %>% 
    dplyr::mutate(intros_mean7d = zoo::rollmean(total, k = 7, fill = NA),
                  intros_mean14d = zoo::rollmean(total, k = 14, fill = NA),
                  intros_median7d = zoo::rollmedian(total, k = 7, fill = NA),
                  intros_sum7d = zoo::rollsum(total, k = 7, fill = NA,align="right")) %>% #right align to sum all prev
    #rolling mean and median weekly importation rate
    dplyr::mutate(intros_meansum7d = zoo::rollmean(intros_sum7d, k = 7, fill = NA), 
                  intros_mediansum7d = zoo::rollmedian(intros_sum7d, k = 7, fill = NA),) %>%
    dplyr::ungroup()

}

## Summarize the rolling means overall

## BY ORIGINS

#go through each list item/subsample
for (k in 1:n.B){
  ## add a column for subsample
  par.state.sing.boots.full[[k]]$SampleSet<-paste("Sample",BOOTS[k])
}

## rbind the summaries
sing.Par.Roll<-bind_rows(par.state.sing.boots.full)

#summarize the mean and confint
sing.Par.Roll.summary<-sing.Par.Roll %>% dplyr::group_by(tmrca.dt.half, par.state) %>%
  dplyr::summarize(.groups="rowwise",
                   intros_meansum7d.mean=(mean(intros_meansum7d,na.rm=T)),
                   intros_meansum7d.sd=sd(intros_meansum7d,na.rm=T)) %>%
  as.data.frame()

#order the geos
ord.count<-sing.Par.Roll.summary%>% dplyr::group_by(par.state) %>%
  dplyr::summarise(.groups="rowwise",n=sum(intros_meansum7d.mean)) %>% as.data.frame()
ord.count<-ord.count[rev(order(ord.count$n)),'par.state']
sing.Par.Roll.summary$par.state<-factor(sing.Par.Roll.summary$par.state,levels=ord.count)

# BY PROVINCE
#go through each list item/subsample
for (k in 1:n.B){
  ## add a column for subsample
  tip.state.sing.boots.full[[k]]$SampleSet<-paste("Sample",BOOTS[k])
}

## rbind the summaries
sing.Prov.Roll<-bind_rows(tip.state.sing.boots.full)
#summarize the mean and confint
sing.Prov.Roll.summary<-sing.Prov.Roll %>% dplyr::group_by(tmrca.dt.half, tip.state) %>%
  dplyr::summarize(.groups="rowwise",
                   intros_meansum7d.mean=(mean(intros_meansum7d,na.rm=T)),
                   intros_meansum7d.sd=(sd(intros_meansum7d,na.rm=T))) %>%
  as.data.frame()

#order the geos
ord.prov.s<-sing.Prov.Roll.summary%>% dplyr::group_by(tip.state) %>%
  dplyr::summarise(.groups="rowwise",n=sum(intros_meansum7d.mean,na.rm=T)) %>% as.data.frame()
ord.prov.s<-ord.prov.s[rev(order(ord.prov.s$n)),'tip.state']
sing.Prov.Roll.summary$tip.state<-factor(sing.Prov.Roll.summary$tip.state,levels=ord.prov.s)
```

## Plot singletons over time using rolling rates, by tmrca, by origin or dest
```{r}
sing.all<-sing.Par.Roll.summary %>% dplyr::group_by(tmrca.dt.half) %>%
  dplyr::summarise(totalmean=sum(intros_meansum7d.mean,na.rm=T))
y.upper<-ceiling(max(sing.all$totalmean,na.rm=T))
#rolling importation rate, by origin
y.lim<-c(0,y.upper)

p1.sing<-sing.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt.half,y=intros_meansum7d.mean,group=par.state,fill=par.state))+
  annotate(geom="rect",xmin = as.Date(int.start), 
           xmax = as.Date(int.end),
             ymin = y.lim[1], ymax =y.lim[2], 
           color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",
           x = (as.Date(int.start)-int.duration/2),y=y.lim[2]-1, 
           vjust=1-0.25, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_density(stat="identity", position="stack",lwd=0,alpha=0.9)+
  CountryFillScale+
  pubThemeDate+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        text=element_text(size=10,face="bold"),
        legend.position = "none")+
  scaleDateFlex+
  labs(x="Sample date",y=paste0("Singletons per week"))+
  scale_y_continuous(expand=c(0,0),limits=y.lim,breaks=seq(0,y.upper,5))+
  guides(fill = guide_legend(keywidth = 0.7,keyheight=0.7,title.position = "top",title="Origin",legend.spacing=0,ncol=4))
# p1.sing

##fake plots to take the legends
p1.f<-sing.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt.half,y=intros_meansum7d.mean,group=par.state,fill=par.state))+
  geom_density(stat="identity", position="stack",lwd=0,alpha=0.9)+
  CountryFillScale+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        text=element_text(size=9,face="bold"),
        legend.position = c(0.1,1),
        legend.margin=margin(0,0,0,0),
        legend.background = element_blank(),
        legend.justification = c(0,1))+
  guides(fill = guide_legend(keywidth = 0.7,keyheight=0.7,title.position = "top",title="Origin",legend.spacing=0,nrow=4))

p1.guide<-get_legend(p1.f)


#rolling importation rate, by province destination
p2.sing<-sing.Prov.Roll.summary %>%
  ggplot(aes(x=tmrca.dt.half,y=intros_meansum7d.mean,group=tip.state,fill=tip.state))+
  annotate(geom="rect",xmin =( as.Date(int.start)), 
           xmax = as.Date(int.end),
             ymin = y.lim[1], ymax =y.lim[2], 
           color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = (as.Date(int.start)+int.duration/2),
           y=y.lim[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_density(stat="identity", position="stack",lwd=0,alpha=0.9)+
  ProvFillScale+
  pubThemeDate+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.position = "none",plot.margin = unit(c(4.5,4.5,4.5,4.5), "pt") #axis.title.y=element_blank()
        )+
  scaleDateFlex+
  labs(x="Sampling date",y=paste0("Singletons per week"))+
  scale_y_continuous(expand=c(0,0),limits=y.lim,breaks=seq(0,y.upper,5))
p2.sing
#fake plot to take legend
p2.f<-sing.Prov.Roll.summary %>%
  ggplot(aes(x=tmrca.dt.half,y=intros_meansum7d.mean,group=tip.state,fill=tip.state))+
  geom_density(stat="identity", position="stack",lwd=0,alpha=0.9)+  
  ProvFillScale+
  theme(axis.text.x=element_text(angle=45,hjust = 1),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background=element_rect("grey95"),
        text=element_text(size=9,face="bold"),
        legend.position = c(0.1,1),
        legend.margin=margin(0,0,0,0),
        legend.background = element_blank(),
        legend.justification = c(0,1))+
    guides(fill = guide_legend(keywidth = 0.7,keyheight=0.7,title.position = "top",title="Province",legend.spacing=0,nrow=4))
p2.guide<-get_legend(p2.f)

#make a list of grobs
## add in the Canadian province representation plots
plot.row<-plot_grid(p1.sing,p2.sing,rel_widths = c(1,0.93),ncol=2,labels=c("A" ,"B"))
plot.leg<-plot_grid(p1.guide,p2.guide,ncol=2,align="h")
plot.all<-plot_grid(plot.leg,plot.row,nrow=2,rel_heights = c(0.25,1),align = "v")

png(file=paste(f.out,"/Singles_OverTime_OriginsDestinations.png",sep=""),
          width=8.5,height=5,units = "in", bg = "white",res=200)
print(plot.all)
dev.off()

```

## Make singletons over time plots using lines
```{r}
##PLOT AS LINE WITH CONFIDENCE INTERVALS
t.here<-qt(0.025,(n.B-1),lower.tail=F)
sing.Par.Roll.summary<-sing.Par.Roll.summary %>% mutate(upper.CI=(intros_meansum7d.mean)+(intros_meansum7d.sd/sqrt(10)*t.here),
       lower.CI=(intros_meansum7d.mean)-(intros_meansum7d.sd/sqrt(10)*t.here))

## Add an indidicator for source country vs not to add alpha channel to other lines
sing.Par.Roll.summary$focal<-0
sing.Par.Roll.summary$focal[which(sing.Par.Roll.summary$par.state%in%focal.source)]<-1
alphavals<-c(0.3,0.9)
names(alphavals)<-c("0","1")  
sing.Par.Roll.summary$focal<-as.factor(sing.Par.Roll.summary$focal)
y.lim2<-c(0,(max(sing.Par.Roll.summary$upper.CI,na.rm=T)+1))
y.jumps2<-ceiling(y.lim2[2]/5)
p1.sing.line.ci<-sing.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt.half,y=intros_meansum7d.mean,group=par.state))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = c(0.8,0.7),
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = int.start, xmax = int.end,
             ymin = y.lim2[1], ymax =y.lim2[2], color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x =( int.start+int.duration/2),
           y=y.lim2[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_ribbon(aes(ymin=lower.CI,ymax=upper.CI, 
                  fill=par.state, alpha=focal))+
  geom_line(stat="identity",lwd=1,aes(alpha=focal,color=par.state))+
  GlobRegColScale+
  GlobRegFillScale+
  scale_alpha_manual(values=alphavals)+
  labs(x="Sample date",y=paste0("Singletons per week"), fill="Origin Location")+
  scaleDateFlex+
  scale_y_continuous(expand=c(0,0),breaks=seq(0,y.lim2[2],y.jumps))+
 guides(fill = guide_legend(keywidth = 0.6,keyheight=0.6,
                              title.position = "top",title="Global origin",
                              legend.spacing=0,ncol=1),
         alpha="none", color="none")+
  coord_cartesian(ylim=y.lim2)
p1.sing.line.ci
ggsave(paste(f.out,"/Rolling-singleton-importRate_line_confint.png",sep=""),height=3,width=3.5)

#Line no CI
p1.sing.line<-sing.Par.Roll.summary %>%
  ggplot(aes(x=tmrca.dt.half,y=intros_meansum7d.mean,group=par.state))+
  pubThemeDate+
  theme(text=element_text(size=10,face="bold"),
        legend.position = c(0.8,0.7),
        legend.text=element_text(size=rel(0.7)),
        legend.title=element_text(size=rel(0.8)),
        legend.margin=margin(1,1,1,1,"pt"))+
  annotate(geom="rect",xmin = int.start, xmax = int.end,
             ymin = y.lim2[1], ymax =y.lim2[2], 
           color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = (int.start+int.duration/2),
           y=y.lim2[2]-0.25, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  geom_line(stat="identity",lwd=1,aes(alpha=focal,color=par.state))+
  GlobRegColScale+
  # GlobRegFillScale+
  scale_alpha_manual(values=alphavals)+
  labs(x="Sample date",y=paste0("Singletons per week"), fill="Origin Location")+
  scaleDateFlex+
  scale_y_continuous(expand=c(0,0),breaks=seq(0,y.lim2[2],y.jumps2))+
  guides(color = guide_legend(keywidth = 0.6,keyheight=0.6,
                              title.position = "top",title="Global origin",
                              legend.spacing=0,ncol=1),
         alpha="none")+
  coord_cartesian(ylim=y.lim2)
p1.sing.line
ggsave(paste(f.out,"/Rolling-singleton-importRate_line.png",sep=""),height=3,width=3.5)

write.csv(sing.Par.Roll.summary, paste0(f.out,"sing.par.roll.summary.csv"))

```


## shortcut to run summaries
```{r}
# sing.Par.Roll.summary<-read.csv(paste0(f.out,"sing.par.roll.summary.csv"))
# can.sing.boots<-replicate(n.B,vector())
# for (k in 1:n.B){
#   can.sing.boots[[k]]<-read.csv(sing.out[k])
#   can.sing.boots[[k]]$tmrca.dt.half<-as.Date(can.sing.boots[[k]]$tmrca.dt.half)
# }
# 
# #mid script lists for rolling means
# par.state.sing.boots<-replicate(n=n.B,vector())
# par.state.sing.boots.full<-replicate(n=n.B,vector())
# 
# for (k in 1:n.B){
#   #make sure this a date
#   can.sing.boots[[k]]$tmrca.dt.half<-as.Date(can.sing.boots[[k]]$tmrca.dt.half)
#   
#   #### Calculate a rolling 7-day mean for origins ####
#   
#   ## count the importations by origin location over time
#   par.state.sing.boots[[k]]<-can.sing.boots[[k]] %>%
#     dplyr::select(par.state, Lineage,tmrca.dt.half) %>%
#     dplyr::group_by(tmrca.dt.half, par.state) %>%
#     dplyr::summarize(.groups="rowwise", total=n()) %>%
#     dplyr::arrange(desc(par.state)) %>% 
#     dplyr::group_by(par.state) 
#   
#   #need to add rows for missing dates
#   alldays<-seq(ymd(first(sort(par.state.sing.boots[[k]]$tmrca.dt.half))),
#       ymd(last(sort(par.state.sing.boots[[k]]$tmrca.dt.half))),
#       by='1 day')
#   
#   #make a empty df in same structure as above then populate it
#   nL<-length(unique(par.state.sing.boots[[k]]$par.state))
#   nD<-length(alldays)
#   par.state.sing.boots.full[[k]]<-data.frame(tmrca.dt.half=rep(alldays,times=nL),
# par.state = sort(rep(unique(par.state.sing.boots[[k]]$par.state),times=nD)),
#                                  total=0)
#   # nrow(par.state.sing.boots[[k]].empty)==nD*nL      
#   
#   #populate it
#   for (i in 1:nrow(par.state.sing.boots.full[[k]])){
#     #look for a match
#     match<-which(par.state.sing.boots[[k]]$par.state==par.state.sing.boots.full[[k]]$par.state[i] &
#             par.state.sing.boots[[k]]$tmrca.dt.half==par.state.sing.boots.full[[k]]$tmrca.dt.half[i])
#     if(length(match)==0) next #no match, no change
#     #else, replace:
#     par.state.sing.boots.full[[k]]$total[i]<-par.state.sing.boots[[k]]$total[match]
#   }
#   
#   # sum(par.state.sing.boots.full[[k]]$total[par.state.sing.boots.full[[k]]$par.state=="USA"])==sum(par.state.sing.boots[[k]]$total[par.state.sing.boots[[k]]$par.state=="USA"])
#   
#   par.state.sing.boots.full[[k]]<-par.state.sing.boots.full[[k]] %>% 
#     dplyr::mutate(intros_mean7d = zoo::rollmean(total, k = 7, fill = NA),
#                   intros_mean14d = zoo::rollmean(total, k = 14, fill = NA),
#                   intros_median7d = zoo::rollmedian(total, k = 7, fill = NA),
#                   intros_sum7d = zoo::rollsum(total, k = 7, fill = NA,align="right")) %>% #right align to sum all prev
#     #rolling mean and median weekly importation rate
#     dplyr::mutate(intros_meansum7d = zoo::rollmean(intros_sum7d, k = 7, fill = NA), 
#                   intros_mediansum7d = zoo::rollmedian(intros_sum7d, k = 7, fill = NA),) %>%
#     dplyr::ungroup()
# }
# 

```

## summarize proportion singletons
```{r}
#singles by location
single.focal.source<-c()
for (k in 1:n.B){
  x<-can.sing.boots[[k]] %>%
    filter(par.state%in%focal.source) %>%
    nrow()
  single.focal.source<-c(single.focal.source,x)}

cat(paste0('Mean (95% CI) # of ', focal.source.brf, '-origin variant singletons = ',
       mean.95ci.X(single.focal.source,0) ), 
    file=text.out,sep="\n",append=T)

cat(paste0('Percent of ', focal.source.brf, '-origin variant singletons = ',
       round((mean(single.focal.source)/mean(single.tot)*100),digits=1)  ), 
    file=text.out,sep="\n",append=T)

## IF there was a variant-specific intervention (specified above)...
if(int.yn==T){
  #how many singletons introduced before intervention
  single.preban<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.start)))
    single.preban<-c(single.preban,x)}
  cat(paste0('Mean (95%CI) # of singles pre-intervention = ',
       mean.95ci.X(single.preban,0) ), 
    file=text.out,sep="\n",append=T)
  
  #how many singletons introduced before intervention from focal
  single.preban1<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.start) &
                      can.sing.boots[[k]]$par.state %in%focal.source))
    single.preban1<-c(single.preban1,x) }
   cat(paste0('Mean (95%CI) # of singles pre-intervention from focal = ',
         mean.95ci.X(single.preban1,0)  ), 
    file=text.out,sep="\n",append=T) 
   
  ## What proportion of singles from focal before the ban
  single.preban.prop<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.start) &
                      can.sing.boots[[k]]$par.state %in%focal.source))
    y<-length(which(can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.start)))
    prop<-x/y
    single.preban.prop<-c(single.preban.prop, prop) }
  cat(paste0('Proportion of singles pre-intervention from focal = ',
         mean.95ci.X(single.preban.prop,2)  ), 
  file=text.out,sep="\n",append=T) 
   
  #DURING BAN
  single.duringban<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.start) &
                    can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.end)  ))
    single.duringban<-c(single.duringban,x)}
   cat(paste0('Mean (95%CI) # of singles during intervention = ',
        mean.95ci.X(single.duringban,0) ), 
    file=text.out,sep="\n",append=T) 
  
  #DURING BAN from focal.source
  single.duringban1<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.start) &
                    can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.end) &
                      can.sing.boots[[k]]$par.state %in%focal.source))
    single.duringban1<-c(single.duringban1,x) }
   cat(paste0('Mean (95%CI) # of singles during intervention from focal = ',
         mean.95ci.X(single.duringban1,0)  ), 
    file=text.out,sep="\n",append=T) 
   
   #proportion of single during ban from focal
    single.durban.prop<-c()
    for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.start) &
                    can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.end) &
                      can.sing.boots[[k]]$par.state %in%focal.source))
    y<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.start) &
                    can.sing.boots[[k]]$tmrca.dt.half<as.Date(int.end) ))
    prop<-x/y
    single.durban.prop<-c(single.durban.prop, prop) }
   cat(paste0('Proportion of singles during-intervention from focal = ',
         mean.95ci.X(single.durban.prop,2)  ), 
    file=text.out,sep="\n",append=T) 
   
   
  #AFTER BAN
  single.afterban<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.end)  ))
    single.afterban<-c(single.afterban,x)}
    cat(paste0('Mean (95%CI) # of singles after intervention = ',
           mean.95ci.X(single.afterban,0)  ), 
    file=text.out,sep="\n",append=T) 
    
  #AFTER BAN from focal.source
  single.afterban1<-c()
  for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.end)  &
                      can.sing.boots[[k]]$par.state %in%focal.source))
    single.afterban1<-c(single.afterban1,x)}
  cat(paste0('Mean (95%CI) # of singles after intervention = ',
          mean.95ci.X(single.afterban1,0) ), 
    file=text.out,sep="\n",append=T) 
  
  #proportion of single after ban form focal
  single.afterban.prop<-c()
    for (k in 1:n.B){
    x<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.end) &
                      can.sing.boots[[k]]$par.state %in%focal.source))
    y<-length(which(can.sing.boots[[k]]$tmrca.dt.half>=as.Date(int.end) ))
    prop<-x/y
    single.afterban.prop<-c(single.afterban.prop, prop) }
   cat(paste0('Proportion of singles post-intervention from focal = ',
         mean.95ci.X(single.afterban.prop,2)  ), 
    file=text.out,sep="\n",append=T) 
   
  ###Fold change in proportion focal pre vs during
  single.fc.prop<-c()
  for (k in 1:n.B){
    x<-single.preban.prop[k]
    y<-single.durban.prop[k]
    fc<-x/y #if pre=0.5 and during=0.1, 0.5/0.1 = 5-fold reduction 
    single.fc.prop<-c(single.fc.prop, fc) }
  cat(paste0('Fold reduction in proportion of singles pre vs during-intervention from focal = ',
       mean.95ci.X(single.fc.prop,2)  ), 
  file=text.out,sep="\n",append=T) 

  
} #end of if loop for int.yn=T


```

## summarize Maximum singleton rolling rates
```{r}
#overall sum of new singletons/week in Canada max
sing.Roll.total<-sing.Prov.Roll.summary %>% dplyr::group_by(tmrca.dt.half) %>%
  dplyr::summarize(.groups="rowwise",
                   total_meansum7d.mean=(sum(intros_meansum7d.mean,na.rm=T)),
                   total_meansum7d.sd=(sum(intros_meansum7d.sd,na.rm=T))) %>%
  as.data.frame()
max.overall<-head(na.omit(sing.Roll.total[rev(order(sing.Roll.total$total_meansum7d.mean)),]),n=1)
dt.temp<-max.overall[1,1]
cat(paste0("Max weekly singles importation rate overall on ", dt.temp,": ",
           mean.95ci.givenmsd(m=max.overall[2],sd=max.overall[3],n=n.B,2)),
    file=text.out,sep="\n",append=T)

#Max from focal.source
max.mean.focal<-max(sing.Par.Roll.summary$intros_meansum7d.mean[sing.Par.Roll.summary$par.state%in%focal.source],na.rm=T) 
max.sd.focal<-sing.Par.Roll.summary$intros_meansum7d.sd[which(sing.Par.Roll.summary$intros_meansum7d.mean ==max(sing.Par.Roll.summary$intros_meansum7d.mean[sing.Par.Roll.summary$par.state%in%focal.source],na.rm=T) )]
dt.focal<-sing.Par.Roll.summary$tmrca.dt.half[which(sing.Par.Roll.summary$intros_meansum7d.mean==max(sing.Par.Roll.summary$intros_meansum7d.mean[sing.Par.Roll.summary$par.state%in%focal.source],na.rm=T)) ] 

cat(paste0("Max weekly singles importation rate from ",focal.source, " on ",dt.focal,
           ": ", mean.95ci.givenmsd(m=max.mean.focal,sd=max.sd.focal, n.B, 2) ),
    file=text.out,sep="\n",append=T)

## Max from USA 
max.mean.usa<-max(sing.Par.Roll.summary$intros_meansum7d.mean[sing.Par.Roll.summary$par.state=="USA"],na.rm=T) 
max.sd.usa<-sing.Par.Roll.summary$intros_meansum7d.sd[which(sing.Par.Roll.summary$intros_meansum7d.mean ==max.mean.usa )]
dt.usa<-sing.Par.Roll.summary$tmrca.dt.half[which(sing.Par.Roll.summary$intros_meansum7d.mean==max.mean.usa) ] 
cat(paste0("Max weekly singles importation rate from ","USA", " on ",dt.usa,
           ": ", mean.95ci.givenmsd(m=max.mean.usa,sd=max.sd.usa, n.B, 2) ),
    file=text.out,sep="\n",append=T)

## IF there was a var-specifc intervention, calculate singles import on date implemented vs two weeks after from focal source
if(int.yn==T){
   rate.pre<-sing.Par.Roll.summary %>% filter(par.state%in%focal.source) %>% filter(tmrca.dt.half==int.start)
   
   rate.2wk<-sing.Par.Roll.summary %>% filter(par.state%in%focal.source) %>% filter(tmrca.dt.half==(int.start+14))
   
    rate.4wk<-sing.Par.Roll.summary %>% filter(par.state%in%focal.source) %>% filter(tmrca.dt.half==(int.start+28))
   
   cat(paste0("Weekly singles importation rate pre-int from ",focal.source, 
              " on ",int.start,": ", 
              mean.95ci.givenmsd(m=rate.pre$intros_meansum7d.mean,
                                    sd=rate.pre$intros_meansum7d.sd, n.B, 2) ),
    file=text.out,sep="\n",append=T)
   
   cat(paste0("Weekly singles importation rate 2-weeks post-int from ",focal.source, 
              " on ",int.start+14, ": ", 
              mean.95ci.givenmsd(m=rate.2wk$intros_meansum7d.mean,
                                    sd=rate.2wk$intros_meansum7d.sd, n.B, 2) ),
    file=text.out,sep="\n",append=T)
   
   
   ## FOLD DECREASE from pre to 2wk
   fc2wk.focal<-c()
   fc4wk.focal<-c()

   for (k in 1:n.B){
     temp<-par.state.sing.boots.full[[k]] %>% 
      dplyr::filter(par.state%in%focal.source) %>%
      as.data.frame()
     temp.pre<-temp %>% filter(tmrca.dt.half==int.start)
     temp.2wk<-temp %>% filter(tmrca.dt.half==(int.start+14))
     temp.4wk<-temp %>% filter(tmrca.dt.half==(int.start+28))

     fc2wk<-temp.pre$intros_meansum7d/temp.2wk$intros_meansum7d #fold change 2wk/pre (ex: 10/1)
     fc2wk.focal<-c(fc2wk.focal, fc2wk)
     
     fc4wk<-temp.pre$intros_meansum7d/temp.4wk$intros_meansum7d #fold change 2wk
     fc4wk.focal<-c(fc4wk.focal, fc4wk)
   }
   fc2wk.focal<-fc2wk.focal[is.finite(fc2wk.focal)]
   fc4wk.focal<-fc4wk.focal[is.finite(fc4wk.focal)]
   
   mean.fc.2wk<-mean.95ci.X(fc2wk.focal,2)
   mean.fc.4wk<-mean.95ci.X(fc4wk.focal,2)

   cat(paste0("Fold decrease singles rate pre-int to 2-weeks after int from ",focal.source, 
              ": ", mean.fc.2wk),    file=text.out,sep="\n",append=T)
   
   cat(paste0("Fold decrease singles rate pre-int to 4-weeks after int from ",focal.source, 
              ": ", mean.fc.4wk),    file=text.out,sep="\n",append=T)
   
}

```


## summary of total intros over time, all sources
```{r}
totalsublin<-sum.Par.Roll.summary %>% 
  dplyr::group_by(tmrca.dt) %>%
  dplyr::summarize(total.intros=sum(intros_meansum7d.mean, na.rm=T)) %>% as.data.frame()
totalsublin$type<-"sublineage"
head(totalsublin)

totalsingle<-sing.Par.Roll.summary %>% 
  dplyr::group_by(tmrca.dt.half) %>%
  dplyr::summarize(total.intros=sum(intros_meansum7d.mean, na.rm=T)) %>% as.data.frame()
totalsingle$type<-"singleton"
colnames(totalsingle)[1]<-"tmrca.dt"

totalintros<-bind_rows(totalsingle, totalsublin)

totalintros %>%
  ggplot()+
  geom_bar(aes(x=tmrca.dt,y=total.intros,color=type),stat="identity",width=1)+
  pubThemeDate+
  scaleDateFlex+
  labs(x=NULL, y="Total daily introductions")+
  theme(legend.position = "top")
ggsave(paste0(f.out,"AllIntros_OVerTime_byType.png"),width=5,height=5)

totalintros %>%
  ggplot()+
  geom_bar(aes(x=tmrca.dt,y=total.intros),stat="identity",width=1,color=var.col)+
  pubThemeDate+
  scaleDateFlex+
  labs(x=NULL, y="Total daily introductions")+
  theme(legend.position = "top")
ggsave(paste0(f.out,"AllIntros_OverTime_AllTypes.png"),width=4,height=4)

#export daily intros all sources
write.csv(totalintros,paste0(f.out, "alldailyintros.csv"))
```


## SUMMARY OF TOP SOURCES FOR SINGLETONS AND SUBLINEAGES
```{r}
#what percent of singletons came from the top sources
top.sources<-names(rev(sort(table(can.sing.boots[[1]]$par.state))))
for (i in 1:length(top.sources)){
  source<-c()
  for (k in 1:n.B){
    perc<- signif(length(which(can.sing.boots[[k]]$par.state==top.sources[i]))/
      nrow(can.sing.boots[[k]])*100, digits = 3)
    source<-c(source, perc)
  }
  # print(top.sources[i])
  # mean.95ci.X(source)
  cat(paste0("Percent singletons from",top.sources[i],"= ",
           mean.95ci.X(source) ),
    file=text.out,sep="\n",append=T)
}


#what percent of sublineages came from the top sources
top.sources<-names(rev(sort(table(sum.boots[[1]]$Parent.Location))))
for (i in 1:length(top.sources)){
  source<-c()
  for (k in 1:n.B){
    perc<- signif(length(which(sum.boots[[k]]$Parent.Location==top.sources[i]))/
      nrow(sum.boots[[k]])*100, digits = 3)
    source<-c(source, perc)
  }
  print(top.sources[i])
  mean.95ci.X(source)
  cat(paste0("Percent sublineages from ",top.sources[i],"= ",
           mean.95ci.X(source) ),
    file=text.out,sep="\n",append=T)
}
```


## Likelihood quadrants
```{r}
likes<-replicate(n.B,vector())
for (k in 1:n.B){
  likes[[k]]<-sum.boots[[k]][,c('Node.Likelihood','Parent.Likelihood')]
}
sum.likes<-bind_rows(likes)

#summary stats
summary(sum.likes$Node.Likelihood)
summary(sum.likes$Parent.Likelihood)

Nlik<-nrow(sum.likes)

#text for plot
p.5.n.5<-round(nrow(sum.likes[sum.likes$Parent.Likelihood<0.5 & sum.likes$Node.Likelihood<0.5,]) / Nlik * 100 ,digits=1)
p.5.n.9<-round(nrow(sum.likes[sum.likes$Parent.Likelihood<0.5 & sum.likes$Node.Likelihood<0.9 & sum.likes$Node.Likelihood>=0.5,]) / Nlik * 100,digits=1)
p.9.n.5<-round(nrow(sum.likes[sum.likes$Parent.Likelihood<0.9 & sum.likes$Parent.Likelihood>=0.5 & sum.likes$Node.Likelihood<0.5,]) / Nlik * 100,digits=1)
p.9.n.9<-round(nrow(sum.likes[sum.likes$Parent.Likelihood<0.9 & sum.likes$Parent.Likelihood>=0.5 & sum.likes$Node.Likelihood>=0.5 & sum.likes$Node.Likelihood<0.9,]) / Nlik * 100,digits=1)

p.5<-round(nrow(sum.likes[sum.likes$Parent.Likelihood<0.5 & sum.likes$Node.Likelihood>=0.9,]) / Nlik * 100,digits=1)
n.5<-round(nrow(sum.likes[sum.likes$Parent.Likelihood>=0.9 & sum.likes$Node.Likelihood<0.5,]) / Nlik * 100,digits=1)
p.9<-round(nrow(sum.likes[sum.likes$Parent.Likelihood<0.9 & sum.likes$Parent.Likelihood>=0.5 & sum.likes$Node.Likelihood>=0.9,]) / Nlik * 100,digits=1)
n.9<-round(nrow(sum.likes[sum.likes$Parent.Likelihood>=0.9 & sum.likes$Node.Likelihood>=0.5 & sum.likes$Node.Likelihood<0.9,]) / Nlik * 100,digits=1)

p.n<-round(nrow(sum.likes[sum.likes$Parent.Likelihood>=0.9 & sum.likes$Node.Likelihood>=0.9,]) / Nlik * 100,digits=1)


#Plot this with 0.5 and 0.9 cutoffs shown
ggplot(sum.likes)+
  geom_point(aes(x=Node.Likelihood, y=Parent.Likelihood),color="cyan4",alpha=0.5)+
  geom_vline(xintercept=c(0.5,0.9),linetype="dashed", color="red",size=0.5)+
  geom_hline(yintercept=c(0.5,0.9),linetype="dashed", color="blue",size=0.5)+
  pubTheme+
  annotate("text",x=0.5,y=0.5,label=paste(p.5.n.5,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=0.9,y=0.5,label=paste(p.5.n.9,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=0.5,y=0.9,label=paste(p.9.n.5,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=0.9,y=0.9,label=paste(p.9.n.9,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=1,y=0.5,label=paste(p.5,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=1,y=0.9,label=paste(p.9,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=0.5,y=1,label=paste(n.5,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=0.9,y=1,label=paste(n.9,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  annotate("text",x=1,y=1,label=paste(p.n,"%",sep=""),size=3.5,hjust=1,vjust=1, fontface="bold")+
  labs(x="Introduction node likelihood",y="Parent node likelihood")+
  theme(panel.background = element_rect(fill="grey95"))

ggsave(paste(f.out,"/sublin.Likelihoods.Nodes.Parents.png",sep=""),width=5,height=5)

#### note: should we apply a cutoff here? #####
## Also incorporate bootstrap tree support here
## try ultrafast bootstrap in IQtree for this or go RAxML or ?
```



# Characterize biggest sublineages

## re-generate desc.df.list using sublin.long
```{r}
desc.df.try<-split(sublin.long.unq[[1]],f = sublin.long.unq[[1]]$Sublineage)
```

## Sublineage plots of cases over time, Re
## stacked plots of sublin descenedants, Canada
```{r}
# function that generates stacked barplot by location
plot.stacked.bars<-function(df){
  df$State<-str_replace_all(df$State,"Canada_","")
  df<-df[which(df$State %in% provs),]
  if(nrow(df)>50){ #only for sublineages > 50 in size because so many
    name<-df$Sublineage[1]
    N<-paste(": ",nrow(df)," sampled Canadian desc.",sep="")
    ggplot(df,aes(x=Date,fill=State)) + 
      geom_bar(position="stack",width=2)+
      ProvFillScale+
     scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", 
                 date_labels = "%b %Y")+
      labs(x=NULL, y="Daily case count", color="Province", 
           title=paste("Sublineage ",name,N,sep=""))+
      theme(legend.position="bottom",title=element_text(size=7))+
      pubThemeDate+
      guides(col=guide_legend(nrow=1,title.position = "left",title="Province"))
      # annotate("text",label=N, y=maxlen,x=first(sort(df$Date)),size=rel(3),hjust=0) 
    ggsave(paste(f.sublin.out,"/",name,"_StackedBars",".png",sep=""),
           width=4,height=4)
  }
}

lapply(desc.df.try, plot.stacked.bars)

```

## stacked plots of sublin descenedants, all geos
```{r}
# df<-desc.df.try[[which(names(desc.df.try)==biggest)]]
# function that generates stacked barplot by location
plot.stacked.bars.allgeo<-function(df){
  df$State<-str_replace_all(df$State,"Canada_","")
  if(nrow(df[which(df$State %in% provs),])>50){ #only for sublineages > 50 in size because so many
    name<-df$Sublineage[1]
    N<-paste(": ",nrow(df)," sampled descendants",sep="")
    
    #make a custom geo order based on frequency
    mygeos<-names(sort(table(df$State),decreasing = T))
    df$State<-factor(df$State,levels=mygeos)
    #make a custom scale fill to reflect this custom geo vector
    geoFillScale<-scale_fill_manual(name = "Location",
                                    values = globalPalette.ch[match(mygeos,names(globalPalette.ch))]
                                    ,na.value="grey60")  
      
    ggplot(df,aes(x=Date,fill=State)) + 
      geom_bar(position="stack",width=2)+
      geoFillScale+
     scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", 
                 date_labels = "%b %Y")+
      labs(x=NULL, y="Daily case count", color="Province", 
           title=paste(name,N,sep=""))+
      theme(legend.position="bottom",title=element_text(size=7),
            legend.margin=margin(0,0,0,0,"pt"),
            plot.margin=margin(2,2,2,2,"pt"))+
      pubThemeDate+
      guides(fill=guide_legend(ncol=4,title.position = "top",title="Location"))
      # annotate("text",label=N, y=maxlen,x=first(sort(df$Date)),size=rel(3),hjust=0) 
    ggsave(paste(f.sublin.out,"/",name,"_StackedBarsAllGeo",".png",sep=""),
           width=4,height=4)
  }
}

lapply(desc.df.try, plot.stacked.bars.allgeo)

```

## chracterize the biggest sublineages' Re
```{r}
require(splines)

#sublin incidence, growth rates, epiestim Re estim
for (i in 1:length(desc.df.try)){
  df<-desc.df.try[[i]]
  df$State<-str_replace_all(df$State,"Canada_","")
  # df<-df[which(df$State %in% provs),]
  if(nrow(df)>50){ #only for sublineages > 50 in size because so many
    df2<-df %>% dplyr::group_by(Date) %>% dplyr::summarise(Incidence=n())
    dateran<-range(df2$Date)
    df3<-data.frame(Date=seq(as.Date(dateran[1]),as.Date(dateran[2]),'days'))
    df3<-left_join(df3,df2,by="Date")
    df3$Incidence[which(is.na(df3$Incidence))]<-0
    #MAke a smoothed incidence
    df3<-df3 %>% mutate(rollIncidence=zoo::rollmean(Incidence,k = 7, align="center",fill = NA))
    df3$rollIncidence[which(is.na(df3$rollIncidence))]<-0
    #double smoothed
    df3<-df3 %>% mutate(rollerIncidence=zoo::rollmean(rollIncidence,k = 7, align="center",fill = NA))
    df3$rollerIncidence[which(is.na(df3$rollerIncidence))]<-0
    
    #TRY a smoothing spline
    date.grid<-seq(from=as.Date(dateran[1]), to =as.Date(dateran[2]),'days')
    #fitting smoothing splines using smooth.spline(X,Y,df=...)
    free<-trunc(as.numeric(dateran[2]-dateran[1])/7) #one df for every 7 days
    fit1<-smooth.spline(df3$Date,df3$rollIncidence,df=free) 
    df3$splineIncidence<-fit1$y
    df3$splineIncidence[which(df3$splineIncidence<0)]<-0
    #plot incidence
    ggplot(df3,aes(x=Date,y=rollIncidence))+
             geom_bar(stat="identity")+
             geom_line(aes(y=splineIncidence),color="red")+
      pubThemeDate+
      labs(y="Rolling sublineage incidence",x=NULL)+
      scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", date_labels = "%b %Y")
    ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_SplineIncidence",".png",sep=""),width=4,height=4)

    FIRST<-df3$Date[first(which(df3$splineIncidence>2))]
    if(is.na(FIRST)){next}
    START<-as.numeric(FIRST-dateran[1])
    # incidence::df$  
    t_start <- seq(3, nrow(df3)-14) # starting at 2 as conditional on the past observations
    t_end <- t_start + 14 # adding 13 to get 14-day windows as bounds included in window

    epi.out<-EpiEstim::estimate_R(df3$splineIncidence, 
                                method="parametric_si",
                                config = make_config(list(
                                  t_start=t_start,
                                  t_end=t_end,
                                  mean_si = 4.8, 
                                  std_si = 2.3)))
    ### SERIAL INTERVAL MEAN 4.8 days, 2.3 sd
    
    # plot(epi.out,"R")
    # ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_epiestim",".png",sep=""),width=4,height=4)
    len.re<-nrow(epi.out$R)
    x<-0
    datez<-seq(as.Date(dateran[1])+(3),as.Date(dateran[2])-x,'days')
    x<- -(len.re-length(datez))
    datez<-seq(as.Date(dateran[1])+(3),as.Date(dateran[2])-x,'days')
    len.re==length(datez)
    epi.df<-data.frame(date=datez,
                       Re.mean=epi.out$R$`Mean(R)`, 
                       Re.med=epi.out$R$`Median(R)`,
                       Re.low=epi.out$R$`Quantile.0.05(R)`,
                       Re.high=epi.out$R$`Quantile.0.95(R)`)
    #remove first Re estimates
     # epi.df<-epi.df[-c(1,2,3,4,5,6),]
    #plot it up
    ggplot(data=epi.df,aes(x=date,y=Re.mean))+
      geom_hline(yintercept =1,lwd=0.5,lty=2)+
      geom_line(color="black")+
      geom_ribbon(aes(ymin=Re.low,ymax=Re.high),fill="grey35",alpha=0.6)+
      pubThemeDate+
      scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", 
                 date_labels = "%b %Y")+      
      labs(x=NULL,y="Effective reproduction number",title=df$Sublineage[1])
      # annotate("text",label=N, y=maxlen,x=first(sort(df$Date)),size=rel(3),hjust=0) 
    ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_Re_SI4.8",".png",sep=""),width=4,height=3)
  }
}
```

## Repeat with a lower serial interval
```{r}
#sublin incidence, growth rates, epiestim Re estim
for (i in 1:length(desc.df.try)){
  df<-desc.df.try[[i]]
  df$State<-str_replace_all(df$State,"Canada_","")
  # df<-df[which(df$State %in% provs),]
  if(nrow(df)>50){ #only for sublineages > 50 in size because so many
    df2<-df %>% dplyr::group_by(Date) %>% dplyr::summarise(Incidence=n())
    dateran<-range(df2$Date)
    df3<-data.frame(Date=seq(as.Date(dateran[1]),as.Date(dateran[2]),'days'))
    df3<-left_join(df3,df2,by="Date")
    df3$Incidence[which(is.na(df3$Incidence))]<-0
    #MAke a smoothed incidence
    df3<-df3 %>% mutate(rollIncidence=zoo::rollmean(Incidence,k = 7, align="center",fill = NA))
    df3$rollIncidence[which(is.na(df3$rollIncidence))]<-0
    #double smoothed
    df3<-df3 %>% mutate(rollerIncidence=zoo::rollmean(rollIncidence,k = 7, align="center",fill = NA))
    df3$rollerIncidence[which(is.na(df3$rollerIncidence))]<-0
    
    #TRY a smoothing spline
    date.grid<-seq(from=as.Date(dateran[1]), to =as.Date(dateran[2]),'days')
    #fitting smoothing splines using smooth.spline(X,Y,df=...)
    free<-trunc(as.numeric(dateran[2]-dateran[1])/7) #one df for every 7 days
    fit1<-smooth.spline(df3$Date,df3$rollIncidence,df=free) 
    df3$splineIncidence<-fit1$y
    df3$splineIncidence[which(df3$splineIncidence<0)]<-0
    #plot incidence
    ggplot(df3,aes(x=Date,y=rollIncidence))+
             geom_bar(stat="identity")+
             geom_line(aes(y=splineIncidence),color="red")+
      pubThemeDate+
      labs(y="Rolling sublineage incidence",x=NULL)+
      scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", date_labels = "%b %Y")
    ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_SplineIncidence",".png",sep=""),width=4,height=4)

    FIRST<-df3$Date[first(which(df3$splineIncidence>2))]
    if(is.na(FIRST)){next}
    START<-as.numeric(FIRST-dateran[1])
    # incidence::df$  
    t_start <- seq(3, nrow(df3)-14) # starting at 2 as conditional on the past observations
    t_end <- t_start + 14 # adding 13 to get 14-day windows as bounds included in window

    epi.out<-EpiEstim::estimate_R(df3$splineIncidence, 
                                method="parametric_si",
                                config = make_config(list(
                                  t_start=t_start,
                                  t_end=t_end,
                                  mean_si = 3, 
                                  std_si = 2)))
    # plot(epi.out,"R")
    # ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_epiestim",".png",sep=""),width=4,height=4)
    len.re<-nrow(epi.out$R)
    x<-0
    datez<-seq(as.Date(dateran[1])+(3),as.Date(dateran[2])-x,'days')
    x<- -(len.re-length(datez))
    datez<-seq(as.Date(dateran[1])+(3),as.Date(dateran[2])-x,'days')
    len.re==length(datez)
    epi.df<-data.frame(date=datez,
                       Re.mean=epi.out$R$`Mean(R)`, 
                       Re.med=epi.out$R$`Median(R)`,
                       Re.low=epi.out$R$`Quantile.0.05(R)`,
                       Re.high=epi.out$R$`Quantile.0.95(R)`)
    #remove first Re estimates
     # epi.df<-epi.df[-c(1,2,3,4,5,6),]
    #plot it up
    ggplot(data=epi.df,aes(x=date,y=Re.mean))+
      geom_hline(yintercept =1,lwd=0.5,lty=2)+
      geom_line(color="black")+
      geom_ribbon(aes(ymin=Re.low,ymax=Re.high),fill="grey35",alpha=0.6)+
      pubThemeDate+
      scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", 
                 date_labels = "%b %Y")+      
      labs(x=NULL,y="Effective reproduction number",title=df$Sublineage[1])
      # annotate("text",label=N, y=maxlen,x=first(sort(df$Date)),size=rel(3),hjust=0) 
    ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_Re_SI3",".png",sep=""),width=4,height=4)
  }
}
```


## Repeat with a low serial interval, but smaller window of estimation
```{r}
#sublin incidence, growth rates, epiestim Re estim
for (i in 1:length(desc.df.try)){
  df<-desc.df.try[[i]]
  df$State<-str_replace_all(df$State,"Canada_","")
  # df<-df[which(df$State %in% provs),]
  if(nrow(df)>50){ #only for sublineages > 50 in size because so many
    df2<-df %>% dplyr::group_by(Date) %>% dplyr::summarise(Incidence=n())
    dateran<-range(df2$Date)
    df3<-data.frame(Date=seq(as.Date(dateran[1]),as.Date(dateran[2]),'days'))
    df3<-left_join(df3,df2,by="Date")
    df3$Incidence[which(is.na(df3$Incidence))]<-0
    #MAke a smoothed incidence
    df3<-df3 %>% mutate(rollIncidence=zoo::rollmean(Incidence,k = 7, align="center",fill = NA))
    df3$rollIncidence[which(is.na(df3$rollIncidence))]<-0
    #double smoothed
    df3<-df3 %>% mutate(rollerIncidence=zoo::rollmean(rollIncidence,k = 7, align="center",fill = NA))
    df3$rollerIncidence[which(is.na(df3$rollerIncidence))]<-0
    
    #TRY a smoothing spline
    date.grid<-seq(from=as.Date(dateran[1]), to =as.Date(dateran[2]),'days')
    #fitting smoothing splines using smooth.spline(X,Y,df=...)
    free<-trunc(as.numeric(dateran[2]-dateran[1])/7) #one df for every 7 days
    fit1<-smooth.spline(df3$Date,df3$rollIncidence,df=free) 
    df3$splineIncidence<-fit1$y
    df3$splineIncidence[which(df3$splineIncidence<0)]<-0
    #plot incidence
    # ggplot(df3,aes(x=Date,y=rollIncidence))+
    #          geom_bar(stat="identity")+
    #          geom_line(aes(y=splineIncidence),color="red")+
    #   pubThemeDate+
    #   labs(y="Rolling sublineage incidence",x=NULL)+
    #   scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", date_labels = "%b %Y")
    # ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_SplineIncidence",".png",sep=""),width=4,height=4)

    FIRST<-df3$Date[first(which(df3$splineIncidence>2))]
    if(is.na(FIRST)){next}
    START<-as.numeric(FIRST-dateran[1])
    # incidence::df$  
    t_start <- seq(3, nrow(df3)-7) # starting at 2 as conditional on the past observations
    t_end <- t_start + 7 # adding 13 to get 14-day windows as bounds included in window

    epi.out<-EpiEstim::estimate_R(df3$splineIncidence, 
                                method="parametric_si",
                                config = make_config(list(
                                  t_start=t_start,
                                  t_end=t_end,
                                  mean_si = 3, 
                                  std_si = 2)))
    # plot(epi.out,"R")
    # ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_epiestim",".png",sep=""),width=4,height=4)
    len.re<-nrow(epi.out$R)
    x<-0
    datez<-seq(as.Date(dateran[1])+(3),as.Date(dateran[2])-x,'days')
    x<- -(len.re-length(datez))
    datez<-seq(as.Date(dateran[1])+(3),as.Date(dateran[2])-x,'days')
    len.re==length(datez)
    epi.df<-data.frame(date=datez,
                       Re.mean=epi.out$R$`Mean(R)`, 
                       Re.med=epi.out$R$`Median(R)`,
                       Re.low=epi.out$R$`Quantile.0.05(R)`,
                       Re.high=epi.out$R$`Quantile.0.95(R)`)
    #remove Re estimates from dates with sparse data     
    # epi.df<-epi.df[-c(1:),]
    #plot it up
    ggplot(data=epi.df,aes(x=date,y=Re.mean))+
      geom_hline(yintercept =1,lwd=0.5,lty=2)+
      geom_line(color="black")+
      geom_ribbon(aes(ymin=Re.low,ymax=Re.high),fill="grey35",alpha=0.6)+
      pubThemeDate+
      scale_x_date(date_breaks = "1 month", date_minor_breaks = "2 weeks", 
                 date_labels = "%b %Y")+      
      labs(x=NULL,y="Effective reproduction number",title=df$Sublineage[1])
      # annotate("text",label=N, y=maxlen,x=first(sort(df$Date)),size=rel(3),hjust=0) 
    ggsave(paste(f.sublin.out,"/",df$Sublineage[1],"_Re_SI3-window7",".png",sep=""),width=4,height=4)
  }
}
```


# Sublineages and cases averted

## Size distributions for different periods / filters
```{r}
## If haven't run all the above, read in here
# sum.boots<-replicate(vector,n=n.B)
# sum.boots[[1]]<-read.csv(sum.out[1])

## before restrictions
k=1 
#Bin sublineage size by frequency
sum.boots.binSize.pre<-sum.boots[[k]] %>% 
  filter(tmrca.dt<=int.end) %>%
  # filter(Parent.Location%in%focal.source) %>%
  group_by(N.Desc.glob,Lineage) %>% 
  dplyr::summarize(.groups="rowwise",n=n())

#bin the larger sizes
# sum.boots.binSize.pre<-bin.sublinsize(sum.boots.binSize.pre)

#make a summary df of n in each category for text and pos
sum.boots.binSize.pre.summ<-sum.boots.binSize.pre%>% dplyr::group_by(Number.Desc.Factor) %>%
  dplyr::summarise(.groups="rowwise",total.subs=sum(n)) %>% as.data.frame()

# #PLOT the grouped sizes
# P3.pre<-ggplot(data=sum.boots.binSize.pre)+
#   geom_bar(aes(x=as.factor(Number.Desc.Factor),y=n, group=Lineage,fill=Lineage),
#                position="stack",stat="identity",alpha=0.9)+
#   pubTheme+
#   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
#         axis.text.x=element_text(angle=45,vjust=1,hjust=1,size=rel(1.2)),
#         # legend.position = c(0.7,0.8),
#         legend.position = "none",
#         legend.text = element_text(size=9),
#         # axis.title.y=element_text(vjust=4),       
#         axis.title.y=element_text(vjust=1,size=rel(1.2)), 
#         plot.margin=unit(c(4,4,4,4),"pt"))+
#   LinFillScale+
#   labs(x="Sublineage size",y="# sublineages")+
#   guides(fill=guide_legend(ncol= 1,title="Lineage",title.position="top",reverse = T, keywidth = 1.2,keyheight = 1.2))+
#   geom_text(data=sum.boots.binSize.summ,aes(y=total.subs,x=Number.Desc.Factor,label=paste(total.subs,sep=""),hjust=0.5,vjust=-0.3),size=2.7)+  
#   scale_y_continuous(limits=c(0,71),expand=c(0.02,0))
  # annotate("rect",xmin=2.5,xmax=16.4,ymin=166,ymax=202, fill="white")

# P3.pre
# ggsave(paste(f.out,"/Sublineagesizes.bylin.Frequencies.untilendrestric.png",sep=""),width=4,height=4,units="in")

#Repeat with a filter for focal source sublineages
sum.boots.binSize.focalpre<-sum.boots[[k]] %>% 
  filter(tmrca.dt<=int.end) %>%
  filter(Parent.Location%in%focal.source) %>%
  group_by(N.Desc.glob,Lineage) %>% 
  dplyr::summarize(.groups="rowwise",n=n())
#bin the larger sizes
# sum.boots.binSize.focalpre<-bin.sublinsize(sum.boots.binSize.focalpre)

## Smooth curves of sublineage size 
## generate smooth density curves for sublineage size before end of restrictions
P3.smooth<-ggplot()+
  geom_density(data=sum.boots.binSize.pre,aes(x=N.Desc.glob),color="black",adjust = 10)+
  geom_density(data=sum.boots.binSize.focalpre,aes(x=N.Desc.glob),color="red",adjust = 10)+
  pubTheme+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        axis.text.x=element_text(angle=45,vjust=1,hjust=1,size=rel(1.2)),
        # legend.position = c(0.7,0.8),
        legend.position = "none",
        legend.text = element_text(size=9),
        # axis.title.y=element_text(vjust=4),       
        axis.title.y=element_text(vjust=1,size=rel(1.2)), 
        plot.margin=unit(c(4,4,4,4),"pt"))+
  labs(x="Sublineage size",y="Density")+
  guides(fill=guide_legend(ncol= 1,title="Lineage",title.position="top",reverse = T, keywidth = 1.2,keyheight = 1.2))
  # scale_y_continuous(limits=c(0,71),expand=c(0.02,0))

P3.smooth
ggsave(paste(f.out,"/Sublineagesizes.Smooth.Frequencies.untilend.restric.png",sep=""),width=4,height=4,units="in")

```

## Estimate introductions averted during intervention using case data
```{r}
# sum.Par.Roll.summary<-read.csv("2023-02-09_alpha_analysis/sum.par.roll.summary.csv")
# sum.Par.Roll.summary$tmrca.dt<-as.Date(sum.Par.Roll.summary$tmrca.dt)

#sublin rate right before the intervetion:
sum.Par.Roll.focal.pre<-sum.Par.Roll.summary %>% 
  filter(Parent.Location%in%focal.source) %>%
  filter(tmrca.dt < int.start)

## read in this data
meta.glob.var.country.daily.full3<-read.csv(meta.glob.cases.in)

# head(meta.glob.var.country.daily.full3)
focal.var.cases<- meta.glob.var.country.daily.full3 %>% 
  filter(country%in%focal.source) %>%
  filter(var.WHO==Focal.var)
focal.var.cases$date<-as.Date(focal.var.cases$date)
## Plot focal cases with overlay for flight ban
y.lim2<-c(0,(ceiling(max(focal.var.cases$avgVOC_cases))+3100))
p.focal.var.cases<-ggplot(focal.var.cases)+
    annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim2[1], ymax =y.lim2[2]-50, 
             color="grey85",size=0,  fill="grey80",alpha=0.9)+
  annotate(geom="text",x =( as.Date(int.start)+int.duration/2),
           y=y.lim2[2]-500, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=4.5)+
  geom_bar(aes(x=date, y=avgVOC_cases,fill=country),stat="identity",width=1)+
  pubThemeDate+
  # scale_x_date(breaks=xdates, date_labels = "%b %Y",
  #                           limits=as.Date(c(start.date,"2021-06-01")),
  #                           expand=c(0,0))+
    scale_x_date(date_labels = "%b %Y",
                            limits=as.Date(c(start.date,"2021-03-01")),
                            expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+
  GlobFillScale+
  theme(legend.position = "none")+
  labs(x=NULL, y=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"))
  
p.focal.var.cases
ggsave(paste0(f.out,"p.focal.var.cases.shorter-x.png"),height=2.7,width=3,unit="in")

## join the focal cases to the uk importation rate pre intervention
sum.Par.Roll.focal.pre$date<-sum.Par.Roll.focal.pre$tmrca.dt
# class(focal.var.cases$tmrca.dt)
focal.cases.intros<-left_join(sum.Par.Roll.focal.pre,focal.var.cases, by="date")
# head(focal.cases.intros)

#Make a linear model
#use this to predict the number of sublineages averted based on cases
lm.cases.intros<-lm(data=focal.cases.intros, intros_meansum7d.mean ~ avgVOC_cases)
sum.lm.cases.intros<-summary(lm.cases.intros) #Adjusted R-squared:  0.9531 ,  p-value: < 2.2e-16
sum.lm.cases.intros
adjustedr2<-round(sum.lm.cases.intros$r.squared,digits = 3)
text.adjustedr2<-paste0("Multiple R-squared = ",adjustedr2)
#Relationship b/w cases in the UK And intros into Canada
p.focal.cases.intros<-ggplot(focal.cases.intros,
                             aes(x=avgVOC_cases,
                                 y=intros_meansum7d.mean,color=country))+
  geom_point()+
  GlobColScale+
  theme(legend.position = "none")+
  labs(x=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"), 
       y="Sublineages per week")+
  geom_smooth(method ="lm",se=T,level=0.95)+
  pubTheme+
  annotate(geom="label",
           x=1,y=max(focal.cases.intros$intros_meansum7d.mean,na.rm=T),
           label=text.adjustedr2,hjust=0)

p.focal.cases.intros

ggsave(paste0(f.out,"p.focal.var.cases.intros.lm.png"),height=3,width=3,unit="in")

#now predict number of sublins per week up until int.end
focal.cases.new<-focal.var.cases %>% filter(date<=int.end & date>=int.start) 
focal.cases.df<-focal.cases.new$avgVOC_cases %>% as.data.frame()
colnames(focal.cases.df)<-"avgVOC_cases"
new.vals<-predict.lm(lm.cases.intros,newdata = focal.cases.df,interval = "prediction")
#join new vals on with the new cases data
focal.cases.df2<-focal.cases.df %>% bind_cols(new.vals)
colnames(focal.cases.df2)[2:4]<-c("intros_meansum7d.mean","lower","upper")
focal.cases.df2$country<-focal.source

#plot this to see how many sublins there might have been during the ban
p.obs.fit<-ggplot(focal.cases.df2,
       aes(x=avgVOC_cases,y=intros_meansum7d.mean,group=country)) +
    geom_ribbon(aes(ymin = lower,ymax = upper,fill=country),alpha=0.3)+
  geom_point(aes(color=country))+
  GlobColScale+
  GlobFillScale+
  theme(legend.position = "none")+
  labs(x=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"), 
       y="Sublineages per week")

## Join this onto the other dataset, and distinguish them as 'no restriction' (predicted) and 'restrictions' (observed)

focal.cases.intros2<-focal.cases.intros[,c("avgVOC_cases","intros_meansum7d.mean","country")]
focal.cases.intros2$type<-"Observed"

## Also need to add confidence intervals to the observed data that was fitted
focal.cases.intros2.new<-predict.lm(lm.cases.intros,newdata = focal.cases.intros2,interval = "prediction")
#only want the upper and lower here
focal.cases.intros2.new<-focal.cases.intros2.new[,c(2,3)]
colnames(focal.cases.intros2.new)<-c("lower","upper")
focal.cases.intros3<-focal.cases.intros2 %>% bind_cols(focal.cases.intros2.new)

#join new vals on with the new cases data
focal.cases.df2<-focal.cases.df %>% bind_cols(new.vals)
colnames(focal.cases.df2)[2:4]<-c("intros_meansum7d.mean","lower","upper")
focal.cases.df2$country<-focal.source
focal.cases.df2$type<-"Predicted"      
focal.obs.pred<-bind_rows(focal.cases.intros3,focal.cases.df2)

## colors for this
pred.pal<-c("deepskyblue4","chartreuse3")
names(pred.pal)<-c("Observed","Predicted")

## Plot and color by type
p.focal.obs.pred<-ggplot(focal.obs.pred,
       aes(x=avgVOC_cases,y=intros_meansum7d.mean,group=type)) +
  geom_ribbon(aes(ymin = lower,ymax = upper,fill=type),alpha=0.5)+
  geom_point(aes(color=type),alpha=0.8)+
  labs(x=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"), y="Sublineages per week")+
  # geom_smooth(method ="lm",alpha=0.2,color=type)+
  pubTheme+
  theme(legend.position = c(1,0.2),
        legend.justification = c(1,0),
        axis.title.x=element_text(margin = margin(t = -30)),
        legend.text=element_text(size=rel(1.1)),
        legend.title=element_text(size=rel(1.2)))+
  scale_fill_manual(values=pred.pal)+
  scale_color_manual(values=pred.pal)+
  guides(color=guide_legend(title="Scenario"),
         fill=guide_legend(title="Scenario"))+ 
    annotate(geom="label",
           x=max(focal.obs.pred$avgVOC_cases,na.rm=T),
           y=1,#mean(focal.obs.pred$intros_meansum7d.mean,na.rm=T),
           label=text.adjustedr2,hjust=1,size=3.5)

p.focal.obs.pred
ggsave(paste0(f.out,"p.obs.pred.png"),height=3,width=3,unit="in")

## Join the predicted values onto the data over time

#get dates back in here
focal.cases.df3<-focal.cases.df2 %>% left_join(focal.var.cases[,c("date","avgVOC_cases")],by="avgVOC_cases")
focal.cases.df3$type<-"Predicted"
# head(focal.cases.df3)

focal.intros.all<-sum.Par.Roll.summary %>% 
  filter(Parent.Location%in%focal.source)
focal.intros.all$type<-"Observed"
focal.intros.all$date<-focal.intros.all$tmrca.dt

colnames(focal.cases.df3)
focal.intros.all.red<-focal.intros.all[,]

### NOW plot this on the sublins per date
pred.pal2<-pred.pal
names(pred.pal2)<-c("Observed","Predicted")
y.lim<-c(0,max(focal.cases.df3$intros_meansum7d.mean)+2)
p.focal.pred.sublin<-focal.intros.all %>%
  ggplot()+
  pubThemeDate+
  annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim[1], ymax =y.lim[2], color="grey85",size=0,  
           fill="grey80",alpha=0.9)+
  annotate(geom="text",x = (as.Date(int.start)+int.duration/2),
           y=y.lim[2]-0.2, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="black",size=4.5)+
  labs(x="Date of most recent common ancestor",y=paste0("Sublineages per week"))+
  scale_x_date(date_labels = "%b %Y",
                            limits=as.Date(c(start.date,"2021-03-01")),
                            expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+
  #observed
   geom_density(aes(x=date,y=intros_meansum7d.mean,color=type,fill=type),
               stat="identity",size=1.5,alpha=0.7)+
  #predicted
  geom_density(data=focal.cases.df3,aes(x=date,y=intros_meansum7d.mean,
                                        color=type,fill=type),
               stat="identity",size=1.5,alpha=0.7)+
  
  guides(color=guide_legend(title="Scenario"),
         fill=guide_legend(title="Scenario"))+
  theme(legend.position = c(1,0.4),
        legend.justification = c(1,0),
        legend.text=element_text(size=rel(1.1)),
        legend.title=element_text(size=rel(1.1)))+
  scale_color_manual(values=pred.pal2)+
  scale_fill_manual(values=pred.pal2)
p.focal.pred.sublin
ggsave(paste0(f.out,"p.focal.pred.sublin.png"),height=3,width=3,unit="in")

#no legend version
p.focal.pred.sublin.noleg<-p.focal.pred.sublin+
  theme(legend.position = "none")
p.focal.pred.sublin.noleg
ggsave(paste0(f.out,"p.focal.pred.sublin.no.leg.png"),height=3,width=3,unit="in")

```

## Calculate the difference in the area under curves
```{r}
#integrate stepwise daily 
observed.df<-focal.intros.all %>% filter(date<=int.end & date>=int.start) %>% dplyr::select (c("date","intros_meansum7d.mean"))
id<-order(observed.df$date)
AUC.observed<-sum(diff(observed.df$date[id])*
                  ((rollmean(observed.df$intros_meansum7d.mean[id],2))/7) ) %>% as.numeric()
#divided by seven to convert it to per day instead of per week
AUC.observed #observed # of sublineages

### Predicted:
predicted.df<-focal.cases.df3 %>% filter(date<=int.end & date>=int.start) %>% dplyr::select (c("date","intros_meansum7d.mean"))
id<-order(predicted.df$date)
AUC.predicted<-sum(diff(predicted.df$date[id])*
                  ((rollmean(predicted.df$intros_meansum7d.mean[id],2))/7) ) %>% as.numeric()
AUC.predicted #observed # of sublineages

## differences
averted.sublin<-AUC.predicted - AUC.observed
averted.sublin

#YAYY - need to repeat this for singletons
## ALso need to repeat it for a lower bound
```

## Calculate cases averted: sum of sublin sizes averted
```{r}
## fit a gamma model to the observed distribution of sublinaege sizes caluclate above (wayy up)
#https://www.geeksforgeeks.org/how-to-fit-a-gamma-distribution-to-a-dataset-in-r/?ref=rp

# Moment matching estimation consists in equalizing theoretical and empirical moments. Estimated values of the distribution parameters are computed by a closed-form formula
ans.pre <- fitdist(as.numeric(sum.boots.binSize.pre$N.Desc.glob), 
               distr = "gamma", method = "mme", #start=c(1,1000),
               discrete=T)
# plot(ans.pre)

ans.focalpre <- fitdist(as.numeric(sum.boots.binSize.focalpre$N.Desc.glob), 
               distr = "gamma", method = "mme", #start=c(1,1000),
               discrete=T)
# plot(ans.focalpre)

#Gamma fit parameters
# ans.focalpre$estimate[1] #shape
# ans.focalpre$estimate[2] #rate

#draw from a disitrbutino with this shape and rate
# set.seed(111) 
# N <- averted.sublin
# # Draw N gamma distributed values
# y_rgamma_pre <- rgamma(N, shape = ans.pre$estimate[1],rate=ans.pre$estimate[2]) 
# y_rgamma_ukpre<-ceiling(y_rgamma_pre) #round up to integers
# # Plot of randomly drawn gamma density
# hist(y_rgamma_pre, breaks = 50, main = "")
# sum(y_rgamma_ukpre) # total cases averted

## Setup a function to loop 1000 draws of N sublineages frm gamma distrib
draw.averted.cases<-function(N, gamma.df){
  cases.averted.all<-vector()
  for (i in 1:1000){
    set.seed(i) 
    # Draw N gamma distributed values
    y_rgamma <- rgamma(N, 
                     shape = gamma.df$estimate[1],
                     rate=gamma.df$estimate[2]) 
    y_rgamma<-ceiling(y_rgamma) #round up 
  
    # Plot of randomly drawn gamma density
    tots<-sum(y_rgamma) # total cases averted
    cases.averted.all<-c(cases.averted.all,tots)
  }  
  cases.averted.all.df<-cases.averted.all %>% as.data.frame()
  colnames(cases.averted.all.df)<-"cases.averted.all"
  return(cases.averted.all.df)
}

#run function to draw
cases.averted.all.df.pre<-draw.averted.cases(N=averted.sublin,
                                             gamma.df=ans.pre)

#plot it
confint<-mean.95ci.X(cases.averted.all.df.pre$cases.averted.all)
mean<-mean(cases.averted.all.df.pre$cases.averted.all)  
ymax<-0.1
p.averted1<-ggplot(cases.averted.all.df.pre) +
  geom_density(aes(x=cases.averted.all,y=..count..),
               color="chartreuse3",fill="chartreuse3",alpha=0.5)+
  labs(x="Cases averted",y="Frequency")+
  geom_vline(xintercept = mean, linetype=2)+
  annotate(geom = "text",x =mean+100,y=ymax,
           label=paste0("Mean cases averted (95% CI)\n= ",confint),
           hjust=-0.05,size=3)+
  pubTheme+
  theme(axis.title.x=element_text(margin = margin(t = -30)))
  # theme(axis.title.x=element_text(margin = margin(t = -30))) #run for grob only
p.averted1
ggsave(paste0(f.out,"cases.averted.allsourcesendrestric.png"),height=2.7,width=3,unit="in")

#make a version in the variant color
p.averted1.varcol<-ggplot(cases.averted.all.df.pre) +
  geom_density(aes(x=cases.averted.all,y=..count..),
               color=var.col,fill=var.col,alpha=0.5)+   
  # geom_density(aes(x=cases.averted.all,y=..count..),
  #              color="chartreuse3",fill="chartreuse3",alpha=0.5)+
  labs(x="Cases averted",y="Frequency")+
  geom_vline(xintercept = mean, linetype=2)+
  annotate(geom = "text",x =mean+100,y=ymax,
           label=paste0("Mean cases averted (95% CI)\n= ",confint),
           hjust=-0.05,size=3)+
  pubTheme
  # theme(axis.title.x=element_text(margin = margin(t = -30))) #run for grob only
p.averted1.varcol
ggsave(paste0(f.out,"cases.averted.allsourcesendrestric_varcol.png"),height=2.7,width=3,unit="in")

##repeat for uk only gamma distribution of sublin sizees
cases.averted.focal.df<-draw.averted.cases(N=averted.sublin,
                                           gamma.df = ans.focalpre)
#plot it
confint<-mean.95ci.X(cases.averted.focal.df$cases.averted.all)
mean<-mean(cases.averted.focal.df$cases.averted.all)  
ymax<-1.2
p.avert2<-ggplot(cases.averted.focal.df) +
  geom_density(aes(x=cases.averted.all,y=..count..),
               color="chartreuse3",fill="chartreuse3",alpha=0.5)+
  labs(x="Cases averted",y="Frequency")+
  geom_vline(xintercept = mean, 
             color="darkblue",linetype=2)+
  annotate(geom = "text",x =mean+100,y=ymax,
           label=paste0("Mean cases averted (95% CI)\n= ",confint),
           hjust=-0.05)+
  pubTheme
p.avert2
ggsave(paste0(f.out,"cases.averted.focalsourcesendrestric.png"),height=3,width=4,unit="in")

```

## Grob together the key plots from this analysis
```{r}
plot_grid(p.focal.var.cases, p.focal.obs.pred, 
          p.focal.pred.sublin, p.averted1, nrow=2,labels=LETTERS[1:4],
          align="hv")
ggsave(paste0(f.out,"composite_casesAverted_full.png"),height=6.2,width=6.2,units = "in")

```


## Add this into the data text export
```{r}
cat(paste0("Sublineages observed during restrictions: ",
           round(AUC.observed,digits = 1)),
    paste0("Sublineages predicted (from mean sublin rate) during restrictions: ",
           round(AUC.predicted,digits = 1)),
    paste0("Sublineages averted (from mean sublin rate) during restrictions: ", 
           round(averted.sublin,digits = 1)),
    paste0("Mean cases (sublin desc) averted, all source distrib, mean: ",
           mean.95ci.X(cases.averted.all,1)),
    paste0("Mean cases (sublin desc) averted, focal distrib, mean: ",
           mean.95ci.X(cases.averted.focal,1)),
    file=text.out, append=T,sep="\n")
```


# Singletons averted

## Estimate singletons averted during intervention using case data
```{r}
#singleton rate right before the intervention:
sing.Par.Roll.focal.pre<-sing.Par.Roll.summary %>% 
  filter(par.state%in%focal.source) %>%
  filter(tmrca.dt.half < int.start)

## join the focal cases to the uk importation rate pre intervention
sing.Par.Roll.focal.pre$date<-sing.Par.Roll.focal.pre$tmrca.dt.half
# class(focal.var.cases$tmrca.dt)
focal.cases.intros.sing<-left_join(sing.Par.Roll.focal.pre, focal.var.cases, by="date")
# head(focal.cases.intros.sing)

#Make a linear model
#use this to predict the number of singletons averted based on cases
lm.cases.intros.sing<-lm(data=focal.cases.intros.sing, intros_meansum7d.mean ~ avgVOC_cases)
sing.lm.cases.intros.sing<-summary(lm.cases.intros.sing) #Adjusted R-squared:  0.9531 ,  p-value: < 2.2e-16
sing.lm.cases.intros.sing
adjustedr2<-round(sing.lm.cases.intros.sing$r.squared,digits = 3)
text.adjustedr2<-paste0("Multiple R-squared = ",adjustedr2)
#Relationship b/w cases in the UK And intros into Canada
p.sing.focal.cases.intros.sing<-ggplot(focal.cases.intros.sing,
                             aes(x=avgVOC_cases,
                                 y=intros_meansum7d.mean,color=country))+
  geom_point()+
  GlobColScale+
  theme(legend.position = "none")+
  labs(x=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"), 
       y="Singletons per week")+
  geom_smooth(method ="lm",se=T,level=0.95)+
  pubTheme+
  annotate(geom="label",
           x=1,y=max(focal.cases.intros.sing$intros_meansum7d.mean,na.rm=T),
           label=text.adjustedr2,hjust=0)

p.sing.focal.cases.intros.sing

ggsave(paste0(f.out,"p.sing.focal.var.cases.intros.sing.lm.png"),height=3,width=3,unit="in")

#now predict number of singletons per week up until int.end
focal.cases.new<-focal.var.cases %>% filter(date<=int.end & date>=int.start) 
focal.cases.df.sing<-focal.cases.new$avgVOC_cases %>% as.data.frame()
colnames(focal.cases.df.sing)<-"avgVOC_cases"
new.vals<-predict.lm(lm.cases.intros.sing,newdata = focal.cases.df.sing,interval = "prediction")
#join new vals on with the new cases data
focal.cases.df.sing2<-focal.cases.df.sing %>% bind_cols(new.vals)
colnames(focal.cases.df.sing2)[2:4]<-c("intros_meansum7d.mean","lower","upper")
focal.cases.df.sing2$country<-focal.source

#plot this to see how many singletons there might have been during the ban
p.sing.obs.fit<-ggplot(focal.cases.df.sing2,
       aes(x=avgVOC_cases,y=intros_meansum7d.mean,group=country)) +
    geom_ribbon(aes(ymin = lower,ymax = upper,fill=country),alpha=0.3)+
  geom_point(aes(color=country))+
  GlobColScale+
  GlobFillScale+
  theme(legend.position = "none")+
  labs(x=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"), 
       y="Singletons per week")

## Join this onto the other dataset, and distinguish them as 'no restriction' (predicted) and 'restrictions' (observed)

focal.cases.intros.sing2<-focal.cases.intros.sing[,c("avgVOC_cases","intros_meansum7d.mean","country")]
focal.cases.intros.sing2$type<-"Observed"

## Also need to add confidence intervals to the observed data that was fitted
focal.cases.intros.sing2.new<-predict.lm(lm.cases.intros.sing,newdata = focal.cases.intros.sing2,interval = "prediction")
#only want the upper and lower here
focal.cases.intros.sing2.new<-focal.cases.intros.sing2.new[,c(2,3)]
colnames(focal.cases.intros.sing2.new)<-c("lower","upper")
focal.cases.intros.sing3<-focal.cases.intros.sing2 %>% bind_cols(focal.cases.intros.sing2.new)

#join new vals on with the new cases data
focal.cases.df.sing2<-focal.cases.df.sing %>% bind_cols(new.vals)
colnames(focal.cases.df.sing2)[2:4]<-c("intros_meansum7d.mean","lower","upper")
focal.cases.df.sing2$country<-focal.source
focal.cases.df.sing2$type<-"Predicted"      
focal.obs.pred<-bind_rows(focal.cases.intros.sing3,focal.cases.df.sing2)

## colors for this
pred.pal<-c("deepskyblue4","chartreuse3")
names(pred.pal)<-c("Observed","Predicted")

## Plot and color by type
p.sing.focal.obs.pred<-ggplot(focal.obs.pred,
       aes(x=avgVOC_cases,y=intros_meansum7d.mean,group=type)) +
  geom_ribbon(aes(ymin = lower,ymax = upper,fill=type),alpha=0.5)+
  geom_point(aes(color=type),alpha=0.8)+
  labs(x=paste0(focal.source.brf, ": Average daily ",Focal.var," cases"), y="Singletons per week")+
  # geom_smooth(method ="lm",alpha=0.2,color=type)+
  pubTheme+
  theme(legend.position = c(0.3,0.8),
        axis.title.x=element_text(margin = margin(t = -30)))+
  scale_fill_manual(values=pred.pal)+
  scale_color_manual(values=pred.pal)+
  guides(color=guide_legend(title="Scenario"),
         fill=guide_legend(title="Scenario"))+ 
    annotate(geom="label",
           x=max(focal.obs.pred$avgVOC_cases,na.rm=T),
           y=mean(focal.obs.pred$intros_meansum7d.mean,na.rm=T),
           label=text.adjustedr2,hjust=1,size=3)

p.sing.focal.obs.pred
ggsave(paste0(f.out,"p.sing.obs.pred.png"),height=3,width=3,unit="in")

## Join the predicted values onto the data over time

#get dates back in here
focal.cases.df.sing3<-focal.cases.df.sing2 %>% left_join(focal.var.cases[,c("date","avgVOC_cases")],by="avgVOC_cases")
focal.cases.df.sing3$type<-"Predicted"
# head(focal.cases.df.sing3)

focal.intros.sing.all<-sing.Par.Roll.summary %>% 
  filter(par.state%in%focal.source)
focal.intros.sing.all$type<-"Observed"
focal.intros.sing.all$date<-focal.intros.sing.all$tmrca.dt
# colnames(focal.cases.df.sing3)

### NOW plot this on the singletons per date
pred.pal2<-pred.pal
names(pred.pal2)<-c("Observed","Predicted")
y.lim<-c(0,max(focal.cases.df.sing3$intros_meansum7d.mean)+1)
p.sing.focal.pred.singleton<-focal.intros.sing.all %>%
  ggplot()+
  pubThemeDate+
  annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim[1], ymax =y.lim[2], color="grey85",size=0,  fill="grey80")+
  annotate(geom="text",x = (as.Date(int.start)+int.duration/2),y=y.lim[2]-0.2, vjust=1, hjust=0.5,
           label=int.descrip,fontface="italic",color="grey25",size=3)+
  labs(x="Date of most recent common ancestor",y=paste0("Singletons per week"))+
  scale_x_date(breaks=xdates, date_labels = "%b %Y",
                            limits=as.Date(c(start.date,"2021-06-01")),
                            expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+
  #observed
   geom_density(aes(x=date,y=intros_meansum7d.mean,color=type,fill=type),
               stat="identity",size=1.5,alpha=0.7)+
  #predicted
  geom_density(data=focal.cases.df.sing3,aes(x=date,y=intros_meansum7d.mean,
                                        color=type,fill=type),
               stat="identity",size=1.5,alpha=0.7)+
  
  guides(color=guide_legend(title="Scenario"),
         fill=guide_legend(title="Scenario"))+
  theme(legend.position = c(0.6,0.8))+
  scale_color_manual(values=pred.pal2)+
  scale_fill_manual(values=pred.pal2)
p.sing.focal.pred.singleton
ggsave(paste0(f.out,"p.sing.focal.pred.singleton.png"),height=3.5,width=3.5,unit="in")

```

## Calculate the difference in the area under curves
```{r}
#integrate stepwise daily 
observed.df<-focal.intros.sing.all %>% filter(date<=int.end & date>=int.start) %>% dplyr::select (c("date","intros_meansum7d.mean"))
id<-order(observed.df$date)
AUC.observed.sing<-sum(diff(observed.df$date[id])*
                  ((rollmean(observed.df$intros_meansum7d.mean[id],2))/7) ) %>% as.numeric()
#divided by seven to convert it to per day instead of per week
AUC.observed.sing #observed # of singletons

### Predicted:
predicted.df<-focal.cases.df.sing3 %>% filter(date<=int.end & date>=int.start) %>% dplyr::select (c("date","intros_meansum7d.mean"))
id<-order(predicted.df$date)
AUC.predicted.sing<-sum(diff(predicted.df$date[id])*
                  ((rollmean(predicted.df$intros_meansum7d.mean[id],2))/7) ) %>% as.numeric()
AUC.predicted.sing #observed # of singletons

## differences
averted.singleton<-AUC.predicted.sing - AUC.observed.sing
averted.singleton

#YAYY - need to repeat this for singletons
## ALso need to repeat it for a lower bound
```

## Add to text export
```{r}
cat(paste0("Singletons observed during restrictions: ",
           round(AUC.observed.sing,digits=1)),
    paste0("Singletons predicted (from mean single rate) during restrictions: ",
           round(AUC.predicted.sing,digits=1)),
    paste0("Singletons averted (from mean single rate) during restrictions: ", 
           round(averted.singleton,digits=1)),
## TOTAL CASES AVERTED= sublineages + singeltons
    paste0("TOTAL mean cases averted, sublins+singles, all source distrib: ",
           round(mean(cases.averted.all)+averted.singleton, digits=1)),
    paste0("TOTAL mean cases averted, sublins+singles, focal distrib: ",
           round(mean(cases.averted.focal)+averted.singleton, digits=1)),
   file=text.out, append=T,sep="\n")

```

## what were the total cases of variant in Canada and what were the averted cases as a percent of that
```{r}
meta.can.var.country.daily.full3<-read.csv(meta.can.cases.in)


can.var.cases<- meta.can.var.country.daily.full3 %>% 
  filter(var.WHO==Focal.var)
can.var.cases$date<-as.Date(can.var.cases$date)

## estimated total alpha cases
can.var.total.cases<-round(sum(can.var.cases$avgVOC_cases,na.rm=T)) #324,547
can.var.total.cases.text<-paste0("Total estimated ",Focal.var,"\n cases in Canada: ",can.var.total.cases)

#percentage averted of total
tots.averted<-round(mean(cases.averted.all)+averted.singleton, digits=1)
percent.focal.averted<-round(tots.averted/can.var.total.cases*100,digits=1)

## Plot cases with overlay for flight ban
y.lim2<-c(0,(ceiling(max(can.var.cases$avgVOC_cases,na.rm=T))+2200))
myvar<-varPalette.ch[which(names(varPalette.ch)==Focal.var)]

p.can.var.cases<-ggplot(can.var.cases)+
    annotate(geom="rect",xmin = as.Date(int.start), xmax = as.Date(int.end),
             ymin = y.lim2[1], ymax =y.lim2[2],
             color="grey85",size=0,  fill="grey80")+
    annotate(geom="text",x =( as.Date(int.start)+int.duration/2),
             y=(y.lim2[2]/100)*99, vjust=1, hjust=0.5,
             label=int.descrip,fontface="italic",color="grey25",size=3)+
    # add annotation for total estimated cases
    annotate(geom="label",x =int.end,
             y=(y.lim2[2]/4)*3, vjust=1, hjust=0.5,
             label=can.var.total.cases.text, size=3)+
    pubThemeDate+
    geom_bar(aes(x=date, y=avgVOC_cases),fill=myvar,stat="identity",width=1)+
    scaleDateFlexMore+
    scale_y_continuous(expand=c(0,0))+
    # GlobFillScale+
    theme(legend.position = "none")+
    labs(x=NULL, y=paste0("Canada",": Average daily ",Focal.var," cases"))
  
p.can.var.cases
ggsave(paste0(f.out,"canada.var.cases.sept.png"),height=3,width=4,unit="in")

```

## Add to text export
```{r}
cat(str_replace_all(can.var.total.cases.text,"\n",""),
    paste0("Percentage averted of total Canadian cases: ",percent.focal.averted),
    file=text.out, append=T,sep="\n")
```

## moved domestic transmisison to a different script